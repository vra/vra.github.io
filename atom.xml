<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yunfeng&#39;s Simple Blog</title>
  
  <subtitle>Love, Life, Linux</subtitle>
  <link href="http://vra.github.io/atom.xml" rel="self"/>
  
  <link href="http://vra.github.io/"/>
  <updated>2025-01-01T15:00:08.554Z</updated>
  <id>http://vra.github.io/</id>
  
  <author>
    <name>Yunfeng Wang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>2024年终总结</title>
    <link href="http://vra.github.io/2024/12/31/summary-2024/"/>
    <id>http://vra.github.io/2024/12/31/summary-2024/</id>
    <published>2024-12-31T14:58:13.000Z</published>
    <updated>2025-01-01T15:00:08.554Z</updated>
    
    <content type="html"><![CDATA[<ul><li>年终总结</li></ul><hr><p>2024年是幸福的一年，因为每天有可爱女儿的陪伴，正如此刻，她在旁边吃着山楂棒，看着我打下这行字。</p><p>父母回老家了，大家庭变成了小家庭，我们也在3月份搬进了自己的房子，老婆在家全职带娃，我上班离公司更近了，骑电瓶车15分钟到公司，大家都皆大欢喜。</p><p>工作内容也从纯视觉算法变化到了多模态算法，语音文本图像，都需要考虑。这种任务其实很有意思，更接近真人处理问题的情况。但难度也不小，未来继续加油吧。</p><p>平时上班，周末大部分时间都在陪娃，自己可支配的时间大大减少，因此写博客和开源项目上没太多产出，总共写了个8篇知乎文章，2个开源项目，一个是关于实时图片驱动人头项目，基于快手LivePortrait坐了一个实时版本的封装，另一个是基于LLM给代码仓库打分网站，可以在这里<a href="https://lcs.simpleai.site/">访问</a>。</p><p>第二个项目其实是一个基于AI驱动的产品尝试。由于AI能力的不断提升，写代码或者说技术壁垒成为一个门槛很低的事情，许多以前没法做的东西，现在在AI的帮助下可以很快地实现，例如那个项目中的Vue代码，完全是大模型不断地根据我的要求生成的，工作的很好。所以我觉得未来成功的产品是体现在创意上，目前来看似乎还没有那个AI产品有很好的创意而引爆C端市场。希望未来有更多的创客借助AI创造出精彩的产品。</p><p>这一年也是不断思考人和AI关系的一年，从实际问题到哲学命题，AI与人类的关系，我觉得在未来几年也会一直被讨论。但无法忽视的事实是，AI的能力提升飞快，已经在很多方面超过了顶尖的人类了。从Assistants，到Copilots，再到Colleagues，再到Critics，再到Twins，这种快速的关系变化可能从根本上改变人类对自己的认知。相信在2025年，还会有更多精彩被创造，希望在这个exciting的时代，能做出自己的一点贡献。</p><span id="more"></span><h3 id="出游与相聚"><a href="#出游与相聚" class="headerlink" title="出游与相聚"></a>出游与相聚</h3><p>1月18日，农历腊月八，初中同学真林结婚，我提前一天坐飞机回家，参加完婚礼下午坐飞机回来。这个陪我度过最后一个单身夜晚的好朋友也结婚了🤣最近可爱的女儿也出生了。</p><p>1月28日，云亮结婚，我们回家参加婚礼，然后彤彤和乖乖去庄浪，我回公司继续干活。</p><p>1月31日，栾京来杭州出差，我们张凯一起去湖滨银泰吃火锅。</p><p>2月8日，腊月二十九，要过年了，我先坐高铁到天水，到汽车站时，已经没有回庄浪的班车了。在汽车站外等了会，也没找到会庄浪到车，只能先坐出租车到秦安，再看怎么办。天水的出租车司机又坑了我一把，说好的的走高速，结果还是沿着低速缓慢走，不诚信的行为再一次上演。到秦安已经天黑了，有点饿，等了半天也没找到车，只能在秦安高铁站的天桥下，找了个卖釀皮的小摊，围着蜂窝煤炉子吃了点东西。之后找到了私家车，拉着四个往庄浪方向的人出发了。到庄浪已经晚上8点半。正月初三回我家，又是一番人在囧途。春节结束后，2月19日，也就是正月初十坐飞机回杭州。</p><p>3.月1日 团队去西溪源谷开年会，垂钓，飞盘，烧烤，抽奖，k歌，放烟花……</p><p>3月20日搬家，从22年年中搬到九堡，终于又回到了余杭。彩虹和龙哥从南通过来参加我们的搬家活动。</p><p>3月30日周末，小家庭去西溪湿地春游，在大树下睡了半天。</p><p>4月5日清明节，我们去桐庐吃桐庐菜，游富春江，爬富春山，负重20斤的小baby登顶富春山东西二个钓台，俯瞰富春江，有点意境。这过得非常舒服的一个假期。</p><p>5月1日劳动节，我们去苏州了，住在吴趋坊附近，夜游平江路独有一番风味，从商场出来的小巷一直走到平江路，人潮拥挤，小店林立，文创美食目不暇接。别的虎丘山，山塘街，泰伯庙，北寺塔，阊门，平门等大大小小的景点，护城河中缓缓驶过的游船，真的很有江南的感觉。还有商场的各种美食，吴趋坊的烤肉，真的美味。</p><p>5 月23日-5月26日我和几个同事去西安参加CCIG会议。参会之余和高中室友魏朝奇于参聚会，我们数年没见了。也和栾京一家吃了烧烤，然后去大唐不夜城，走路到地铁站回去。上次见他们还是去榆树参加他们的婚礼。</p><p>6月21-6月22日两天，小团队去千岛湖outing，吃鱼，K歌，烧烤，摘杨梅。</p><p>7月1日去富阳考驾照，科二挂了科三过了，7月21日重考科二和科四，拿到驾证。从5月5号开始练，总共耗时两个半月。</p><p>8月31日，我们去版本馆，上次来是版本馆刚开放的时候，天气炎热，没有深度看展馆内容。</p><p>9月7日，我们去玉鸟集玩，在玉鸟雕塑的草坪上坐了很久，有些惬意。然后去旁边的村民食堂吃饭，接着去单向空间大屋顶，单向空间自由阅读的感觉很棒。</p><p>9月15日，打车去下斗门村，在村北面拐角的时候，整个田野突然出现在眼前，仿佛走进了宫崎骏的田园世界。我们沿着河堤走到下陡门村网红树，休息后再走回北塘春池，玩了会吃了土菜，味道不错，然后打车回家。</p><p>9月17日中秋节，下午去杭师大北面的大草坪露营地等月亮升起。夜晚月亮从东边楼房上面探出头，然后往中天走。我们和月亮合影，然后点了水饺外卖，吃完才回去。</p><p>国庆节请了2天假，9月28先到天水，包叔顺路送我们到武山，第二天回家。10月3日云亮和明霞送我们到庄浪。由于10月2号晚上我们去k歌，大家都是食物中毒了，国庆接下来的几天都特别难受。</p><p>11月2日， 我们去良渚古城遗址公园，水稻黄了很好看，还有秋风送来远处好听的歌声，循着歌声而去，发现是有稻香音乐会，在草坪上听了会，然后去看了日落，又大又红又圆，真的是难以忘怀的一天。</p><p>11月3日，再次去西溪湿地，在老地方铺了垫子吃东西，拍照。</p><p>12月1日，和东升夫妇和东升妈妈一起去吃了兰木肆，东升也换工作了。</p><p>12月13日大团队爬九曜山，游净慈寺，第一次爬西湖西南角的山。</p><p>12月27日小团队年末聚餐，去吃铁锅炖，感觉吃的比之前好吃多了。</p><h3 id="读书"><a href="#读书" class="headerlink" title="读书"></a>读书</h3><p>《乔布斯传》<br>《创造：用非传统方式做有价值的事》<br>《李飞飞自传》<br>《一地鸡毛》<br>《万物皆计算：科学奇才的探索之旅》</p><h3 id="影视"><a href="#影视" class="headerlink" title="影视"></a>影视</h3><p>你想活出怎样的人生<br>年会不能停！<br>飞驰人生2<br>阿索卡<br>内景唐人街<br>老练律师<br>谜探路德维希<br>豺狼的日子</p>]]></content>
    
    
    <summary type="html">&lt;ul&gt;
&lt;li&gt;年终总结&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;2024年是幸福的一年，因为每天有可爱女儿的陪伴，正如此刻，她在旁边吃着山楂棒，看着我打下这行字。&lt;/p&gt;
&lt;p&gt;父母回老家了，大家庭变成了小家庭，我们也在3月份搬进了自己的房子，老婆在家全职带娃，我上班离公司更近了，骑电瓶车15分钟到公司，大家都皆大欢喜。&lt;/p&gt;
&lt;p&gt;工作内容也从纯视觉算法变化到了多模态算法，语音文本图像，都需要考虑。这种任务其实很有意思，更接近真人处理问题的情况。但难度也不小，未来继续加油吧。&lt;/p&gt;
&lt;p&gt;平时上班，周末大部分时间都在陪娃，自己可支配的时间大大减少，因此写博客和开源项目上没太多产出，总共写了个8篇知乎文章，2个开源项目，一个是关于实时图片驱动人头项目，基于快手LivePortrait坐了一个实时版本的封装，另一个是基于LLM给代码仓库打分网站，可以在这里&lt;a href=&quot;https://lcs.simpleai.site/&quot;&gt;访问&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;第二个项目其实是一个基于AI驱动的产品尝试。由于AI能力的不断提升，写代码或者说技术壁垒成为一个门槛很低的事情，许多以前没法做的东西，现在在AI的帮助下可以很快地实现，例如那个项目中的Vue代码，完全是大模型不断地根据我的要求生成的，工作的很好。所以我觉得未来成功的产品是体现在创意上，目前来看似乎还没有那个AI产品有很好的创意而引爆C端市场。希望未来有更多的创客借助AI创造出精彩的产品。&lt;/p&gt;
&lt;p&gt;这一年也是不断思考人和AI关系的一年，从实际问题到哲学命题，AI与人类的关系，我觉得在未来几年也会一直被讨论。但无法忽视的事实是，AI的能力提升飞快，已经在很多方面超过了顶尖的人类了。从Assistants，到Copilots，再到Colleagues，再到Critics，再到Twins，这种快速的关系变化可能从根本上改变人类对自己的认知。相信在2025年，还会有更多精彩被创造，希望在这个exciting的时代，能做出自己的一点贡献。&lt;/p&gt;</summary>
    
    
    
    
  </entry>
  
  <entry>
    <title>GitHub Models-免费的大模型Playgroud和API服务</title>
    <link href="http://vra.github.io/2024/09/14/github-models/"/>
    <id>http://vra.github.io/2024/09/14/github-models/</id>
    <published>2024-09-14T00:50:04.000Z</published>
    <updated>2024-10-23T01:08:07.669Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-功能说明"><a href="#1-功能说明" class="headerlink" title="1. 功能说明"></a>1. 功能说明</h3><p>GitHub在2024年8月10号左右的时候推出了GitHub Models新功能，提供运行大模型的Playground和免费API服务，用于进行AI大模型的实验和AI应用的原型验证。目前已经支持的模型包括GPT-4o系列，phi-3系列，Llama-3系列，以及一些Embedding模型等（OpenAI o1-mini和o1-preview虽然列出来了，但需要登陆Azure来使用）。</p><p><img data-src="/imgs/github-models/20240914083033.png"></p><span id="more"></span><h3 id="2-申请waitlist"><a href="#2-申请waitlist" class="headerlink" title="2. 申请waitlist"></a>2. 申请waitlist</h3><p>GitHub Models功能还在limited public beta阶段，需要先申请加入<a href="https://github.com/marketplace/models/waitlist/join">waitlist</a>，通过后才能体验。</p><p>本来以为跟之前Copilot，Codespace等功能一样，国内无法申请或者申请通过后无法使用，但这次却没有卡这些条件，我从8月13号提交申请，9月11号通过，目前测试国内网络也可以使用免费的API服务，因为服务都是搭建在Azure云服务上面的。</p><h3 id="3-请求限制"><a href="#3-请求限制" class="headerlink" title="3. 请求限制"></a>3. 请求限制</h3><p>GitHub 定位是给开发者开发AI应用原型提供免费的服务（某种程度上也是给Azure引流），所以有请求限制，具体来说，大模型限制级别分为Low和High，Low级别每分钟最多请求15次，每天上限是150，每次请求的最大输入token是8000，最大输出token数是4000，最大并发请求5个，High级别每分钟最多请求10次，每天上限是50，每次请求的最大输入token是8000，最大输出token数是4000，最大并发请求2个，所以这种quota，可能真的就够自己做原型调试用了。Embedding模型有单独的级别，具体数据见下表：</p><p><img data-src="/imgs/github-models/20240914083717.png"></p><h3 id="4-使用流程"><a href="#4-使用流程" class="headerlink" title="4. 使用流程"></a>4. 使用流程</h3><p>下面简单介绍一下使用的流程。</p><p>GitHub Models的网址是<a href="https://github.com/marketplace/models">https://github.com/marketplace/models</a>,除了开始图片展示的，还包含下面这些模型：<br><img data-src="/imgs/github-models/20240914084921.png"></p><p>选择一个模型后，进入到详情页面，有模型的介绍，还有Web上直接使用的Playground选项，以及API调用的 Get started选项，以及请求限制级别：<br><img data-src="/imgs/github-models/20240914085054.png"></p><p>点击Playground进入Web使用页面，看起来跟OpenAI网站很像，可以直接聊天，也可以调整右边的参数进行控制，同时除了Chat，还是Code和Raw模式：<br><img data-src="/imgs/github-models/20240914085230.png"><br>Chat 模式下，直接进行提问，返回结果，还可以点赞点踩，重新提问：<br><img data-src="/imgs/github-models/20240914085442.png"><br>Code模式下，会给出在Python代码中调用接口的示例：<br><img data-src="/imgs/github-models/20240914085629.png"><br>Raw模式下，会以JSON格式显示用户的问题，模型的回答：<br><img data-src="/imgs/github-models/20240914085721.png"></p><p>Raw模式和Chat模式都可以进行对话，JSON内容会实时更新：<br><img data-src="/imgs/github-models/20240914085935.png"></p><p>点Get Started按钮后，会显示API调用的详细说明：<br><img data-src="/imgs/github-models/20240914090039.png"><br>像这个模型，支持Python, JS， C#和REST四种形式的调用（有些模型只支持Python和JS）,<br>SDK可以选择OpenAI SDK（pip install openai）或者Azure AI Inference SDK(pip install  azure-ai-inference)，右边给出了详细的使用说明<br><img data-src="/imgs/github-models/20240914090137.png"></p><h3 id="5-API调用"><a href="#5-API调用" class="headerlink" title="5. API调用"></a>5. API调用</h3><p>首先需要在<a href="https://github.com/settings/tokens">GitHub 这里</a>生成TOKEN，这个TOKEN跟OpenAI Key一样，用于模型调用的鉴权等等。</p><h4 id="5-1-使用OpenAI-SDK"><a href="#5-1-使用OpenAI-SDK" class="headerlink" title="5.1 使用OpenAI SDK"></a>5.1 使用OpenAI SDK</h4><p>将上面GITHUB_TOKEN加入环境变量，然后就是熟悉的调用方式了，下面将单次对话，多次对话，流式输出，传入图片和调用工具的示例代码放上来，供参考</p><h5 id="5-1-1-单次对话"><a href="#5-1-1-单次对话" class="headerlink" title="5.1.1 单次对话"></a>5.1.1 单次对话</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">token = os.environ[<span class="string">&quot;GITHUB_TOKEN&quot;</span>]</span><br><span class="line">endpoint = <span class="string">&quot;https://models.inference.ai.azure.com&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;gpt-4o-mini&quot;</span></span><br><span class="line"></span><br><span class="line">client = OpenAI(</span><br><span class="line">    base_url=endpoint,</span><br><span class="line">    api_key=token,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response = client.chat.completions.create(</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">            <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a helpful assistant.&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,</span><br><span class="line">            <span class="string">&quot;content&quot;</span>: <span class="string">&quot;What is the capital of France?&quot;</span>,</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    model=model_name,</span><br><span class="line">    temperature=<span class="number">1.0</span>,</span><br><span class="line">    max_tokens=<span class="number">1000</span>,</span><br><span class="line">    top_p=<span class="number">1.0</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.choices[<span class="number">0</span>].message.content)</span><br></pre></td></tr></table></figure><h5 id="5-1-2-多轮对话"><a href="#5-1-2-多轮对话" class="headerlink" title="5.1.2 多轮对话"></a>5.1.2 多轮对话</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">token = os.environ[<span class="string">&quot;GITHUB_TOKEN&quot;</span>]</span><br><span class="line">endpoint = <span class="string">&quot;https://models.inference.ai.azure.com&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;gpt-4o-mini&quot;</span></span><br><span class="line"></span><br><span class="line">client = OpenAI(</span><br><span class="line">    base_url=endpoint,</span><br><span class="line">    api_key=token,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response = client.chat.completions.create(</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">            <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a helpful assistant.&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,</span><br><span class="line">            <span class="string">&quot;content&quot;</span>: <span class="string">&quot;What is the capital of France?&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;role&quot;</span>: <span class="string">&quot;assistant&quot;</span>,</span><br><span class="line">            <span class="string">&quot;content&quot;</span>: <span class="string">&quot;The capital of France is Paris.&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,</span><br><span class="line">            <span class="string">&quot;content&quot;</span>: <span class="string">&quot;What about Spain?&quot;</span>,</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    model=model_name,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.choices[<span class="number">0</span>].message.content)</span><br></pre></td></tr></table></figure><h5 id="5-1-3-流式输出"><a href="#5-1-3-流式输出" class="headerlink" title="5.1.3 流式输出"></a>5.1.3 流式输出</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">token = os.environ[<span class="string">&quot;GITHUB_TOKEN&quot;</span>]</span><br><span class="line">endpoint = <span class="string">&quot;https://models.inference.ai.azure.com&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;gpt-4o-mini&quot;</span></span><br><span class="line"></span><br><span class="line">client = OpenAI(</span><br><span class="line">    base_url=endpoint,</span><br><span class="line">    api_key=token,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response = client.chat.completions.create(</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">            <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a helpful assistant.&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,</span><br><span class="line">            <span class="string">&quot;content&quot;</span>: <span class="string">&quot;Give me 5 good reasons why I should exercise every day.&quot;</span>,</span><br><span class="line">        &#125;</span><br><span class="line">    ],</span><br><span class="line">    model=model_name,</span><br><span class="line">    stream=<span class="literal">True</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> update <span class="keyword">in</span> response:</span><br><span class="line">    <span class="keyword">if</span> update.choices[<span class="number">0</span>].delta.content:</span><br><span class="line">        <span class="built_in">print</span>(update.choices[<span class="number">0</span>].delta.content, end=<span class="string">&quot;&quot;</span>)</span><br></pre></td></tr></table></figure><h5 id="5-1-4-图片输入"><a href="#5-1-4-图片输入" class="headerlink" title="5.1.4 图片输入"></a>5.1.4 图片输入</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> base64</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">token = os.environ[<span class="string">&quot;GITHUB_TOKEN&quot;</span>]</span><br><span class="line">endpoint = <span class="string">&quot;https://models.inference.ai.azure.com&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;gpt-4o-mini&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_image_data_url</span>(<span class="params">image_file: <span class="built_in">str</span>, image_format: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Helper function to converts an image file to a data URL string.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        image_file (str): The path to the image file.</span></span><br><span class="line"><span class="string">        image_format (str): The format of the image file.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        str: The data URL of the image.</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(image_file, <span class="string">&quot;rb&quot;</span>) <span class="keyword">as</span> f:</span><br><span class="line">            image_data = base64.b64encode(f.read()).decode(<span class="string">&quot;utf-8&quot;</span>)</span><br><span class="line">    <span class="keyword">except</span> FileNotFoundError:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Could not read &#x27;<span class="subst">&#123;image_file&#125;</span>&#x27;.&quot;</span>)</span><br><span class="line">        exit()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">f&quot;data:image/<span class="subst">&#123;image_format&#125;</span>;base64,<span class="subst">&#123;image_data&#125;</span>&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">client = OpenAI(</span><br><span class="line">    base_url=endpoint,</span><br><span class="line">    api_key=token,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response = client.chat.completions.create(</span><br><span class="line">    messages=[</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>,</span><br><span class="line">            <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You are a helpful assistant that describes images in details.&quot;</span>,</span><br><span class="line">        &#125;,</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>,</span><br><span class="line">            <span class="string">&quot;content&quot;</span>: [</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;text&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;text&quot;</span>: <span class="string">&quot;What&#x27;s in this image?&quot;</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;image_url&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;image_url&quot;</span>: &#123;</span><br><span class="line">                        <span class="string">&quot;url&quot;</span>: get_image_data_url(<span class="string">&quot;sample.jpg&quot;</span>, <span class="string">&quot;jpg&quot;</span>),</span><br><span class="line">                        <span class="string">&quot;detail&quot;</span>: <span class="string">&quot;low&quot;</span></span><br><span class="line">                    &#125;,</span><br><span class="line">                &#125;,</span><br><span class="line">            ],</span><br><span class="line">        &#125;,</span><br><span class="line">    ],</span><br><span class="line">    model=model_name,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.choices[<span class="number">0</span>].message.content)</span><br></pre></td></tr></table></figure><h5 id="5-1-5-工具调用"><a href="#5-1-5-工具调用" class="headerlink" title="5.1.5 工具调用"></a>5.1.5 工具调用</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line">token = os.environ[<span class="string">&quot;GITHUB_TOKEN&quot;</span>]</span><br><span class="line">endpoint = <span class="string">&quot;https://models.inference.ai.azure.com&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;gpt-4o-mini&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a function that returns flight information between two cities (mock implementation)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_flight_info</span>(<span class="params">origin_city: <span class="built_in">str</span>, destination_city: <span class="built_in">str</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> origin_city == <span class="string">&quot;Seattle&quot;</span> <span class="keyword">and</span> destination_city == <span class="string">&quot;Miami&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> json.dumps(&#123;</span><br><span class="line">            <span class="string">&quot;airline&quot;</span>: <span class="string">&quot;Delta&quot;</span>,</span><br><span class="line">            <span class="string">&quot;flight_number&quot;</span>: <span class="string">&quot;DL123&quot;</span>,</span><br><span class="line">            <span class="string">&quot;flight_date&quot;</span>: <span class="string">&quot;May 7th, 2024&quot;</span>,</span><br><span class="line">            <span class="string">&quot;flight_time&quot;</span>: <span class="string">&quot;10:00AM&quot;</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> json.dumps(&#123;<span class="string">&quot;error&quot;</span>: <span class="string">&quot;No flights found between the cities&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a function tool that the model can ask to invoke in order to retrieve flight information</span></span><br><span class="line">tool=&#123;</span><br><span class="line">    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;function&quot;</span>,</span><br><span class="line">    <span class="string">&quot;function&quot;</span>: &#123;</span><br><span class="line">        <span class="string">&quot;name&quot;</span>: <span class="string">&quot;get_flight_info&quot;</span>,</span><br><span class="line">        <span class="string">&quot;description&quot;</span>: <span class="string">&quot;&quot;&quot;Returns information about the next flight between two cities.</span></span><br><span class="line"><span class="string">            This includes the name of the airline, flight number and the date and time</span></span><br><span class="line"><span class="string">            of the next flight&quot;&quot;&quot;</span>,</span><br><span class="line">        <span class="string">&quot;parameters&quot;</span>: &#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;object&quot;</span>,</span><br><span class="line">            <span class="string">&quot;properties&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;origin_city&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;description&quot;</span>: <span class="string">&quot;The name of the city where the flight originates&quot;</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&quot;destination_city&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>, </span><br><span class="line">                    <span class="string">&quot;description&quot;</span>: <span class="string">&quot;The flight destination city&quot;</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&quot;required&quot;</span>: [</span><br><span class="line">                <span class="string">&quot;origin_city&quot;</span>,</span><br><span class="line">                <span class="string">&quot;destination_city&quot;</span></span><br><span class="line">            ],</span><br><span class="line">        &#125;,</span><br><span class="line">    &#125;,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">client = OpenAI(</span><br><span class="line">    base_url=endpoint,</span><br><span class="line">    api_key=token,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">messages=[</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;system&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;You an assistant that helps users find flight information.&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>, <span class="string">&quot;content&quot;</span>: <span class="string">&quot;I&#x27;m interested in going to Miami. What is the next flight there from Seattle?&quot;</span>&#125;,</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response = client.chat.completions.create(</span><br><span class="line">    messages=messages,</span><br><span class="line">    tools=[tool],</span><br><span class="line">    model=model_name,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We expect the model to ask for a tool call</span></span><br><span class="line"><span class="keyword">if</span> response.choices[<span class="number">0</span>].finish_reason == <span class="string">&quot;tool_calls&quot;</span>:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Append the model response to the chat history</span></span><br><span class="line">    messages.append(response.choices[<span class="number">0</span>].message)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We expect a single tool call</span></span><br><span class="line">    <span class="keyword">if</span> response.choices[<span class="number">0</span>].message.tool_calls <span class="keyword">and</span> <span class="built_in">len</span>(response.choices[<span class="number">0</span>].message.tool_calls) == <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">        tool_call = response.choices[<span class="number">0</span>].message.tool_calls[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># We expect the tool to be a function call</span></span><br><span class="line">        <span class="keyword">if</span> tool_call.<span class="built_in">type</span> == <span class="string">&quot;function&quot;</span>:</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Parse the function call arguments and call the function</span></span><br><span class="line">            function_args = json.loads(tool_call.function.arguments.replace(<span class="string">&quot;&#x27;&quot;</span>, <span class="string">&#x27;&quot;&#x27;</span>))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Calling function `<span class="subst">&#123;tool_call.function.name&#125;</span>` with arguments <span class="subst">&#123;function_args&#125;</span>&quot;</span>)</span><br><span class="line">            callable_func = <span class="built_in">locals</span>()[tool_call.function.name]</span><br><span class="line">            function_return = callable_func(**function_args)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Function returned = <span class="subst">&#123;function_return&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Append the function call result fo the chat history</span></span><br><span class="line">            messages.append(</span><br><span class="line">                &#123;</span><br><span class="line">                    <span class="string">&quot;tool_call_id&quot;</span>: tool_call.<span class="built_in">id</span>,</span><br><span class="line">                    <span class="string">&quot;role&quot;</span>: <span class="string">&quot;tool&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;name&quot;</span>: tool_call.function.name,</span><br><span class="line">                    <span class="string">&quot;content&quot;</span>: function_return,</span><br><span class="line">                &#125;</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Get another response from the model</span></span><br><span class="line">            response = client.chat.completions.create(</span><br><span class="line">                messages=messages,</span><br><span class="line">                tools=[tool],</span><br><span class="line">                model=model_name,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Model response = <span class="subst">&#123;response.choices[<span class="number">0</span>].message.content&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="5-2-使用Azure-AI-Inference-SDK"><a href="#5-2-使用Azure-AI-Inference-SDK" class="headerlink" title="5.2 使用Azure AI Inference SDK"></a>5.2 使用Azure AI Inference SDK</h4><p>整体上与使用OpenAI SDK类似，有些函数接口有变化</p><h5 id="5-2-1-单次推理"><a href="#5-2-1-单次推理" class="headerlink" title="5.2.1 单次推理"></a>5.2.1 单次推理</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> azure.ai.inference <span class="keyword">import</span> ChatCompletionsClient</span><br><span class="line"><span class="keyword">from</span> azure.ai.inference.models <span class="keyword">import</span> SystemMessage, UserMessage</span><br><span class="line"><span class="keyword">from</span> azure.core.credentials <span class="keyword">import</span> AzureKeyCredential</span><br><span class="line"></span><br><span class="line">endpoint = <span class="string">&quot;https://models.inference.ai.azure.com&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;gpt-4o-mini&quot;</span></span><br><span class="line">token = os.environ[<span class="string">&quot;GITHUB_TOKEN&quot;</span>]</span><br><span class="line"></span><br><span class="line">client = ChatCompletionsClient(</span><br><span class="line">    endpoint=endpoint,</span><br><span class="line">    credential=AzureKeyCredential(token),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response = client.complete(</span><br><span class="line">    messages=[</span><br><span class="line">        SystemMessage(content=<span class="string">&quot;You are a helpful assistant.&quot;</span>),</span><br><span class="line">        UserMessage(content=<span class="string">&quot;What is the capital of France?&quot;</span>),</span><br><span class="line">    ],</span><br><span class="line">    model=model_name,</span><br><span class="line">    temperature=<span class="number">1.0</span>,</span><br><span class="line">    max_tokens=<span class="number">1000</span>,</span><br><span class="line">    top_p=<span class="number">1.0</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.choices[<span class="number">0</span>].message.content)</span><br></pre></td></tr></table></figure><h5 id="5-2-2-多轮推理"><a href="#5-2-2-多轮推理" class="headerlink" title="5.2.2 多轮推理"></a>5.2.2 多轮推理</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> azure.ai.inference <span class="keyword">import</span> ChatCompletionsClient</span><br><span class="line"><span class="keyword">from</span> azure.ai.inference.models <span class="keyword">import</span> AssistantMessage, SystemMessage, UserMessage</span><br><span class="line"><span class="keyword">from</span> azure.core.credentials <span class="keyword">import</span> AzureKeyCredential</span><br><span class="line"></span><br><span class="line">token = os.environ[<span class="string">&quot;GITHUB_TOKEN&quot;</span>]</span><br><span class="line">endpoint = <span class="string">&quot;https://models.inference.ai.azure.com&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;gpt-4o-mini&quot;</span></span><br><span class="line"></span><br><span class="line">client = ChatCompletionsClient(</span><br><span class="line">    endpoint=endpoint,</span><br><span class="line">    credential=AzureKeyCredential(token),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">messages = [</span><br><span class="line">    SystemMessage(content=<span class="string">&quot;You are a helpful assistant.&quot;</span>),</span><br><span class="line">    UserMessage(content=<span class="string">&quot;What is the capital of France?&quot;</span>),</span><br><span class="line">    AssistantMessage(content=<span class="string">&quot;The capital of France is Paris.&quot;</span>),</span><br><span class="line">    UserMessage(content=<span class="string">&quot;What about Spain?&quot;</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response = client.complete(messages=messages, model=model_name)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.choices[<span class="number">0</span>].message.content)</span><br></pre></td></tr></table></figure><h5 id="5-2-3-流式输出"><a href="#5-2-3-流式输出" class="headerlink" title="5.2.3 流式输出"></a>5.2.3 流式输出</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> azure.ai.inference <span class="keyword">import</span> ChatCompletionsClient</span><br><span class="line"><span class="keyword">from</span> azure.ai.inference.models <span class="keyword">import</span> SystemMessage, UserMessage</span><br><span class="line"><span class="keyword">from</span> azure.core.credentials <span class="keyword">import</span> AzureKeyCredential</span><br><span class="line"></span><br><span class="line">token = os.environ[<span class="string">&quot;GITHUB_TOKEN&quot;</span>]</span><br><span class="line">endpoint = <span class="string">&quot;https://models.inference.ai.azure.com&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;gpt-4o-mini&quot;</span></span><br><span class="line"></span><br><span class="line">client = ChatCompletionsClient(</span><br><span class="line">    endpoint=endpoint,</span><br><span class="line">    credential=AzureKeyCredential(token),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response = client.complete(</span><br><span class="line">    stream=<span class="literal">True</span>,</span><br><span class="line">    messages=[</span><br><span class="line">        SystemMessage(content=<span class="string">&quot;You are a helpful assistant.&quot;</span>),</span><br><span class="line">        UserMessage(content=<span class="string">&quot;Give me 5 good reasons why I should exercise every day.&quot;</span>),</span><br><span class="line">    ],</span><br><span class="line">    model=model_name,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> update <span class="keyword">in</span> response:</span><br><span class="line">    <span class="keyword">if</span> update.choices:</span><br><span class="line">        <span class="built_in">print</span>(update.choices[<span class="number">0</span>].delta.content <span class="keyword">or</span> <span class="string">&quot;&quot;</span>, end=<span class="string">&quot;&quot;</span>)</span><br><span class="line"></span><br><span class="line">client.close()</span><br></pre></td></tr></table></figure><h5 id="5-2-4-调用图片"><a href="#5-2-4-调用图片" class="headerlink" title="5.2.4 调用图片"></a>5.2.4 调用图片</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> azure.ai.inference <span class="keyword">import</span> ChatCompletionsClient</span><br><span class="line"><span class="keyword">from</span> azure.ai.inference.models <span class="keyword">import</span> (</span><br><span class="line">    SystemMessage,</span><br><span class="line">    UserMessage,</span><br><span class="line">    TextContentItem,</span><br><span class="line">    ImageContentItem,</span><br><span class="line">    ImageUrl,</span><br><span class="line">    ImageDetailLevel,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> azure.core.credentials <span class="keyword">import</span> AzureKeyCredential</span><br><span class="line"></span><br><span class="line">token = os.environ[<span class="string">&quot;GITHUB_TOKEN&quot;</span>]</span><br><span class="line">endpoint = <span class="string">&quot;https://models.inference.ai.azure.com&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;gpt-4o-mini&quot;</span></span><br><span class="line"></span><br><span class="line">client = ChatCompletionsClient(</span><br><span class="line">    endpoint=endpoint,</span><br><span class="line">    credential=AzureKeyCredential(token),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">response = client.complete(</span><br><span class="line">    messages=[</span><br><span class="line">        SystemMessage(</span><br><span class="line">            content=<span class="string">&quot;You are a helpful assistant that describes images in details.&quot;</span></span><br><span class="line">        ),</span><br><span class="line">        UserMessage(</span><br><span class="line">            content=[</span><br><span class="line">                TextContentItem(text=<span class="string">&quot;What&#x27;s in this image?&quot;</span>),</span><br><span class="line">                ImageContentItem(</span><br><span class="line">                    image_url=ImageUrl.load(</span><br><span class="line">                        image_file=<span class="string">&quot;sample.jpg&quot;</span>,</span><br><span class="line">                        image_format=<span class="string">&quot;jpg&quot;</span>,</span><br><span class="line">                        detail=ImageDetailLevel.LOW)</span><br><span class="line">                ),</span><br><span class="line">            ],</span><br><span class="line">        ),</span><br><span class="line">    ],</span><br><span class="line">    model=model_name,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(response.choices[<span class="number">0</span>].message.content)</span><br></pre></td></tr></table></figure><h5 id="5-2-5-使用工具"><a href="#5-2-5-使用工具" class="headerlink" title="5.2.5 使用工具"></a>5.2.5 使用工具</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">from</span> azure.ai.inference <span class="keyword">import</span> ChatCompletionsClient</span><br><span class="line"><span class="keyword">from</span> azure.ai.inference.models <span class="keyword">import</span> (</span><br><span class="line">    AssistantMessage,</span><br><span class="line">    ChatCompletionsToolCall,</span><br><span class="line">    ChatCompletionsToolDefinition,</span><br><span class="line">    CompletionsFinishReason,</span><br><span class="line">    FunctionDefinition,</span><br><span class="line">    SystemMessage,</span><br><span class="line">    ToolMessage,</span><br><span class="line">    UserMessage,</span><br><span class="line">)</span><br><span class="line"><span class="keyword">from</span> azure.core.credentials <span class="keyword">import</span> AzureKeyCredential</span><br><span class="line"></span><br><span class="line">token = os.environ[<span class="string">&quot;GITHUB_TOKEN&quot;</span>]</span><br><span class="line">endpoint = <span class="string">&quot;https://models.inference.ai.azure.com&quot;</span></span><br><span class="line">model_name = <span class="string">&quot;gpt-4o-mini&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a function that returns flight information between two cities (mock implementation)</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_flight_info</span>(<span class="params">origin_city: <span class="built_in">str</span>, destination_city: <span class="built_in">str</span></span>):</span></span><br><span class="line">    <span class="keyword">if</span> origin_city == <span class="string">&quot;Seattle&quot;</span> <span class="keyword">and</span> destination_city == <span class="string">&quot;Miami&quot;</span>:</span><br><span class="line">        <span class="keyword">return</span> json.dumps(&#123;</span><br><span class="line">            <span class="string">&quot;airline&quot;</span>: <span class="string">&quot;Delta&quot;</span>,</span><br><span class="line">            <span class="string">&quot;flight_number&quot;</span>: <span class="string">&quot;DL123&quot;</span>,</span><br><span class="line">            <span class="string">&quot;flight_date&quot;</span>: <span class="string">&quot;May 7th, 2024&quot;</span>,</span><br><span class="line">            <span class="string">&quot;flight_time&quot;</span>: <span class="string">&quot;10:00AM&quot;</span>&#125;)</span><br><span class="line">    <span class="keyword">return</span> json.dumps(&#123;<span class="string">&quot;error&quot;</span>: <span class="string">&quot;No flights found between the cities&quot;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Define a function tool that the model can ask to invoke in order to retrieve flight information</span></span><br><span class="line">flight_info = ChatCompletionsToolDefinition(</span><br><span class="line">    function=FunctionDefinition(</span><br><span class="line">        name=<span class="string">&quot;get_flight_info&quot;</span>,</span><br><span class="line">        description=<span class="string">&quot;&quot;&quot;Returns information about the next flight between two cities.</span></span><br><span class="line"><span class="string">            This includes the name of the airline, flight number and the date and</span></span><br><span class="line"><span class="string">            time of the next flight&quot;&quot;&quot;</span>,</span><br><span class="line">        parameters=&#123;</span><br><span class="line">            <span class="string">&quot;type&quot;</span>: <span class="string">&quot;object&quot;</span>,</span><br><span class="line">            <span class="string">&quot;properties&quot;</span>: &#123;</span><br><span class="line">                <span class="string">&quot;origin_city&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;description&quot;</span>: <span class="string">&quot;The name of the city where the flight originates&quot;</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                <span class="string">&quot;destination_city&quot;</span>: &#123;</span><br><span class="line">                    <span class="string">&quot;type&quot;</span>: <span class="string">&quot;string&quot;</span>,</span><br><span class="line">                    <span class="string">&quot;description&quot;</span>: <span class="string">&quot;The flight destination city&quot;</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">            &#125;,</span><br><span class="line">            <span class="string">&quot;required&quot;</span>: [<span class="string">&quot;origin_city&quot;</span>, <span class="string">&quot;destination_city&quot;</span>],</span><br><span class="line">        &#125;,</span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">client = ChatCompletionsClient(</span><br><span class="line">    endpoint=endpoint,</span><br><span class="line">    credential=AzureKeyCredential(token),</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">messages = [</span><br><span class="line">    SystemMessage(content=<span class="string">&quot;You an assistant that helps users find flight information.&quot;</span>),</span><br><span class="line">    UserMessage(content=<span class="string">&quot;I&#x27;m interested in going to Miami. What is the next flight there from Seattle?&quot;</span>),</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">response = client.complete(</span><br><span class="line">    messages=messages,</span><br><span class="line">    tools=[flight_info],</span><br><span class="line">    model=model_name,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We expect the model to ask for a tool call</span></span><br><span class="line"><span class="keyword">if</span> response.choices[<span class="number">0</span>].finish_reason == CompletionsFinishReason.TOOL_CALLS:</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Append the model response to the chat history</span></span><br><span class="line">    messages.append(AssistantMessage(tool_calls=response.choices[<span class="number">0</span>].message.tool_calls))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># We expect a single tool call</span></span><br><span class="line">    <span class="keyword">if</span> response.choices[<span class="number">0</span>].message.tool_calls <span class="keyword">and</span> <span class="built_in">len</span>(response.choices[<span class="number">0</span>].message.tool_calls) == <span class="number">1</span>:</span><br><span class="line"></span><br><span class="line">        tool_call = response.choices[<span class="number">0</span>].message.tool_calls[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># We expect the tool to be a function call</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(tool_call, ChatCompletionsToolCall):</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Parse the function call arguments and call the function</span></span><br><span class="line">            function_args = json.loads(tool_call.function.arguments.replace(<span class="string">&quot;&#x27;&quot;</span>, <span class="string">&#x27;&quot;&#x27;</span>))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Calling function `<span class="subst">&#123;tool_call.function.name&#125;</span>` with arguments <span class="subst">&#123;function_args&#125;</span>&quot;</span>)</span><br><span class="line">            callable_func = <span class="built_in">locals</span>()[tool_call.function.name]</span><br><span class="line">            function_return = callable_func(**function_args)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Function returned = <span class="subst">&#123;function_return&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Append the function call result fo the chat history</span></span><br><span class="line">            messages.append(ToolMessage(tool_call_id=tool_call.<span class="built_in">id</span>, content=function_return))</span><br><span class="line"></span><br><span class="line">            <span class="comment"># Get another response from the model</span></span><br><span class="line">            response = client.complete(</span><br><span class="line">                messages=messages,</span><br><span class="line">                tools=[flight_info],</span><br><span class="line">                model=model_name,</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;Model response = <span class="subst">&#123;response.choices[<span class="number">0</span>].message.content&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure><h3 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h3><p>GitHub Models总体上来说还是一个有用的工具，有下面的优点：</p><ol><li>免费</li><li>服务部署在Azure云服务器，国内网络可访问</li><li>有GPT-4o系列模型和对应API，对于没有OpenAI账号的开发者可以基于这里的API开发应用</li><li>设计良好的SDK，支持Python, JS, C#和REST等形式</li></ol><p>当然缺点也有：</p><ol><li>访问次数有上限，输入输出token有限制</li><li>模型并不多，目前只有30个模型，像Claude就没有</li></ol><p>希望这篇文章能让你对GitHub Models这个功能有更清晰的认识，欢迎点赞，收藏和评论！</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-功能说明&quot;&gt;&lt;a href=&quot;#1-功能说明&quot; class=&quot;headerlink&quot; title=&quot;1. 功能说明&quot;&gt;&lt;/a&gt;1. 功能说明&lt;/h3&gt;&lt;p&gt;GitHub在2024年8月10号左右的时候推出了GitHub Models新功能，提供运行大模型的Playground和免费API服务，用于进行AI大模型的实验和AI应用的原型验证。目前已经支持的模型包括GPT-4o系列，phi-3系列，Llama-3系列，以及一些Embedding模型等（OpenAI o1-mini和o1-preview虽然列出来了，但需要登陆Azure来使用）。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;/imgs/github-models/20240914083033.png&quot;&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="GitHub" scheme="http://vra.github.io/tags/GitHub/"/>
    
    <category term="GitHub Models" scheme="http://vra.github.io/tags/GitHub-Models/"/>
    
  </entry>
  
  <entry>
    <title>国内加速 GitHub 代码克隆的一种方案</title>
    <link href="http://vra.github.io/2024/09/14/speedup-github-clone/"/>
    <id>http://vra.github.io/2024/09/14/speedup-github-clone/</id>
    <published>2024-09-14T00:47:18.000Z</published>
    <updated>2024-10-23T00:49:13.736Z</updated>
    
    <content type="html"><![CDATA[<p>国内下载 GitHub 上代码一直是一件让人很头疼的事情，相信大家都深有体会。</p><p>最近偶然发现一个比较好用的解决方案，是采用<a href="http://gitclone.com/">http://gitclone.com</a>的加速，这里记录一下。</p><p>具体来说，在仓库url中增加<code>gitclone.com</code>的前缀，别的地方不变，即<code>https://github.com/</code>修改为<code>https://gitclone.com/github.com/</code>，例如原始的clone命令是:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/huggingface/transformers</span><br></pre></td></tr></table></figure><p>替换成下面的命令即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://gitclone.com/github.com/huggingface/transformers</span><br></pre></td></tr></table></figure><p>实测基本上能做到1M/s的下载速度。</p><p>这种加速目前只支持git clone 和git pull 命令，所以适用于拉取别人代码进行本地查看的应用场景。</p><p>另外发现这种加速方式下载的仓库，有一些只有最新的一次提交，有一些则包含完整提交，原因未知。</p><p>此外，请确认克隆的代码是否与GitHub上一致，我们无法保证拉取的代码是否被修改过。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;国内下载 GitHub 上代码一直是一件让人很头疼的事情，相信大家都深有体会。&lt;/p&gt;
&lt;p&gt;最近偶然发现一个比较好用的解决方案，是采用&lt;a href=&quot;http://gitclone.com/&quot;&gt;http://gitclone.com&lt;/a&gt;的加速，这里记录一下。&lt;/p&gt;</summary>
      
    
    
    
    
    <category term="GitHub" scheme="http://vra.github.io/tags/GitHub/"/>
    
    <category term="git" scheme="http://vra.github.io/tags/git/"/>
    
  </entry>
  
  <entry>
    <title>hangzhou-line1-benchmark-一个简单的图片理解问题集</title>
    <link href="http://vra.github.io/2024/09/01/hangzhou-line1-benchmark/"/>
    <id>http://vra.github.io/2024/09/01/hangzhou-line1-benchmark/</id>
    <published>2024-09-01T00:44:16.000Z</published>
    <updated>2024-10-23T01:04:02.373Z</updated>
    
    <content type="html"><![CDATA[<h3 id="0-概述"><a href="#0-概述" class="headerlink" title="0. 概述"></a>0. 概述</h3><p>最近qwen2发布了多模态系列模型Qwen2-VL，查看blog发现，72B的模型在很多benchmark上都超过了GPT-4o，而根据之前的经验，标准测试集上的效果与实际使用体验并不总是一致的。之前在某个多模态模型出来的时候，随手拍了一张地铁线路图做测试，发现效果不尽如人意。这两天花时间将这张地铁线路截图中的问题进行了标准化，构建了一个简单的图片理解测试集，让我们看看Qwen2-VL到底行不行。</p><span id="more"></span><h3 id="1-测试问题构建"><a href="#1-测试问题构建" class="headerlink" title="1. 测试问题构建"></a>1. 测试问题构建</h3><p>为了保证测试问题构建简单，只围绕下面这张地铁截图进行问题设计，所以考察的并不是模型的综合能力，而是考察日常生活中的一个小的实际场景下的效果的好坏，这样有点以偏概全，但这种随机的场景上的明显提升，才能真正体现模型的能力。</p><p>另外实际问题时也跟标准测试集不同，尽量口语化，非标准化，不会像法律文书那样精准描述，这也是为了模拟日常对话的情况。</p><p>总共10个问题：</p><p>仅根据上传截图中的信息，回答下面问题：<br>这张截图显示的是几号线<br>这张截图总共包含了多少个地铁站<br>这站截图的地铁站中，总共有多少个换乘站<br>当前是在哪个站<br>沿着红色箭头方向，闸弄口的下下站是什么站<br>终点站是哪个站<br>从彭埠到龙翔桥，总共要坐几站（包含彭埠和龙翔桥）<br>图中的地铁线路与5号线有几个换乘站<br>有几个站可以坐火车<br>图中的地铁线路总共可以几条线路换乘</p><p>这10个问题考察模型下面几个方面的能力：</p><ol><li>文字识别理解，如地铁线路编号，</li><li>图片理解，如换乘标识，火车logo，箭头方向</li><li>推理能力，如从站A到站B总共要坐几站</li><li>NLP能力，如”下下站”（发现大多数模型没理解这个词）</li><li>多维度理解能力，例如结合箭头方向和线路图，寻找下下站是哪一站</li></ol><p>为了保证模型的分数可以量化，这里选择的都是确定性问题。<br>得分情况是答对一题算一分，否则算0分，因此满分10分，最低0分。</p><h3 id="2-测试模型说明"><a href="#2-测试模型说明" class="headerlink" title="2. 测试模型说明"></a>2. 测试模型说明</h3><p>为了保证测试的简单，这里只对比了几个PC 网页端可以访问的多模态模型，测试日期为2024-09-01, 具体访问网址如下：</p><ol><li>豆包：<a href="https://www.doubao.com/chat/">https://www.doubao.com/chat/</a></li><li><a href="https://kimi.moonshot.cn/chat">Kimi.ai - 帮你看更大的世界 (moonshot.cn)</a></li><li>讯飞星火：<a href="https://xinghuo.xfyun.cn/desk">讯飞星火大模型-AI大语言模型-星火大模型-科大讯飞 (xfyun.cn)</a></li><li>智谱清言：<a href="https://chatglm.cn/">https://chatglm.cn/</a></li><li>GPT-4o mini: API</li><li>Qwen2-VL-7B: <a href="https://modelscope.cn/studios/qwen/Qwen2-7B-VL-demo">千问2多模态视觉模型-7B体验空间 · 创空间 (modelscope.cn)</a></li><li>Qwen2-VL-72B: <a href="https://huggingface.co/spaces/Qwen/Qwen2-VL">Qwen2-VL-72B - a Hugging Face Space by Qwen</a></li></ol><p>除了GPT-4o mini，别的模型都可以直接点击网址进行体验。</p><p>测试方式很简单，访问网页，新建对话，上传图片，将上面的问题粘贴进去，回车等待结果。</p><h3 id="3-分值量化"><a href="#3-分值量化" class="headerlink" title="3. 分值量化"></a>3. 分值量化</h3><p>先上总的结果表格：<br> <img data-src="/imgs/hangzhou_line1_benchmark/results.jpg"><br>可以看到最新发布的Qwen2-VL-7B还是比较一般，只有4分，Qwen2-VL-72B效果提升很明显，从7B的4分提升到了8分，也是几个模型里面唯一及格的。</p><p>具体每个模型的回答截图如下，供参考。</p><h3 id="4-Qwen2-VL-72B-的解题细节"><a href="#4-Qwen2-VL-72B-的解题细节" class="headerlink" title="4. Qwen2-VL-72B 的解题细节"></a>4. Qwen2-VL-72B 的解题细节</h3><p>QWen2-VL-72B真的这么强吗，为了进一步分析，我让它不光返回结果，还对中间的分析过程进行说明，结果如下：<br><img data-src="/imgs/hangzhou_line1_benchmark/qwen2-v2-72b-explain.jpg"></p><p>发现结果答对的题目中，有几个题目分析结果并不对：</p><ol><li>第3题中，换乘站少了近江，多了闸弄口</li><li>第8题中，换乘站多了一个火车东站，少了一个打铁关</li></ol><p>所以说，其实qwen2蒙对了2道题，或者说中间解题过程有错误，如果只考最终结果，能得80分，如果要写中间过程，那估计只能得60分了。</p><p>另外通过中间回答，发现它对“下下站”的理解不对，理解成了下一站，但单独问，却能正确回答：<br><img data-src="/imgs/hangzhou_line1_benchmark/20240901082920.png"></p><p>另外多维度联想能力不太好，例如第7题目，沿着红色箭头方向，应该是从下往上的方向，但Qwen2-VL-72B搞反了。</p><h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><p>到这个程度，我觉得多模态模型差能够解决一些日常生活中的推理问题了，玩起来会更有趣一些。问题和图片放到这个仓库了，后面出来新的模型还会继续用这个hangzhou_line1_benchmark进行测试，希望我的这个简单测试问题集早日被打爆。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;0-概述&quot;&gt;&lt;a href=&quot;#0-概述&quot; class=&quot;headerlink&quot; title=&quot;0. 概述&quot;&gt;&lt;/a&gt;0. 概述&lt;/h3&gt;&lt;p&gt;最近qwen2发布了多模态系列模型Qwen2-VL，查看blog发现，72B的模型在很多benchmark上都超过了GPT-4o，而根据之前的经验，标准测试集上的效果与实际使用体验并不总是一致的。之前在某个多模态模型出来的时候，随手拍了一张地铁线路图做测试，发现效果不尽如人意。这两天花时间将这张地铁线路截图中的问题进行了标准化，构建了一个简单的图片理解测试集，让我们看看Qwen2-VL到底行不行。&lt;/p&gt;</summary>
    
    
    
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="Deep Learning" scheme="http://vra.github.io/tags/Deep-Learning/"/>
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="GPT-4o" scheme="http://vra.github.io/tags/GPT-4o/"/>
    
    <category term="LVM" scheme="http://vra.github.io/tags/LVM/"/>
    
    <category term="Qwen" scheme="http://vra.github.io/tags/Qwen/"/>
    
    <category term="Qwen2-VL" scheme="http://vra.github.io/tags/Qwen2-VL/"/>
    
  </entry>
  
  <entry>
    <title>谷歌Gemini和Gemma大模型的Python调用</title>
    <link href="http://vra.github.io/2024/08/29/gemini-python-api/"/>
    <id>http://vra.github.io/2024/08/29/gemini-python-api/</id>
    <published>2024-08-29T00:30:32.000Z</published>
    <updated>2024-10-23T00:46:41.977Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-说明"><a href="#1-说明" class="headerlink" title="1. 说明"></a>1. 说明</h2><p>Google 发布了Python 包google-generativeai，可以方便地调用Gemini和Gemma 系列的模型，免费模型只需要申请一个Key，无需任何费用。<br><img data-src="/imgs/gemini-python-api/gemini-1.png"></p><p>而且Gemini 1.5 Pro模型还支持一些多模态任务，例如检测bbox，实际测试下来效果还不错。<br>这里简单写一个流程，体验效果。</p><span id="more"></span><h2 id="2-key获取与包安装"><a href="#2-key获取与包安装" class="headerlink" title="2. key获取与包安装"></a>2. key获取与包安装</h2><p>访问Google AIStudio 来进行Key注册：<a href="https://aistudio.google.com/app/prompts/new_chat">Google AI Studio</a><br>Python包安装:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U google-generativeai </span><br></pre></td></tr></table></figure><h2 id="3-文本输入"><a href="#3-文本输入" class="headerlink" title="3. 文本输入"></a>3. 文本输入</h2><p>简单使用大模型的对话能力，例如讲一个鬼故事：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip install -U google-generativeai</span></span><br><span class="line"><span class="keyword">import</span> google.generativeai <span class="keyword">as</span> genai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> PIL.Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># obtain your key at https://aistudio.google.com/</span></span><br><span class="line">genai.configure(api_key=os.environ[<span class="string">&quot;GOOGLE_API_KEY&quot;</span>])</span><br><span class="line">model = genai.GenerativeModel(<span class="string">&#x27;gemini-1.0-pro-latest&#x27;</span>)</span><br><span class="line">response = model.generate_content(<span class="string">&quot;讲一个鬼故事&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure><p>输出结果:<br><img data-src="/imgs/gemini-python-api/gemini-2.png"></p><p>最后一句有点惊悚…</p><h2 id="4-多模态输入"><a href="#4-多模态输入" class="headerlink" title="4. 多模态输入"></a>4. 多模态输入</h2><p>随便找了一张跳舞的人的图片，测试一下人体框检测效果，这里使用Gemini-1.5-pro来多模态检测人体框：</p><p>prompt如下：’Return bounding boxes of the <object>, in the format of [ymin, xmin, ymax, xmax] </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pip install -U google-generativeai</span></span><br><span class="line"><span class="keyword">import</span> google.generativeai <span class="keyword">as</span> genai</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> PIL.Image</span><br><span class="line"></span><br><span class="line"><span class="comment"># obtain your key at https://aistudio.google.com/</span></span><br><span class="line">genai.configure(api_key=os.environ[<span class="string">&quot;GOOGLE_API_KEY&quot;</span>])</span><br><span class="line">model = genai.GenerativeModel(<span class="string">&#x27;gemini-1.5-pro-latest&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output bbox</span></span><br><span class="line">img = PIL.Image.<span class="built_in">open</span>(<span class="string">&quot;dancer.jpg&quot;</span>)</span><br><span class="line">prompt = <span class="string">&#x27;Return bounding boxes of the dancer, in the format of [ymin, xmin, ymax, xmax]&#x27;</span></span><br><span class="line">response = model.generate_content([img, prompt])</span><br><span class="line"><span class="built_in">print</span>(response.text)</span><br></pre></td></tr></table></figure><p>检测结果:<br><img data-src="/imgs/gemini-python-api/gemini-3.png"></p><h2 id="5-参考"><a href="#5-参考" class="headerlink" title="5. 参考"></a>5. 参考</h2><ol><li><a href="https://pypi.org/project/google-generativeai/">google-generativeai · PyPI</a></li><li><a href="https://simonwillison.net/2024/Aug/26/gemini-bounding-box-visualization/">Building a tool showing how Gemini Pro can return bounding boxes for objects in images (simonwillison.net)</a></li><li><a href="https://ai.google.dev/gemini-api/docs/vision?lang=python#bbox">Explore vision capabilities with the Gemini API  |  Google AI for Developers</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-说明&quot;&gt;&lt;a href=&quot;#1-说明&quot; class=&quot;headerlink&quot; title=&quot;1. 说明&quot;&gt;&lt;/a&gt;1. 说明&lt;/h2&gt;&lt;p&gt;Google 发布了Python 包google-generativeai，可以方便地调用Gemini和Gemma 系列的模型，免费模型只需要申请一个Key，无需任何费用。&lt;br&gt;&lt;img data-src=&quot;/imgs/gemini-python-api/gemini-1.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;而且Gemini 1.5 Pro模型还支持一些多模态任务，例如检测bbox，实际测试下来效果还不错。&lt;br&gt;这里简单写一个流程，体验效果。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="Deep Learning" scheme="http://vra.github.io/tags/Deep-Learning/"/>
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="Google" scheme="http://vra.github.io/tags/Google/"/>
    
    <category term="Gemini" scheme="http://vra.github.io/tags/Gemini/"/>
    
    <category term="Gemma" scheme="http://vra.github.io/tags/Gemma/"/>
    
  </entry>
  
  <entry>
    <title>GPT实用功能之润色README</title>
    <link href="http://vra.github.io/2024/07/28/gpt-write-readme/"/>
    <id>http://vra.github.io/2024/07/28/gpt-write-readme/</id>
    <published>2024-07-28T00:18:25.000Z</published>
    <updated>2024-10-23T00:43:00.771Z</updated>
    
    <content type="html"><![CDATA[<p>今天发现GPT-4o对于GitHub Readme文档的润色还是很不错的，很自动添加一些花里胡哨的功能，看起来很fancy。</p><span id="more"></span><p>具体的提示词如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">将下面github 仓库的readme进行细化重写，添加更多fancy的标记和文本，使得更加详细和吸引人</span><br><span class="line">实际测试效果对比：</span><br></pre></td></tr></table></figure><p>润色前：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># mimic<span class="emphasis">_head </span></span></span><br><span class="line"><span class="emphasis"><span class="section">Unofficial One-click Version of LivePortrait, with Webcam Support</span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">## Features</span></span></span><br><span class="line"><span class="emphasis"><span class="section">+ with webcam, video and single image support</span></span></span><br><span class="line"><span class="emphasis"><span class="section">+ with cpu, mps and cuda backend support, you can run it without Nvidia GPU!</span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">## Screenshot</span></span></span><br><span class="line"><span class="emphasis"><span class="section">+ Image mode:</span></span></span><br><span class="line"><span class="emphasis"><span class="section">![<span class="string">mimic_head image mode screenshot</span>](<span class="link">assets/imgs/img_demo.jpg</span>)</span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">+ Video mode:</span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">https://github.com/user-attachments/assets/1aef9ae6-7d05-4fea-a03c-2c3de76df8b1</span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">+ Webcam mode:</span></span></span><br><span class="line"><span class="emphasis"><span class="section">NOTE: FPS ~= 13 on my mac laptop and there is observable delay in this video</span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">https://github.com/user-attachments/assets/6a2ce4c5-e3f2-40cd-9fe9-c081407aaca1</span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">## Install and use</span></span></span><br><span class="line"><span class="emphasis"><span class="section">```bash</span></span></span><br><span class="line"><span class="emphasis"><span class="section">pip install mimic_</span>head </span></span><br><span class="line">mimic<span class="emphasis">_head run</span></span><br><span class="line"><span class="emphasis">```</span></span><br></pre></td></tr></table></figure><p>润色后：</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="section"># 🎭 mimic<span class="emphasis">_head </span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">![<span class="string">Project Image</span>](<span class="link">https://img.shields.io/badge/Maintained-yes-green.svg</span>)</span></span></span><br><span class="line"><span class="emphasis"><span class="section">![<span class="string">License: MIT</span>](<span class="link">https://img.shields.io/github/license/vra/mimic_head</span>)</span></span></span><br><span class="line"><span class="emphasis"><span class="section">![<span class="string">GitHub release (latest by date)</span>](<span class="link">https://img.shields.io/github/v/release/vra/mimic_head</span>)</span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">🚀 <span class="strong">**Unofficial One-click Version of LivePortrait**</span> with Webcam Support!</span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">## 🌟 Features</span></span></span><br><span class="line"><span class="emphasis"><span class="section">- 📷 <span class="strong">**Webcam, Video, and Single Image Support**</span>:</span></span></span><br><span class="line"><span class="emphasis"><span class="section">  - Easily switch between different input modes to suit your needs.</span></span></span><br><span class="line"><span class="emphasis"><span class="section">- 🖥️ <span class="strong">**CPU, MPS, and CUDA Backend Support**</span>:</span></span></span><br><span class="line"><span class="emphasis"><span class="section">  - Run seamlessly without needing an Nvidia GPU!</span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">## 📸 Screenshot</span></span></span><br><span class="line"><span class="emphasis"><span class="section">### Image Mode:</span></span></span><br><span class="line"><span class="emphasis"><span class="section">![<span class="string">mimic_head image mode screenshot</span>](<span class="link">assets/imgs/img_demo.jpg</span>)</span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">### Video Mode:</span></span></span><br><span class="line"><span class="emphasis"><span class="section">https://github.com/user-attachments/assets/1aef9ae6-7d05-4fea-a03c-2c3de76df8b1</span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">### Webcam Mode:</span></span></span><br><span class="line"><span class="emphasis"><span class="section"><span class="strong">**Note: FPS ~ 13 on a Mac laptop with noticeable delay.**</span></span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">https://github.com/user-attachments/assets/6a2ce4c5-e3f2-40cd-9fe9-c081407aaca1</span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">## 🚀 Getting Started</span></span></span><br><span class="line"><span class="emphasis"><span class="section"></span></span></span><br><span class="line"><span class="emphasis"><span class="section">### 📦 Installation</span></span></span><br><span class="line"><span class="emphasis"><span class="section">To install and use `mimic_</span>head`, simply run the following command:</span></span><br><span class="line"></span><br><span class="line"><span class="code">```bash</span></span><br><span class="line"><span class="code">pip install mimic_head</span></span><br><span class="line"><span class="code">```</span></span><br><span class="line"></span><br><span class="line"><span class="section">### 🛠️ Usage</span></span><br><span class="line">Once installed, you can start the application by running:</span><br><span class="line"></span><br><span class="line"><span class="code">```bash</span></span><br><span class="line"><span class="code">mimic_head run</span></span><br><span class="line"><span class="code">```</span></span><br><span class="line"></span><br><span class="line"><span class="section">## 📚 Documentation</span></span><br><span class="line"></span><br><span class="line">For detailed instructions and advanced usage, please refer to our [<span class="string">README</span>](<span class="link">https://github.com/vra/mimic_head</span>).</span><br><span class="line"></span><br><span class="line"><span class="section">## 🤝 Contributing</span></span><br><span class="line">We welcome contributions! If you&#x27;d like to contribute, please fork the repository and use a feature branch. Pull requests are warmly welcomed.</span><br><span class="line"></span><br><span class="line"><span class="bullet">1.</span> Fork the Project</span><br><span class="line"><span class="bullet">2.</span> Create your Feature Branch (<span class="code">`git checkout -b feature/AmazingFeature`</span>)</span><br><span class="line"><span class="bullet">3.</span> Commit your Changes (<span class="code">`git commit -m &#x27;Add some AmazingFeature&#x27;`</span>)</span><br><span class="line"><span class="bullet">4.</span> Push to the Branch (<span class="code">`git push origin feature/AmazingFeature`</span>)</span><br><span class="line"><span class="bullet">5.</span> Open a Pull Request</span><br><span class="line"></span><br><span class="line"><span class="section">## 🛡️ License</span></span><br><span class="line">This project is licensed under the MIT License - see the [<span class="string">LICENSE</span>](<span class="link">LICENSE</span>) file for details.</span><br><span class="line"></span><br><span class="line"><span class="section">## 💬 Contact</span></span><br><span class="line">For any inquiries, questions, or issues, please open an issue in this repository or contact me at <span class="xml">&lt;wyf.brz@gmail.com&gt;</span>.</span><br><span class="line"></span><br><span class="line"><span class="section">## 📝 Acknowledgments</span></span><br><span class="line"><span class="bullet">-</span> Special thanks to the original creators of LivePortrait for their work.</span><br><span class="line"><span class="bullet">-</span> Inspired by the amazing community contributions and ideas.</span><br><span class="line"></span><br><span class="line"><span class="section">## ⭐ Support</span></span><br><span class="line">If you like this project, please give it a ⭐ on [<span class="string">GitHub</span>](<span class="link">https://github.com/vra/mimic_head</span>)!</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">Made with ❤️ by [<span class="string">Yunfeng Wang</span>](<span class="link">https://github.com/vra</span>).</span><br></pre></td></tr></table></figure><p>可以看到，自动添加了：</p><ul><li>项目徽章：添加了一些项目徽章（例如维护状态和许可证），使得README.md看起来更专业。</li><li>标题和说明：使用表情符号和强调文本使标题和说明更具吸引力。</li><li>Features：详细描述了项目的主要功能，并添加了适当的表情符号来增强视觉效果。</li><li>Screenshot：各个模式下的截图分别展示，并链接到对应的视频。</li><li>Getting Started：以更加详细和有条理的方式提供安装和使用说明。</li><li>Documentation：提供了指向详细文档的链接。</li><li>Contributing：提供了详细的贡献指南，鼓励用户参与。</li><li>License：明确项目的许可证信息。</li><li>Contact：提供联系信息。</li><li>Acknowledgments：感谢原始创作者和社区对项目的贡献。</li><li>Support：鼓励用户给项目打星。</li></ul><p>看上去专业了很多，算是很实用的工具了。</p><pre><code></code></pre>]]></content>
    
    
    <summary type="html">&lt;p&gt;今天发现GPT-4o对于GitHub Readme文档的润色还是很不错的，很自动添加一些花里胡哨的功能，看起来很fancy。&lt;/p&gt;</summary>
    
    
    
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="GPT" scheme="http://vra.github.io/tags/GPT/"/>
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="GPT-4o" scheme="http://vra.github.io/tags/GPT-4o/"/>
    
    <category term="README" scheme="http://vra.github.io/tags/README/"/>
    
    <category term="Markdown" scheme="http://vra.github.io/tags/Markdown/"/>
    
  </entry>
  
  <entry>
    <title>mimic-head-实时摄像头驱动图片动起来</title>
    <link href="http://vra.github.io/2024/07/13/mimic-head/"/>
    <id>http://vra.github.io/2024/07/13/mimic-head/</id>
    <published>2024-07-13T00:08:12.000Z</published>
    <updated>2024-10-23T00:28:53.920Z</updated>
    
    <content type="html"><![CDATA[<p>整了一个快手人头驱动项目<a href="https://github.com/KwaiVGI/LivePortrait">LivePortrait</a>的demo，一键安装（自动下载模型），同时增加了官方demo中没有的实时摄像头驱动，也支持cpu和mps这两个后端了。</p><span id="more"></span><p>安装超easy:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install mimic_head</span><br></pre></td></tr></table></figure><p>使用超easy:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mimic_head run</span><br></pre></td></tr></table></figure><p>打开浏览器访问127.0.0.1:7860就可以开始玩了。</p><p>摄像头驱动效果在<a href="https://zhuanlan.zhihu.com/p/708618764">这里</a></p><p>不得不说，快手这个效果真的牛，太好玩了。</p><p>源码：<a href="https://github.com/vra/mimic_head">https://github.com/vra/mimic_head</a></p><p>欢迎star，fork and 魔改。</p><p>Have fun!</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;整了一个快手人头驱动项目&lt;a href=&quot;https://github.com/KwaiVGI/LivePortrait&quot;&gt;LivePortrait&lt;/a&gt;的demo，一键安装（自动下载模型），同时增加了官方demo中没有的实时摄像头驱动，也支持cpu和mps这两个后端了。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="pip" scheme="http://vra.github.io/tags/pip/"/>
    
    <category term="Deep Learning" scheme="http://vra.github.io/tags/Deep-Learning/"/>
    
    <category term="LivePortrait" scheme="http://vra.github.io/tags/LivePortrait/"/>
    
    <category term="快手" scheme="http://vra.github.io/tags/%E5%BF%AB%E6%89%8B/"/>
    
    <category term="Kwai" scheme="http://vra.github.io/tags/Kwai/"/>
    
  </entry>
  
  <entry>
    <title>uv-速度飞快的pip替代</title>
    <link href="http://vra.github.io/2024/03/31/uv-tutorial1/"/>
    <id>http://vra.github.io/2024/03/31/uv-tutorial1/</id>
    <published>2024-03-31T00:03:13.000Z</published>
    <updated>2024-10-23T00:07:04.160Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-uv是什么"><a href="#1-uv是什么" class="headerlink" title="1. uv是什么"></a>1. uv是什么</h2><p><a href="https://github.com/astral-sh/uv">uv</a>是开发ruff的公司 Astral 前一段时间发布的高性能Python工具，用途是安装python包，以及解析包版本之间的依赖。它的最大特点是快，相比现有的的工具都能够快一大截（如下图），<br>![[Pasted image 20240329074004.png]]</p><p>发布uv的愿景，是希望构造类似Rust的cargo，快速、可依赖，易用的包管理工具。</p><p>通过在不同的系统进行几个常见包的测试，uv相比pip，加速比在1～13之间，因此是一个值得一试的工具。</p><p>下面我先介绍一下uv的安装和使用，然后从一个普通用户使用pip的标准流程，尝试用uv替代pip，进行Windows, Linux 和macOS上实测速度对比，最后对uv发展的现状做一个说明，以及我的一些看法。</p><span id="more"></span><h2 id="2-uv安装与使用"><a href="#2-uv安装与使用" class="headerlink" title="2. uv安装与使用"></a>2. uv安装与使用</h2><h3 id="2-1-安装"><a href="#2-1-安装" class="headerlink" title="2.1 安装"></a>2.1 安装</h3><p>可以用pip来安装uv：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install uv</span><br></pre></td></tr></table></figure><p>我认为这是安装uv最简单最通用的方式，基本上适用于所有Python场景。即使是在venv环境中安装的，uv也会复制自己的可执行文件也会被复制到系统的PATH目录中，保证退出或切换虚拟环境后，uv命令依然能够正常使用。</p><p>uv还支持别的很多种安装方式，这里也列出来供参考：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 直接下载安装脚本，支持 macOS和Linux.</span></span><br><span class="line">curl -LsSf https://astral.sh/uv/install.sh | sh</span><br><span class="line"></span><br><span class="line"><span class="comment"># On Windows.</span></span><br><span class="line">powershell -c <span class="string">&quot;irm https://astral.sh/uv/install.ps1 | iex&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># With pipx.</span></span><br><span class="line">pipx install uv</span><br><span class="line"></span><br><span class="line"><span class="comment"># With Homebrew.</span></span><br><span class="line">brew install uv</span><br><span class="line"></span><br><span class="line"><span class="comment"># With Pacman.</span></span><br><span class="line">pacman -S uv</span><br></pre></td></tr></table></figure><p>不过需要注意一个问题：像apt、brew这些包管理器中的uv可能不是最新的，而旧版本的uv可能会有潜在的问题。</p><p>例如我用brew安装的uv 0.1.8版本在安装tensorflow时会卡住并超时，报下面的错误：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">error: Failed to download distributions</span><br><span class="line">  Caused by: Failed to fetch wheel: grpcio==1.62.1</span><br><span class="line">  Caused by: Failed to extract <span class="built_in">source</span> distribution</span><br><span class="line">  Caused by: request or response body error: operation timed out</span><br><span class="line">  Caused by: operation timed out</span><br></pre></td></tr></table></figure><p>如果出现这个错误，试试更新uv到最新版，并建议用pip来安装uv。</p><h3 id="2-2-uv-help-查看帮助"><a href="#2-2-uv-help-查看帮助" class="headerlink" title="2.2 uv help-查看帮助"></a>2.2 uv help-查看帮助</h3><p>在安装好uv后，就可以一步步地开始uv命令的探索。uv的命令不算多，而且有比较好的命令说明，如果想详细了解uv的所有命令和子命令以及命令行参数，可以按照下面的命令来依次探索：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">uv --<span class="built_in">help</span></span><br><span class="line">uv pip --<span class="built_in">help</span></span><br><span class="line">uv pip install --<span class="built_in">help</span></span><br></pre></td></tr></table></figure><p>下面我将比较重要的uv命令进行列举，并做简单的解释。</p><h3 id="2-3-uv-venv-创建环境"><a href="#2-3-uv-venv-创建环境" class="headerlink" title="2.3 uv venv-创建环境"></a>2.3 uv venv-创建环境</h3><p>创建环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建虚拟环境，不加环境路径的话默认是保存在当前的.venv目录下</span></span><br><span class="line">uv venv </span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定环境保存目录</span></span><br><span class="line">uv venv /path/to/venv</span><br><span class="line"></span><br><span class="line"><span class="comment"># 指定Python版本，注意需要对应版本的Python已经安装</span></span><br><span class="line">uv venv -p 3.12</span><br><span class="line"></span><br><span class="line"><span class="comment"># --python 同 -p</span></span><br><span class="line">uv venv --python 3.12</span><br></pre></td></tr></table></figure><p>注意：uv工具不会自动下载Python包，因此如果设置<code>-p</code>时指定系统不存在的Python版本，则会报下面的错：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ uv venv -p 3.13</span><br><span class="line">No Python 3.13 In `PATH`. Is Python 3.13 installed?</span><br></pre></td></tr></table></figure><p>启用环境的命令同Python的标准库venv:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Unix</span></span><br><span class="line"><span class="built_in">source</span> venv/bin/activate</span><br><span class="line"></span><br><span class="line"><span class="comment"># Windows</span></span><br><span class="line">venv\Scripts\activate</span><br></pre></td></tr></table></figure><h3 id="2-4-uv-pip-install-安装包"><a href="#2-4-uv-pip-install-安装包" class="headerlink" title="2.4  uv pip install-安装包"></a>2.4  uv pip install-安装包</h3><p>安装包的命令是<code>uv pip install</code>，很好记，在普通的<code>pip install</code> 前面加一个uv，而且大部分<code>pip install</code> 的参数都支持：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从 pypi上安装包，默认安装最新版本</span></span><br><span class="line">uv pip install flask</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从镜像网站上拉取安装包</span></span><br><span class="line">uv pip install flask -i https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新包版本</span></span><br><span class="line">uv pip install -U flask</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装特定版本的包：</span></span><br><span class="line">uv pip install -U flask==3.0.0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从当前目录安装</span></span><br><span class="line">uv pip install .</span><br><span class="line"></span><br><span class="line"><span class="comment"># 从当前目录安装，并且支持editable实时更新代码模式</span></span><br><span class="line">uv pip install -e .</span><br></pre></td></tr></table></figure><p>一个非常重要的点：uv 默认不会读<code>pip.conf</code>这种类型的镜像配置，因此在国内的话，包的默认下载速度是比较慢的，需要手动加<code>--index-url/-i</code>和<code>-extra-index-url</code>，才能达到比较快的下载速度。</p><p>卸载包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv pip uninstall flask</span><br></pre></td></tr></table></figure><p>注意：与<code>pip</code>不同，<code>uv pip uninstall</code>时默认不会让你再确认一遍。</p><h3 id="2-5-uv-pip-compile-查看包依赖"><a href="#2-5-uv-pip-compile-查看包依赖" class="headerlink" title="2.5 uv pip compile-查看包依赖"></a>2.5 uv pip compile-查看包依赖</h3><p><code>uv pip compile</code> 可以将pip-tools工作流中的<code>requirements.in</code>格式的没有精确依赖库版本的文件转换为包含精确依赖库版本<code>requirements.txt</code>的工具，也可以处理任意包含python包的txt文件，比如我们有下面的文件<code>my_packages.txt</code>：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">flask</span><br><span class="line">six</span><br></pre></td></tr></table></figure><p>利用<code>uv pip compile</code>就能得到精确的版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv pip compile my_packages.txt</span><br></pre></td></tr></table></figure><p>注意不需要安装<code>my_packages.txt</code>中的包，也就是说，我们可以将任意的python包列在<code>my_packages.txt</code>中，来查看安装他们需要依赖哪些库。<br>举个好玩的例子，试试安装<a href="https://pypistats.org/top">下载量前20的python包</a>都会有哪些依赖：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">boto3</span><br><span class="line">botocore</span><br><span class="line">urllib3</span><br><span class="line">requests</span><br><span class="line">wheel</span><br><span class="line">certifi</span><br><span class="line">typing-extensions</span><br><span class="line">charset-normalizer</span><br><span class="line">setuptools</span><br><span class="line">idna</span><br><span class="line">pip</span><br><span class="line">python-dateutil</span><br><span class="line">packaging</span><br><span class="line">s3transfer</span><br><span class="line">aiobotocore</span><br><span class="line">six</span><br><span class="line">pyyaml</span><br><span class="line">s3fs</span><br><span class="line">numpy</span><br><span class="line">cryptography</span><br></pre></td></tr></table></figure><p>将结果写入到文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv pip compile --no-annotate my_packages.txt -o requirements.txt</span><br></pre></td></tr></table></figure><p>输出<code>requirements.txt</code>内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">aiobotocore==2.12.1</span><br><span class="line">aiohttp==3.9.3</span><br><span class="line">aioitertools==0.11.0</span><br><span class="line">aiosignal==1.3.1</span><br><span class="line">attrs==23.2.0</span><br><span class="line">boto3==1.34.51</span><br><span class="line">botocore==1.34.51</span><br><span class="line">certifi==2024.2.2</span><br><span class="line">cffi==1.16.0</span><br><span class="line">charset-normalizer==3.3.2</span><br><span class="line">cryptography==42.0.5</span><br><span class="line">frozenlist==1.4.1</span><br><span class="line">fsspec==2024.3.1</span><br><span class="line">idna==3.6</span><br><span class="line">jmespath==1.0.1</span><br><span class="line">multidict==6.0.5</span><br><span class="line">numpy==1.26.4</span><br><span class="line">packaging==24.0</span><br><span class="line">pip==24.0</span><br><span class="line">pycparser==2.22</span><br><span class="line">python-dateutil==2.9.0.post0</span><br><span class="line">pyyaml==6.0.1</span><br><span class="line">requests==2.31.0</span><br><span class="line">s3fs==2024.3.1</span><br><span class="line">s3transfer==0.10.1</span><br><span class="line">setuptools==69.2.0</span><br><span class="line">six==1.16.0</span><br><span class="line">typing-extensions==4.10.0</span><br><span class="line">urllib3==2.0.7</span><br><span class="line">wheel==0.43.0</span><br><span class="line">wrapt==1.16.0</span><br><span class="line">yarl==1.9.4</span><br></pre></td></tr></table></figure><p>32个依赖，也就是说安装下载量前20的Python包，包括它们自己，只需要安装32个包。</p><p>可以通过<code>echo &lt;package_name&gt;| uv pip compile -</code> 的方式查找某个包的依赖。<br>我们来看看安装<code>tensorflow</code>需要哪些依赖：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> tensorflow | uv pip compile --no-annotate -</span><br></pre></td></tr></table></figure><p>就会生成下面的输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">absl-py==2.1.0</span><br><span class="line">astunparse==1.6.3</span><br><span class="line">certifi==2024.2.2</span><br><span class="line">charset-normalizer==3.3.2</span><br><span class="line">flatbuffers==24.3.25</span><br><span class="line">gast==0.5.4</span><br><span class="line">google-pasta==0.2.0</span><br><span class="line">grpcio==1.62.1</span><br><span class="line">h5py==3.10.0</span><br><span class="line">idna==3.6</span><br><span class="line">keras==3.1.1</span><br><span class="line">libclang==18.1.1</span><br><span class="line">markdown==3.6</span><br><span class="line">markdown-it-py==3.0.0</span><br><span class="line">markupsafe==2.1.5</span><br><span class="line">mdurl==0.1.2</span><br><span class="line">ml-dtypes==0.3.2</span><br><span class="line">namex==0.0.7</span><br><span class="line">numpy==1.26.4</span><br><span class="line">opt-einsum==3.3.0</span><br><span class="line">optree==0.11.0</span><br><span class="line">packaging==24.0</span><br><span class="line">protobuf==4.25.3</span><br><span class="line">pygments==2.17.2</span><br><span class="line">requests==2.31.0</span><br><span class="line">rich==13.7.1</span><br><span class="line">setuptools==69.2.0</span><br><span class="line">six==1.16.0</span><br><span class="line">tensorboard==2.16.2</span><br><span class="line">tensorboard-data-server==0.7.2</span><br><span class="line">tensorflow==2.16.1</span><br><span class="line">tensorflow-io-gcs-filesystem==0.36.0</span><br><span class="line">termcolor==2.4.0</span><br><span class="line">typing-extensions==4.10.0</span><br><span class="line">urllib3==2.0.7</span><br><span class="line">werkzeug==3.0.1</span><br><span class="line">wheel==0.43.0</span><br><span class="line">wrapt==1.16.0</span><br></pre></td></tr></table></figure><p>包含38个依赖，比下载量前20的包的总的依赖还要多……</p><h3 id="2-6-uv-pip-sync-更新当前环境的包版本"><a href="#2-6-uv-pip-sync-更新当前环境的包版本" class="headerlink" title="2.6 uv pip sync-更新当前环境的包版本"></a>2.6 uv pip sync-更新当前环境的包版本</h3><p>利用<code>uv pip compile</code>，可以方便地将当前环境所有安装的包以及它们的依赖的版本都导出到requirements.txt中，然后在别的机器上快速复现同样的安装环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv pip freeze |uv pip compile - -o requirements.txt</span><br></pre></td></tr></table></figure><p>拿到<code>requirements.txt</code>后，就可以用<code>uv pip sync</code>命令来将其中的版本信息更新到当前的<br>虚拟环境：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">uv pip sync requirements.txt</span><br></pre></td></tr></table></figure><p>但需要注意一点，uv的requirements.txt并不是跨平台的，也就是Windows上的requirements.txt并不适用于Linux环境，反之亦然。</p><p>例如，同样是<code>tensorflow==2.16.1</code>版本，macOS和Linux的依赖库就有2个不同(macOS vs Linux)：</p><figure class="highlight diff"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> tensorboard==2.16.2</span><br><span class="line"> tensorboard-data-server==0.7.2</span><br><span class="line"> tensorflow==2.16.1</span><br><span class="line"><span class="deletion">-tensorflow-io-gcs-filesystem==0.36.0</span></span><br><span class="line"> termcolor==2.4.0</span><br><span class="line"> typing-extensions==4.10.0</span><br><span class="line"><span class="deletion">-urllib3==2.0.7</span></span><br><span class="line"><span class="addition">+urllib3==2.2.1</span></span><br><span class="line"> werkzeug==3.0.1</span><br><span class="line"> wheel==0.43.0</span><br><span class="line"> wrapt==1.16.0</span><br></pre></td></tr></table></figure><p>因此最好还是在相同的操作系统之间执行<code>uv pip sync</code>，不同操作系统之间可能需要手动修改<code>requirements.txt</code>。</p><h4 id="2-7-uv-cache-缓存"><a href="#2-7-uv-cache-缓存" class="headerlink" title="2.7 uv cache-缓存"></a>2.7 uv cache-缓存</h4><p>uv有一个顶级命令<code>uv cache</code>，用于cache的管理。</p><p>首先类似<code>pip cache dir</code> ，uv也有一个cache dir命令来查看缓存目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ uv cache dir</span><br><span class="line">/home/gitpod/.cache/uv</span><br></pre></td></tr></table></figure><p>注意不同系统的默认cache目录是不同的，我的观察是：</p><ul><li>Linux: <code>$HOME/.cache/uv</code></li><li>macOS: <code>/Users/&lt;user&gt;/Library/Caches/uv</code></li><li>Windows: <code>C:/Users/&lt;user&gt;/AppData/Local/uv/cache</code> </li></ul><p>当然是可以修改cache目录的，指定<code>UV_CACHE_DIR</code> 环境变量就可以。</p><p>然后可以用<code>uv cache prune</code> 清除没有用到的缓存数据，比如删除包后，可以用此命令来清除空间。</p><p>最后可以彻底地删除cache，命令为<code>uv cache clean</code>，整个cache目录都会被清除掉：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ uv cache clean</span><br><span class="line">Clearing cache at: /home/gitpod/.cache/uv</span><br><span class="line">Removed 78 files (16.7MiB)</span><br></pre></td></tr></table></figure><h3 id="2-8-环境变量"><a href="#2-8-环境变量" class="headerlink" title="2.8 环境变量"></a>2.8 环境变量</h3><p>UV支持一些环境变量的设置，例如缓存目录，index-url等，常见的包括下面这些，这些环境变量可以临时使用，不过建议时加入到你的shell到配置文件，就不用每次都敲一遍。可以复制下面的代码到<code>.bashrc</code>中然后修改对应的变量</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 缓存目录</span></span><br><span class="line"><span class="built_in">export</span> UV_CACHE_DIR=/path/to/cache/dir</span><br><span class="line"></span><br><span class="line"><span class="comment"># 镜像地址</span></span><br><span class="line"><span class="built_in">export</span> UV_INDEX_URL=https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br><span class="line"><span class="comment"># 额外镜像地址</span></span><br><span class="line"><span class="built_in">export</span> EXTRA_INDEX_URL=https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不用缓存</span></span><br><span class="line"><span class="built_in">export</span> UV_NO_CACHE=0</span><br><span class="line"></span><br><span class="line"><span class="comment"># 下载包时的超时时间，单位为秒</span></span><br><span class="line">UV_HTTP_TIMEOUT=60</span><br></pre></td></tr></table></figure><h2 id="3-uv-速度测试"><a href="#3-uv-速度测试" class="headerlink" title="3. uv 速度测试"></a>3. uv 速度测试</h2><p>为了测试uv是否能加速python包的安装，我在macOS，Linux和Windows上对uv和pip进行了速度对比，安装下面四个包：</p><ul><li>transformers</li><li>tensorflow</li><li>flask</li><li>numpy</li><li>pytorch</li></ul><p>测试系统和Python版本：</p><ul><li>macOS 14.2.1  + Python 3.12.2</li><li>Ubuntu 22.04 + Python 3.12.2</li><li>Windows 11 + Python 3.10.8</li></ul><p>测试流程如下：</p><ol><li>新建环境并启用</li><li>清除缓存，安装对应的包</li></ol><p>macOS和Linux下，用下面的命令进行测速：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">time (uv pip venv venv_1 &amp;&amp; <span class="built_in">source</span> venv_1/bin/activate &amp;&amp; uv pip install &lt;package&gt;)</span><br></pre></td></tr></table></figure><p>Windows下，用Powershell，用下面的命令测速：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Measure-Command &#123;python -m venv venv_1; venv_1\Scripts\activate; pip install &lt;package&gt;&#125;</span><br></pre></td></tr></table></figure><p>另外国内机器测速使用了清华Pypi源来进行加速。</p><p>对比结果如下：<br>可以看到，uv加速还是比较明显的，加速比在1～13倍之间。</p><p>也欢迎读者朋友在评论区提交你测试的加速比数据。</p><h2 id="4-uv的发展现状"><a href="#4-uv的发展现状" class="headerlink" title="4. uv的发展现状"></a>4. uv的发展现状</h2><p>我之前写过一篇介绍<a href="https://zhuanlan.zhihu.com/p/629989128">介绍类似工具Rye的文章</a>，其实注意到uv也是看到Rye的作者将Rye托管给了Astral 团队，而且Rye的作者还写了一篇<a href="https://lucumr.pocoo.org/2024/2/15/rye-grows-with-uv/">文章</a>，描述Rye的未来，以及为什么要让Astral托管Rye，以及最终Rye将会和uv融合，共同实现 “Cargo for Python”的愿景。</p><p>uv目前还在快速发展的阶段，从5个月前才开始<a href="https://github.com/astral-sh/uv/commit/d226e0a2cd7e275e1a0403a880e76db38b79eb67">开发</a>，<a href="https://astral.sh/blog/uv">开源</a>到现在1个多月，版本号还是<a href="https://github.com/astral-sh/uv/releases/tag/0.1.26">0.1.x</a>。</p><p>Python官方论坛也有关于uv的<a href="https://discuss.python.org/t/uv-another-rust-tool-written-to-replace-pip/46039/55">讨论</a>，大家觉得<code>uv pip</code>的命令太容易引起误解，作者也亲自回复了。未来<code>uv pip</code> 改成别的命令也不是不可能。</p><p>另外上面也提到了，目前<code>uv</code> 是不支持<code>pip.conf</code>这种配置的，GitHub上有人反馈以后，目前官方开始加入对镜像配置的支持，但实现貌似是一个比较复杂的版本，具体参见<a href="https://github.com/astral-sh/uv/issues/1404#issuecomment-2015778851">这个isuse</a>。</p><p>对于使用来说，鉴于uv还在开发比较早期的阶段（虽然使用体验起来已经很完善了），建议在自己的个人项目中尝试使用uv，大的生产项目再观察一段时候后再切换。</p><h2 id="5-我的看法"><a href="#5-我的看法" class="headerlink" title="5. 我的看法"></a>5. 我的看法</h2><p>作用Python用户，对于Python工具提速这件事情，总是值得激动一下的。通过几条简单的命令就能获取极大的提速，何乐而不为。只不过希望有一天这些第三方库都能被集成到标准库或标准流程中，不要再让工具库碎片化了。</p><p>目前来看，未来的一个大的方向是利用Rust来开发Python的工具链，帮助人们来更好地写Python代码。Python语言最大的优势是易用性和生态完善性，这个是目前Rust还没法替代Python的原因。未来Python的优势会继续保持下去，但包管理设计上工具太多，导致非常的混乱，借鉴Rust的经验来解决这个问题，是个好的方向。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-uv是什么&quot;&gt;&lt;a href=&quot;#1-uv是什么&quot; class=&quot;headerlink&quot; title=&quot;1. uv是什么&quot;&gt;&lt;/a&gt;1. uv是什么&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/astral-sh/uv&quot;&gt;uv&lt;/a&gt;是开发ruff的公司 Astral 前一段时间发布的高性能Python工具，用途是安装python包，以及解析包版本之间的依赖。它的最大特点是快，相比现有的的工具都能够快一大截（如下图），&lt;br&gt;![[Pasted image 20240329074004.png]]&lt;/p&gt;
&lt;p&gt;发布uv的愿景，是希望构造类似Rust的cargo，快速、可依赖，易用的包管理工具。&lt;/p&gt;
&lt;p&gt;通过在不同的系统进行几个常见包的测试，uv相比pip，加速比在1～13之间，因此是一个值得一试的工具。&lt;/p&gt;
&lt;p&gt;下面我先介绍一下uv的安装和使用，然后从一个普通用户使用pip的标准流程，尝试用uv替代pip，进行Windows, Linux 和macOS上实测速度对比，最后对uv发展的现状做一个说明，以及我的一些看法。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="pip" scheme="http://vra.github.io/tags/pip/"/>
    
    <category term="Rust" scheme="http://vra.github.io/tags/Rust/"/>
    
    <category term="uv" scheme="http://vra.github.io/tags/uv/"/>
    
    <category term="Astral" scheme="http://vra.github.io/tags/Astral/"/>
    
  </entry>
  
  <entry>
    <title>2023年终总结</title>
    <link href="http://vra.github.io/2023/12/31/summary-2023/"/>
    <id>http://vra.github.io/2023/12/31/summary-2023/</id>
    <published>2023-12-31T11:43:26.000Z</published>
    <updated>2024-01-01T14:14:29.000Z</updated>
    
    <content type="html"><![CDATA[<p>2023年对我来说是一个惊喜的年份，因为可爱的女儿降生了。也是一个难言的年份，在零基础学带娃+长途通勤+家庭矛盾+工作压力的组合作用下，时常burnout，切身体会到人到中年的不容易。好在娃娃的每一个笑容都如此治愈，陪我度过艰难的2023。</p><h2 id="技术"><a href="#技术" class="headerlink" title="技术"></a>技术</h2><p>技术上，这一年开始担任组内一些项目的Owner，负责与外部团队对接。对于之前习惯做单点技术的我来说，还是个不小的挑战，在小组内沟通、任务规划与拆解、按期交付等方面都需要改进。</p><p>在开源项目上，主要做了两个项目。</p><p>一个是周刊类的项目 <a href="https://vra.github.io/weekly-posts/">weekly-post</a>，记录我每周看到的一些技术文章，希望给国内的技术同行们一些信息来源和灵感启发。不过在年中的时候断更之后再没更新。反思了下， 本身没有做中文翻译，且只有GitHub一个途径，因此触达的用户不多，反馈也少，很容易坚持不下去，未来或许还会继续尝试这种项目，参考潮流周刊等项目的经验。</p><p>另一个是语音聊天对话AI <a href="https://github.com/vra/talkGPT4All">talkGPT4All</a>，语音输入问题，GPT产生回复，再通过TTS合成声音。本身是一个简单的缝合项目，不过是实现了我长久以来一直想做的对话Bot的功能。未来考虑在手机上迁移，触达更多的普通人。当然这类App要做到真正好玩，还需要大量的开发工作。</p><p>别的还有一些小的AI工具，都发布到PyPI了，可以pip直接安装：</p><ul><li>bing_brush: DALLE-3图像生成工具</li><li>dinov2-retrieval: 基于DINO V2的图像检索工具</li><li>mp-face-stylizer: 基于MediaPipe的人脸风格化工具</li></ul><h2 id="生活"><a href="#生活" class="headerlink" title="生活"></a>生活</h2><p>2023年5月，女儿出生，这是过去一年最值得纪念的事情。女儿的到来给我们二人组近十年的二人生活带来了太多惊喜，爸妈也过来一起带娃，五人的家庭是全新的体验，有乐也有苦，总归是度过最难的时候了。</p><p>下面是这一年和身边的人的相聚，虽然相聚的机会不多，但每一次相聚都值得铭记：</p><ul><li>1月12日大团队年会。</li><li>1月14日去小营巷钱学森故居参观。</li><li>1月14和董政潇哥去刘旸家聚餐。</li><li>2月12日游黄龙洞和保俶塔。</li><li>2月18日和彤彤金沙湖春游。</li><li>4月8日带父母游西湖。</li><li>5月5日女儿出生。</li><li>5月14日东升和老婆来看王茗溪小朋友。</li><li>5月19日团队京城一锅聚餐。</li><li>6月21日，团队在华夏之心闻老头聚餐。</li><li>6月23日下午，张凯来看娃，带了好多水果还有孩子看的书。</li><li>7月2日参加何同学线下测试活动，见到了何同学本尊并合影。</li><li>8月11日参加淘天三年醇活动。</li><li>8月19日去净慈寺，尝素烧鹅，捐了48元一片瓦，内心愉悦。净慈美术馆《山中妙音》画展很不错。</li><li>9月20日带父母去临平体育中心看亚运会男排比赛。</li><li>国庆和彤彤带娃回家看彤彤爷爷。坐飞机到兰州，坐高铁去秦安，再打车回庄浪。返程先去咸阳，再坐飞机回杭。</li><li>10月19日团队疆小羊聚餐。</li><li>10月22日游飞来峰，韬光寺和永福寺。韬光寺第二次来，桂花还是谢了，半路买茶叶的老人还在。永福寺第一次去，里面很大。</li><li>10月31日下午和团队参加云栖大会。</li><li>11月10日晚，和刘旸，董政，杨珈蒙去嘉里中心吃了云南菜一坐一忘。</li><li>12月31日，和赵彤同事们一起去径山寺</li></ul><h2 id="书籍"><a href="#书籍" class="headerlink" title="书籍"></a>书籍</h2><p>看完的：</p><ul><li>刘少奇传</li><li>一百年，许多人，许多事：杨苡口述自传</li><li>朱德传</li><li>南京大屠杀</li><li>己亥杂诗</li><li>爱你的一万种方式</li></ul><p>在看的：</p><ul><li>植物的战斗</li><li>迷路员</li><li>我在北京送快递</li><li>创造：用非传统方式做有价值的事</li><li>史蒂夫乔布斯传</li><li>生活蒙太奇</li><li>荷花淀</li></ul><h2 id="电影和电视剧"><a href="#电影和电视剧" class="headerlink" title="电影和电视剧"></a>电影和电视剧</h2><ul><li>流浪地球2</li><li>拾荒者统治</li><li>中国奇谭(小妖怪给看哭了)</li><li>椒麻堂会</li><li>最后生还者第一季</li><li>过往人生</li><li>阿索卡</li><li>曼达洛人第三季</li><li>伯爵</li><li>我是格鲁特第二季<br>没看完的：</li><li>三体电视剧</li><li>五月十二月</li><li>银河护卫队3</li><li>奥本海默</li><li>星条红与皇室蓝</li><li>忠犬八公</li><li>流人第二季</li><li>蓝眼武士</li><li>万神殿第二季</li><li>公寓大楼里的谋杀案</li><li>足球教练</li><li>史前星球第二季</li></ul><h2 id="面向2024"><a href="#面向2024" class="headerlink" title="面向2024"></a>面向2024</h2><p>2024年，不奢望太多，孩子健康成长就好。</p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;2023年对我来说是一个惊喜的年份，因为可爱的女儿降生了。也是一个难言的年份，在零基础学带娃+长途通勤+家庭矛盾+工作压力的组合作用下，时常burnout，切身体会到人到中年的不容易。好在娃娃的每一个笑容都如此治愈，陪我度过艰难的2023。&lt;/p&gt;
&lt;h2 id=&quot;技术&quot;&gt;</summary>
      
    
    
    
    
    <category term="年终总结" scheme="http://vra.github.io/tags/%E5%B9%B4%E7%BB%88%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title>talkGPT4All 2.5-更多模型以及更加真实的TTS</title>
    <link href="http://vra.github.io/2023/11/22/talkgpt4all-2.5/"/>
    <id>http://vra.github.io/2023/11/22/talkgpt4all-2.5/</id>
    <published>2023-11-22T04:57:30.000Z</published>
    <updated>2023-12-09T12:22:48.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p><a href="https://link.zhihu.com/?target=https://github.com/vra/talkGPT4All">talkGPT4All</a>是基于<a href="https://link.zhihu.com/?target=https://gpt4all.io/index.html">GPT4All</a>的一个语音聊天程序，运行在本地CPU上，支持Linux，Mac和Windows。它利用OpenAI的Whisper模型将用户输入的语音转换为文本，再调用GPT4All的语言模型得到回答文本，最后利用文本转语音(TTS)的程序将回答文本朗读出来。</p><p>今年4、5月份的时候，我发布了talkGPT4All 1.0版本和2.0版本，链接见下：</p><p><a href="https://zhuanlan.zhihu.com/p/618826760">talkGPT4All: 基于GPT4All的智能语音聊天程序</a><br><a href="https://zhuanlan.zhihu.com/p/632592897">talkGPT4All 2.0:现在支持8个语言模型了</a></p><p>大家反馈最大的问题是TTS太机械了，听着很难受（具体可以看前面两篇文章的评论区）。而最近TTS领域的进展很多，例如很受欢迎的 coqui-ai的<a href="https://github.com/coqui-ai/TTS">TTS</a> 库，提供了TTS、声音克隆和声音变换的功能。上周末尝试了一下，发现内置了一些开箱即用的TTS模型，刚好可以集成到 talkGPT4All 中，解决目前采用的 <a href="https://pypi.org/project/pyttsx3/">pyttsx3</a>合成声音太机械的问题。</p><span id="more"></span><p>另外查看 GPT4All 的文档，从2.5.0开始，之前的.bin 格式的模型文件不再支持，只支持.gguf 格式的模型。因此我也是将上游仓库的更新合并进来，修改一下 talkGPT4All 的接口。</p><p>由于GPT4All 是从2.5.0开始不兼容.bin 格式老模型的，是一个很大的 break change。为了统一，我将更新后的 talkGPT4All 版本也命名为 2.5.0。</p><p>2.5.0版本效果视频见<a href="https://zhuanlan.zhihu.com/p/668275615">这里</a>。</p><h3 id="2-如何使用"><a href="#2-如何使用" class="headerlink" title="2. 如何使用"></a>2. 如何使用</h3><p>如果想直接使用的话，采用pip安装talkGPT4All包即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install talkgpt4all</span><br></pre></td></tr></table></figure><p>安装完后进入聊天：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">talkgpt4ll </span><br></pre></td></tr></table></figure><p>talkGPT4All 现在支持15个模型，可以通过-m 来切换你想用的GPT模型，所有模型列表见 3.2章节。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">talkgpt4all -m gpt4all-13b-snoozy-q4_0.gguf</span><br></pre></td></tr></table></figure><h3 id="3-实现细节"><a href="#3-实现细节" class="headerlink" title="3. 实现细节"></a>3. 实现细节</h3><p>这里重点讲一下此次更新中涉及到的两个点：coqui-ai/TTS如何使用以及GPT4All 2.5.0以后如何调用GPT模型。</p><h4 id="3-1-coqui-ai-TTS使用"><a href="#3-1-coqui-ai-TTS使用" class="headerlink" title="3.1 coqui-ai/TTS使用"></a>3.1 coqui-ai/TTS使用</h4><p>直接使用pip install TTS 即可安装 coqui-ai/TTS包，里面包含了很多功能，这里只简单展示如何调用一个现有的TTS模型。</p><p>首先列出所有的TTS模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> TTS.api <span class="keyword">import</span> TTS</span><br><span class="line"><span class="built_in">print</span>(TTS().list_models()) </span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;tts_models/multilingual/multi-dataset/xtts_v2&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/multilingual/multi-dataset/xtts_v1.1&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/multilingual/multi-dataset/your_tts&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/multilingual/multi-dataset/bark&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/bg/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/cs/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/da/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/et/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/ga/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ek1/tacotron2&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/tacotron2-DDC&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/tacotron2-DDC_ph&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/glow-tts&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/speedy-speech&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/tacotron2-DCA&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/vits--neon&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/fast_pitch&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/overflow&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/neural_hmm&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/vctk/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/vctk/fast_pitch&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/sam/tacotron-DDC&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/blizzard2013/capacitron-t2-c50&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/blizzard2013/capacitron-t2-c150_v2&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/multi-dataset/tortoise-v2&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/jenny/jenny&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/es/mai/tacotron2-DDC&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/es/css10/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/fr/mai/tacotron2-DDC&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/fr/css10/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/uk/mai/glow-tts&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/uk/mai/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/zh-CN/baker/tacotron2-DDC-GST&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/nl/mai/tacotron2-DDC&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/nl/css10/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/de/thorsten/tacotron2-DCA&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/de/thorsten/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/de/thorsten/tacotron2-DDC&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/de/css10/vits-neon&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/ja/kokoro/tacotron2-DDC&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/tr/common-voice/glow-tts&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/it/mai_female/glow-tts&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/it/mai_female/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/it/mai_male/glow-tts&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/it/mai_male/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/ewe/openbible/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/hau/openbible/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/lin/openbible/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/tw_akuapem/openbible/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/tw_asante/openbible/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/yor/openbible/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/hu/css10/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/el/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/fi/css10/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/hr/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/lt/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/lv/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/mt/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/pl/mai_female/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/pt/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/ro/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/sk/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/sl/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/sv/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/ca/custom/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/fa/custom/glow-tts&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/bn/custom/vits-male&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/bn/custom/vits-female&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/be/common-voice/glow-tts&#x27;</span></span><br></pre></td></tr></table></figure><p>我从英文(‘en’)的 TTS 模型中挑选了一个听起来比较好的 <code>tts_models/en/ljspeech/glow-tts</code>, 作为 talkGPT4All的默认 TTS，调用方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> TTS.api <span class="keyword">import</span> TTS</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化TTS模型</span></span><br><span class="line">tts = TTS(model_name=<span class="string">&quot;tts_models/en/ljspeech/glow-tts&quot;</span>, progress_bar=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者用离线下载的模型路径</span></span><br><span class="line">tts = TTS(model_path=<span class="string">&quot;/path/to/model&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合成文本对应的音频并保存到文件</span></span><br><span class="line">tts.tts_to_file(text=<span class="string">&quot;Hello there&quot;</span>, file_path=<span class="string">&quot;hello.wav&quot;</span>)</span><br></pre></td></tr></table></figure><p>如果因为网络原因模型在Python代码中下载不了，可以手动下载模型，然后指定TTS初始化中的model_path 为模型的本地路径。</p><h4 id="3-2-GPT4All-2-5-0以后模型的调用"><a href="#3-2-GPT4All-2-5-0以后模型的调用" class="headerlink" title="3.2 GPT4All 2.5.0以后模型的调用"></a>3.2 GPT4All 2.5.0以后模型的调用</h4><p>gguf 格式的模型目前有15个，各有特点：</p><p><img data-src="https://picx.zhimg.com/80/v2-be3555b71a240b52bbc48865090126cc_1440w.png?source=d16d100b"></p><p>所有模型的详细信息在<a href="https://github.com/nomic-ai/gpt4all/blob/a328f9ed3fdf238835429dd45940850724d0a652/gpt4all-chat/metadata/models2.json#L145">这里</a>，下面我列出所有支持的模型，方便命令行调用时参考：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mistral-7b-openorca.Q4_0.gguf</span><br><span class="line">mistral-7b-instruct-v0.1.Q4_0.gguf</span><br><span class="line">gpt4all-falcon-q4_0.gguf</span><br><span class="line">orca-2-7b.Q4_0.gguf</span><br><span class="line">orca-2-13b.Q4_0.gguf</span><br><span class="line">wizardlm-13b-v1.2.Q4_0.gguf</span><br><span class="line">nous-hermes-llama2-13b.Q4_0.gguf</span><br><span class="line">gpt4all-13b-snoozy-q4_0.gguf</span><br><span class="line">mpt-7b-chat-merges-q4_0.gguf</span><br><span class="line">orca-mini-3b-gguf2-q4_0.gguf</span><br><span class="line">replit-code-v1_5-3b-q4_0.gguf</span><br><span class="line">starcoder-q4_0.gguf</span><br><span class="line">rift-coder-v0-7b-q4_0.gguf</span><br><span class="line">all-MiniLM-L6-v2-f16.gguf</span><br><span class="line">em_german_mistral_v01.Q4_0.gguf</span><br></pre></td></tr></table></figure><p>而 GPT4All chat 模式的调用方式也发生了变化，新版本需要这么调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gpt_model = GPT4All(<span class="string">&quot;mistral-7b-openorca.Q4_0.gguf&quot;</span>, allow_download=<span class="literal">True</span>)       </span><br><span class="line"><span class="keyword">with</span> gpt_model.chat_session():</span><br><span class="line">    answer = gpt_model.generate(prompt=<span class="string">&quot;hello&quot;</span>)</span><br></pre></td></tr></table></figure><p>需要显式地创建<code>chat_session</code> context manager。</p><h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h3><p>上面就是这次更新的主要内容，总的来说就是采用了更自然的TTS，更新代码以支持 GPT4All最新的break change。</p><p>欢迎大家试用、反馈bug。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https://github.com/vra/talkGPT4All&quot;&gt;talkGPT4All&lt;/a&gt;是基于&lt;a href=&quot;https://link.zhihu.com/?target=https://gpt4all.io/index.html&quot;&gt;GPT4All&lt;/a&gt;的一个语音聊天程序，运行在本地CPU上，支持Linux，Mac和Windows。它利用OpenAI的Whisper模型将用户输入的语音转换为文本，再调用GPT4All的语言模型得到回答文本，最后利用文本转语音(TTS)的程序将回答文本朗读出来。&lt;/p&gt;
&lt;p&gt;今年4、5月份的时候，我发布了talkGPT4All 1.0版本和2.0版本，链接见下：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/618826760&quot;&gt;talkGPT4All: 基于GPT4All的智能语音聊天程序&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/632592897&quot;&gt;talkGPT4All 2.0:现在支持8个语言模型了&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;大家反馈最大的问题是TTS太机械了，听着很难受（具体可以看前面两篇文章的评论区）。而最近TTS领域的进展很多，例如很受欢迎的 coqui-ai的&lt;a href=&quot;https://github.com/coqui-ai/TTS&quot;&gt;TTS&lt;/a&gt; 库，提供了TTS、声音克隆和声音变换的功能。上周末尝试了一下，发现内置了一些开箱即用的TTS模型，刚好可以集成到 talkGPT4All 中，解决目前采用的 &lt;a href=&quot;https://pypi.org/project/pyttsx3/&quot;&gt;pyttsx3&lt;/a&gt;合成声音太机械的问题。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="pip" scheme="http://vra.github.io/tags/pip/"/>
    
    <category term="Whisper" scheme="http://vra.github.io/tags/Whisper/"/>
    
    <category term="GPT4All" scheme="http://vra.github.io/tags/GPT4All/"/>
    
  </entry>
  
  <entry>
    <title>AI小实验：大语言模型能否帮助我们理解古诗？</title>
    <link href="http://vra.github.io/2023/11/13/can-ai-explain-ancient-chinese-poetry/"/>
    <id>http://vra.github.io/2023/11/13/can-ai-explain-ancient-chinese-poetry/</id>
    <published>2023-11-13T04:57:30.000Z</published>
    <updated>2023-12-09T12:07:50.000Z</updated>
    
    <content type="html"><![CDATA[<p>昨天在读龚自珍《己亥杂诗》的时候，看到一句“千秋名教吾谁愧？愧读羲之誓墓文”，怎么想都想不明白这句什么意思。<br><img data-src="/imgs/ai_explain_poetry/000.jpg"><br>突发奇想，既然大语言模型进展突飞猛进，能否帮助我来解读这句诗是什么意思呢？</p><span id="more"></span><p>因此打开手机上的文心一言，讯飞星火、通义千问和智谱清言，向它们提问：“千秋名教吾谁愧？愧读羲之誓墓文。怎么理解”，各个App的回复如下：<br><img data-src="/imgs/ai_explain_poetry/004.png"><br><img data-src="/imgs/ai_explain_poetry/005.png"><br><img data-src="/imgs/ai_explain_poetry/007.jpg"><br><img data-src="/imgs/ai_explain_poetry/006.jpg"><br>可以看到四个App给出了完全不同的回答，其中有两个是明显的胡编乱造了，因为作者和出处都不对。</p><p>忽略掉作者和出处的错误，作者为什么要“愧”呢？四个App给出的答案也不一样，分别是：</p><ul><li>敬佩先贤，自愧不如</li><li>敬仰王羲之书法，但对自己的文学水平和自信</li><li>愧对王羲之临终时守护和传承传统文化的情操</li><li>感慨自己的水平无法与王羲之相提并论</li></ul><p>由于各个App给出了完全不同的回答，而我也不知道正确解释是什么，因此我又用传统的搜索引擎来搜索同样的问题，尝试了微信搜索、微信读书搜索、百度搜索和谷歌搜索。</p><p>与之前搜索古诗的经验不同，这句诗在搜索引擎上很少有解释。之前搜索古诗时，总会找到现代文的翻译，因此意思很容易就能搞懂。但或许由于这句诗实在太生僻了，网上找不到任何的完整的现代文解释。中途我甚至在怀疑：难道我需要去图书馆查找专业资料才能搞懂这句诗的意思吗？</p><p>最终在百度搜索上找到了“羲之誓墓”这个典故的含义：<br><img data-src="/imgs/ai_explain_poetry/003.jpg"><br>所以“羲之誓墓”含义是辞官归隐，隐约能明白作者意思，大概是”后悔误入尘网中，一去三十年”的感觉。<br>然后在谷歌搜索往下翻，找到了一片杭州日报纪念龚自珍的文章，里面提到了这句诗的含义：<br><img data-src="/imgs/ai_explain_poetry/001.jpg"></p><p>总体来说意思就是：王羲之曾在父母墓前发誓不再做官，而我为了在外做官十四年没有给母亲扫墓，真的是羞愧不已，枉读了羲之誓墓的文章。</p><p>所以结论是：至少在生僻的古诗上面，大模型还不能作为一个专家来帮我解读诗词的含义，在搜索引擎中进行信息检索和筛选还是有必要的。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;昨天在读龚自珍《己亥杂诗》的时候，看到一句“千秋名教吾谁愧？愧读羲之誓墓文”，怎么想都想不明白这句什么意思。&lt;br&gt;&lt;img data-src=&quot;/imgs/ai_explain_poetry/000.jpg&quot;&gt;&lt;br&gt;突发奇想，既然大语言模型进展突飞猛进，能否帮助我来解读这句诗是什么意思呢？&lt;/p&gt;</summary>
    
    
    
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="通义千问" scheme="http://vra.github.io/tags/%E9%80%9A%E4%B9%89%E5%8D%83%E9%97%AE/"/>
    
    <category term="文心一言" scheme="http://vra.github.io/tags/%E6%96%87%E5%BF%83%E4%B8%80%E8%A8%80/"/>
    
  </entry>
  
  <entry>
    <title>关于LLaVA-Plus 的一些思考</title>
    <link href="http://vra.github.io/2023/11/12/llava-plus/"/>
    <id>http://vra.github.io/2023/11/12/llava-plus/</id>
    <published>2023-11-12T04:57:30.000Z</published>
    <updated>2023-12-09T11:57:23.000Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://llava-vl.github.io/llava-plus">LLaVA-Plus</a> 是LLAVA团队最近放出来的LMM工作，对LLaVA进行了改进升级，相比LLaVA对输入图像只能进行文本回答的情况，LLaVA-PLUS则包含相当丰富的功能：</p><ul><li>可以调用SD生成与输入类似的图像</li><li>可以对图像进行编辑，例如调用Instruct pix2pix在图像上放置一只动物</li><li>可以对图像进行物体检测，分割，Cpation，OCR，打标签等多模态处理的功能</li><li>还可以调用外部知识来对未知的信息进行检索</li><li>支持用户交互，如对用户点击的区域进行实例分割</li><li>对图像进行美化，然后生成可以发布到社交媒体上的文案</li></ul><p>那么LMM是怎么获得到这么多的多模态能力的呢？论文中提出了一个叫<code>Skill Repository</code> 的概念，就是一些AI 子任务的能力和对应的模型，利用这个Skill Repository来完成丰富的功能。也就是说LLaVA-Plus将用户输入的任务通过进行拆分，然后调用合适的子任务模型来实现，再对结果进行一定的处理返回给用户。</p><span id="more"></span><p>具体的Skill Repository 包括下面这些：<br><img data-src="/imgs/llava_plus/20231112094107.png"></p><p>其实会发现，这种思路跟Visual ChatGPT是很类似的，不过与Visual ChatGPT不同的是，LLaVA-Plus没有调用外部的大语言模型ChatGPT，而是将LLM部分融合进了统一的网络结构中。这样的好处是图像特征在整个对话过程中都是有感知的，而Visual ChatGPT的方案则只在调用子模型的时候有图像感知，语言模型部分并不知道图像的特征，毕竟那会的ChatGPT还无法理解图像。</p><p>我觉得这种思路是LMM模型最可行的方案，即语言模型部分理解用户的要求，得到需要调用能力的列表，再调用对应的多模态模型，将多模态模型的输出进行总结，以自然语言的形式返回给用户。<br>这样的好处也是非常明显的：</p><ul><li>将子任务模型与LMM模型解耦，只要增加自己子任务的模型，就能支持用户的输入要求</li><li>每个子任务模型解决自己的特定的任务，结果肯定是最好的，而不是用一个什么都想做但都做的不是最好的模型</li><li>子任务可以利用现有的开源模型，降低整个系统学习的难度，避免了重复工作</li></ul><p>我觉得LLaVA-Plus对AI应用的进一步涌现很有促进作用。首先是这个方向有很多有意思的东西可以来做着玩了。<br>比如自动发朋友圈/微博/Ins/Twitter的Bot，可以将用户拍的照片进行美化，提高分辨率，然后自动生成I文案并发送出去。更发散一点，AI可以有自己的朋友圈了。</p><p>还有自动标注数据集的工具，所有类型的标注都自动来标注，甚至可以利用不同模型之间的一致性对标注质量进行提高。</p><p>另一方面，包含语言模型和子任务模型的LMMs也许真的会让CV和AI离普通人更近，因为自然语言的接口相比之前的计算机语言的接口要更易用。也许未来我们真的不需要单独的子任务模型了，通过LLaVA-Plus就可以用自然语言调用这些模型，甚至未来这些子任务模型我们可能都感知不到了，毕竟对用户来说，只是希望解决问题，而不关系底层用的是检测模型还是分割模型。</p><p><img data-src="/imgs/llava_plus/20231112093744.png"></p><p><img data-src="/imgs/llava_plus/20231112095143.png"><img data-src="/imgs/llava_plus/20231112095628.png"></p><p>另一个有意思的结果是，利用LLaVA-Plus可以对文生图的过程进行改进，就像WALLE-3利用ChatGPT来生成更好的Prompt一样，LLaVA-Plus也可以对用户输入的提示词进行优化，得到更适合SD的提示词：<br><img data-src="/imgs/llava_plus/20231112132234.png"></p><p>最后对论文的大致思路进行一个总结，也是比较粗糙，具体细节看论文吧。<br>作者提出了一种通用的多模态任务的问答形式：<br><img data-src="/imgs/llava_plus/20231112122754.png"></p><p>Iq是问题图像输入，Xq是问题文本输入，Xanswer是回答输出。</p><p>形式看似简单，但要看到这种统一形式的重要意义，利用统一的形式定义，可以将大量的不同子任务训练数据组织到一起，为LMM强大功能奠定基础。</p><p>为了得到准确的Xanswer，需要借助 Skill Repository里面的工具，得到Xskill_result，再得到Xanswer，<img data-src="/imgs/llava_plus/20231112094913.png"><br><img data-src="/imgs/llava_plus/20231112122933.png"><br>为了能够找到输入任务对应的模型并得到输出，作者设置了”thoughts”, “actions”和“value”三个阶段的， 分别进行输入到子模型的拆分、子模型调用API和参数，以及子模型的输出。</p><p>下面是一个具体调用的例子：<br><img data-src="/imgs/llava_plus/20231112123129.png"></p><p>在训练数据的构造方面也比较有意思。<br>为了利用LLaVA没有thoughts-actions-value过程的数据，作者添加了“空白”的thoughts-actions-value占位符：<br><img data-src="/imgs/llava_plus/20231112131254.png"></p><p>为了增加问题的多样性，让GPT4来改写问题：<img data-src="/imgs/llava_plus/20231112131711.png"></p><p>根据caption 数据，让ChatGPT/GPT4来提问题，构造训练数据，这里的提示词工程挺有意思，有些trick在里面，可以细看一下：<br><img data-src="/imgs/llava_plus/20231112131934.png"></p><p>论文附录中有很多例子，可以参考。</p><p>Online功能体验地址： <a href="https://llavaplus.ngrok.io/">LLaVA-Plus (llavaplus.ngrok.io)</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://llava-vl.github.io/llava-plus&quot;&gt;LLaVA-Plus&lt;/a&gt; 是LLAVA团队最近放出来的LMM工作，对LLaVA进行了改进升级，相比LLaVA对输入图像只能进行文本回答的情况，LLaVA-PLUS则包含相当丰富的功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以调用SD生成与输入类似的图像&lt;/li&gt;
&lt;li&gt;可以对图像进行编辑，例如调用Instruct pix2pix在图像上放置一只动物&lt;/li&gt;
&lt;li&gt;可以对图像进行物体检测，分割，Cpation，OCR，打标签等多模态处理的功能&lt;/li&gt;
&lt;li&gt;还可以调用外部知识来对未知的信息进行检索&lt;/li&gt;
&lt;li&gt;支持用户交互，如对用户点击的区域进行实例分割&lt;/li&gt;
&lt;li&gt;对图像进行美化，然后生成可以发布到社交媒体上的文案&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那么LMM是怎么获得到这么多的多模态能力的呢？论文中提出了一个叫&lt;code&gt;Skill Repository&lt;/code&gt; 的概念，就是一些AI 子任务的能力和对应的模型，利用这个Skill Repository来完成丰富的功能。也就是说LLaVA-Plus将用户输入的任务通过进行拆分，然后调用合适的子任务模型来实现，再对结果进行一定的处理返回给用户。&lt;/p&gt;</summary>
    
    
    
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="Deep Learning" scheme="http://vra.github.io/tags/Deep-Learning/"/>
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="LMM" scheme="http://vra.github.io/tags/LMM/"/>
    
    <category term="CV" scheme="http://vra.github.io/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Bing Brush-Python代码和命令行中调用必应 DALL·E 3文生图模型</title>
    <link href="http://vra.github.io/2023/11/04/bing-brush/"/>
    <id>http://vra.github.io/2023/11/04/bing-brush/</id>
    <published>2023-11-04T15:22:03.000Z</published>
    <updated>2023-12-09T03:13:56.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-说明"><a href="#1-说明" class="headerlink" title="1. 说明"></a>1. 说明</h3><p>今早看到一个好玩的项目，利用Bing Image Creator 来生成每日诗词的图像，研究了一下，发现有人提供了<a href="https://github.com/acheong08/BingImageCreator">BingImageCreator</a>仓库来调用Bing的API在代码中生成图像，但还需要下载源码，没有提供cli，cookie怎么获取也没有讲太细。</p><p>因此我基于这个仓库，做了一些精简和封装，提供了一个可以直接pip安装的工具<a href="https://github.com/vra/bing_brush">bing_brush</a>, 获取cookie后可以直接命令行调用。</p><span id="more"></span><p><img data-src="https://pic1.zhimg.com/80/v2-4d60e7c55a9388e56903c58fd3b1432f_1440w.png?source=d16d100b"></p><p>整体流程很简单：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install bing_brush</span><br><span class="line"><span class="comment"># 获取bing.com的cookie，见下文</span></span><br><span class="line">bing_brush -c cookie.txt -p <span class="string">&#x27;a cute panda eating bamboos&#x27;</span> -o output_folder</span><br></pre></td></tr></table></figure><p>就会output_folder 下生成4张图像：</p><p><img data-src="https://pica.zhimg.com/80/v2-9f13f504f3431d1018acd2bf8d3a7241_1440w.png?source=d16d100b"></p><p>源码：<a href="https://github.com/vra/bing_brush">vra/bing_brush (github.com)</a><br>欢迎Watch, Star, Fork 和Contribute！</p><h3 id="2-cookie获取"><a href="#2-cookie获取" class="headerlink" title="2. cookie获取"></a>2. cookie获取</h3><p>整个过程中稍微有些繁琐的是获取cookie，详细操作见下。</p><p>首先打开 <a href="https://www.bing.com/images/create">https://www.bing.com/images/create</a></p><p>如果访问不了的话，那这个工具也没法使用，因此确保这个页面可以正常打开。</p><p><img data-src="https://picx.zhimg.com/80/v2-aa9f3e5f8e645d02f9ad174fa11a0f50_1440w.jpeg?source=d16d100b"></p><p>然后按F12，打开开发者页面，然后刷新页面，会看到很多请求，选择任一类型为xhr的请求，点击前面的lianjie：</p><p><img data-src="https://picx.zhimg.com/80/v2-173983dcdd28069d41dce7af3f2d61eb_1440w.jpeg?source=d16d100b"></p><p>进入详情页面后，往下翻找到Cookie 部分，将对应的右边的复制到cookie.txt即可，后面-c 指定这个路径就行。</p><p><img data-src="https://picx.zhimg.com/80/v2-cd8f0e48096c0b990a931764610bf5ad_1440w.jpeg?source=d16d100b"></p><h3 id="3-使用流程"><a href="#3-使用流程" class="headerlink" title="3. 使用流程"></a>3. 使用流程</h3><p>pip安装bing_brush，并且获取cookie后，就可以用一条命令来运行图像生成：</p><p>bing_brush -c cookie.txt -p ‘a cute panda eating bamboos’ -o output_folder</p><p>然后就可以发挥你的创意来在命令行跑图了。</p><h3 id="4-Python代码中使用"><a href="#4-Python代码中使用" class="headerlink" title="4. Python代码中使用"></a>4. Python代码中使用</h3><p>pip 安装后，也可以在Python代码中使用 Bing Brush:</p><p>from bing_brush import BingBrush</p><p>brush = BingBrush(cookie=’cookie.txt’)<br>brush.process(prompt=’a cute panda eating bamboos’, out_folder=’output_folder’)</p><h3 id="5-彩蛋"><a href="#5-彩蛋" class="headerlink" title="5. 彩蛋"></a>5. 彩蛋</h3><p>这个项目的Logo也是用Bing生成的，prompt如下：</p><blockquote><p>A minimalist logo vector image, square-shaped, with a magical brush implemented in Python language in the center, colorful, digital art</p></blockquote><p>画出了三张logo，最后选择第三张作为项目的Logo</p><p><img data-src="https://picx.zhimg.com/80/v2-28772285cca47864cbd9a6bf396a6bb1_1440w.png?source=d16d100b"></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-说明&quot;&gt;&lt;a href=&quot;#1-说明&quot; class=&quot;headerlink&quot; title=&quot;1. 说明&quot;&gt;&lt;/a&gt;1. 说明&lt;/h3&gt;&lt;p&gt;今早看到一个好玩的项目，利用Bing Image Creator 来生成每日诗词的图像，研究了一下，发现有人提供了&lt;a href=&quot;https://github.com/acheong08/BingImageCreator&quot;&gt;BingImageCreator&lt;/a&gt;仓库来调用Bing的API在代码中生成图像，但还需要下载源码，没有提供cli，cookie怎么获取也没有讲太细。&lt;/p&gt;
&lt;p&gt;因此我基于这个仓库，做了一些精简和封装，提供了一个可以直接pip安装的工具&lt;a href=&quot;https://github.com/vra/bing_brush&quot;&gt;bing_brush&lt;/a&gt;, 获取cookie后可以直接命令行调用。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="pip" scheme="http://vra.github.io/tags/pip/"/>
    
    <category term="DALL·E" scheme="http://vra.github.io/tags/DALL%C2%B7E/"/>
    
    <category term="Bing" scheme="http://vra.github.io/tags/Bing/"/>
    
  </entry>
  
  <entry>
    <title>使用 Python 88 行代码写一个简易的 Android AI 程序</title>
    <link href="http://vra.github.io/2023/10/14/android-ai-app-in-88-lines-of-python-code/"/>
    <id>http://vra.github.io/2023/10/14/android-ai-app-in-88-lines-of-python-code/</id>
    <published>2023-10-14T15:22:03.000Z</published>
    <updated>2023-12-09T02:52:12.000Z</updated>
    
    <content type="html"><![CDATA[<p>TL;DR:<br>我基于 LeptonAI 和 Beeware Python 库，利用 88 行的Python，不用写一行Java代码，在手机上做了一个 SDXL text-to-image 的Demo，效果见<a href="https://zhuanlan.zhihu.com/p/661358058">这里</a>的视频。</p><p>作为一个爱折腾写Python比较多的人，我一直在想一个事情：能否将熟悉的Python技术栈的能力带到移动平台中，不用写哪些繁琐的Native开发代码，就能在移动端跑起来一个AI Demo呢？因为相比PC，移动端设备的用户数多得多，每个人都有一台手机，但并不是每个人都有一台电脑。</p><p>一次偶然的机会，我发现了 <a href="https://beeware.org/">Beeware</a>，一个目标 “Write once. Deploy everywhere.“ 的跨平台 Python 工具箱。基于 Beeware 工具箱写的 Python 程序可以在 PC，Web，Android 和 iOS 上运行，因此正是我想要的。</p><p>一切听起来很美好，但实际使用时也遇到很多问题。</p><span id="more"></span><p>首先是 Beeware 在移动端支持的 Python 包有限，比如像对 Pytorch 的支持就有问题 (可以import但运行时报错)，所以手机本地没法直接运行 Pytorch AI模型，至少我没有跑通。</p><p>另一个是 Beeware 工具链中的 GUI 库 toga 太简单了，一些复杂的功能实现不了，比如网络推理时加一个显示在窗口最顶层的转圈的特效。所以只能做一些比较toy的小的项目，没法做真正可以用的产品。</p><p>所以不想写繁琐的 Natvie代码的话，另一个选择可能就是写 基于小程序的 Web 代码了，至少小程序的UI功能还是很齐全的。</p><p>Anyway，虽然有这些约束，但还是可以用 Beeware 做一些简单的 Python Demo，比如这里我就结合 <a href="https://www.lepton.ai/">LeptonAI</a>和 Beeware，一行 Android 开发的都不用写，总共利用 88 行的 Python 代码，做出来了一个简单的 SDXL text-to-image Android 端 Demo。</p><p>首先说说一下服务端。SDXL 部署在 LeptonAI 的云平台上，提供公网可访问的 AI 服务。关于 LeptonAI 的使用和 SDXL 的部署，可以参考我这篇<a href="https://zhuanlan.zhihu.com/p/661243511">文章</a>。简单来说安装 LeptonAI Python SDK 后，使用下面的三条命令创建模型镜像，然后在 LeptonAI 的云平台进行部署:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建镜像</span></span><br><span class="line">lep photon create --name sdxl --model hf:hotshotco/SDXL-512</span><br><span class="line"></span><br><span class="line"><span class="comment"># 登录云平台</span></span><br><span class="line">lep login -c xxx:xxxxxxxxxxxxxxxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推送镜像到云平台</span></span><br><span class="line">lep photon push --name sdxl</span><br></pre></td></tr></table></figure><p>客户端就是这个App， 整体功能很简陋，用户在输入框填入提示词，点击生成图片的按钮后，代码读取用户输入，构造网络请求，然后将 text-to-image 生成的图像返回给客户端，客户端进行解析后再展示。</p><p>开发流程是先在 Mac 上调试代码，成功后再进行一些微调，就能跑到手机上。</p><p>具体来说，整个过程中用到的 Beeware 命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交互式地构建项目目录</span></span><br><span class="line">briefcase new</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在Mac上调试代码</span></span><br><span class="line">briefcase dev</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Android 开发环境，会自动在命令行下载NDK等</span></span><br><span class="line">briefcase create android </span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译代码，生成 APK文件</span></span><br><span class="line">briefcase build android</span><br></pre></td></tr></table></figure><p><code>briefcase</code> 是 Beeware 工具箱中用来将 Python 代码转换为 Native 应用的工具。</p><p>在 Mac 上运行正常，往手机上微调过程中，也有一些细节要注意。</p><p>首先是需要将依赖包写入到<code>pyproject.toml</code>中的<code>requires</code> 字段中，Mac上可能因为已经提前安装了一些第三方包而在使用时没有报错，但在移动端使用时需要将所有用到的包都加入到apk中。</p><p>由于 Beeware 貌似不支持 requests 包，所以需要将 比较简洁的 requests 请求方式修改为基于系统库的<code>urllib.request</code> 请求方式。</p><p>由于Android环境没有环境变量，因此需要将原先代码中读取环境变量中的TOKEN的代码去掉，这里采用了不太科学的方法，直接将TOKEN写死在代码中。</p><p>Python 代码更新有时候不会生效，需要手动删除 Build 目录再执行 <code>briefcase build android</code>的命令。</p><p>最后也将 88 行代码列出来，完整代码仓库在<a href="https://github.com/vra/sdxl-python-app">这里</a>，感兴趣的小伙伴可以自己玩玩。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">An Application based on Python and LeptonAI!</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image <span class="keyword">as</span> PIL_Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> toga</span><br><span class="line"><span class="keyword">from</span> toga.style <span class="keyword">import</span> Pack</span><br><span class="line"><span class="keyword">from</span> toga.style.pack <span class="keyword">import</span> COLUMN, ROW</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AISDK</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># Android 端没法用环境变量，这里只能将 TOKEN 写死在代码中</span></span><br><span class="line">        api_token = <span class="string">&quot;xxxxxxxxxxxx&quot;</span></span><br><span class="line">        self.url = <span class="string">&quot;https://xxx-sdxl-deploy.bjz.edr.lepton.ai/run&quot;</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/json&quot;</span>,</span><br><span class="line">            <span class="string">&quot;accept&quot;</span>: <span class="string">&quot;image/png&quot;</span>,</span><br><span class="line">            <span class="string">&quot;Authorization&quot;</span>: <span class="string">f&quot;Bearer <span class="subst">&#123;api_token&#125;</span>&quot;</span>,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span>(<span class="params">self, prompt, img_save_path</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;ai processing begin...&quot;</span>)</span><br><span class="line">        data = &#123;<span class="string">&quot;num_inference_steps&quot;</span>: <span class="number">25</span>, <span class="string">&quot;prompt&quot;</span>: prompt, <span class="string">&quot;seed&quot;</span>: <span class="number">42</span>&#125;</span><br><span class="line">        req = urllib.request.Request(self.url, headers=self.headers, data=json.dumps(data).encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">        response = urllib.request.urlopen(req)</span><br><span class="line">        res = response.read()</span><br><span class="line"></span><br><span class="line">        image_data = io.BytesIO(res)</span><br><span class="line">        image = PIL_Image.<span class="built_in">open</span>(image_data)</span><br><span class="line">        image.save(img_save_path)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;ai processing done&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SDXLApp</span>(<span class="params">toga.App</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">startup</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.sdk = AISDK()</span><br><span class="line">        self.img_save_path = os.path.join(os.path.dirname(__file__), <span class="string">&quot;aigc_img.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line">        main_box = toga.Box(style=Pack(direction=COLUMN))</span><br><span class="line"></span><br><span class="line">        name_label = toga.Label(<span class="string">&quot;Your prompt: &quot;</span>, style=Pack(padding=(<span class="number">0</span>, <span class="number">5</span>)))</span><br><span class="line">        self.name_input = toga.TextInput(style=Pack(flex=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        name_box = toga.Box(style=Pack(direction=ROW, padding=<span class="number">5</span>))</span><br><span class="line">        name_box.add(name_label)</span><br><span class="line">        name_box.add(self.name_input)</span><br><span class="line"></span><br><span class="line">        button = toga.Button(</span><br><span class="line">            <span class="string">&quot;Generate Image&quot;</span>, on_press=self.run_aigc, style=Pack(padding=<span class="number">5</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        main_box.add(name_box)</span><br><span class="line">        main_box.add(button)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(self.img_save_path)</span><br><span class="line">        self.image = toga.Image(self.img_save_path)</span><br><span class="line">        self.image_view = toga.ImageView(self.image)</span><br><span class="line"></span><br><span class="line">        self.main_window = toga.MainWindow(title=self.formal_name)</span><br><span class="line">        self.main_window.content = main_box</span><br><span class="line">        self.main_window.content.add(self.image_view)</span><br><span class="line">        self.main_window.show()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_aigc</span>(<span class="params">self, widget</span>):</span></span><br><span class="line">        <span class="comment"># 清除已有结果</span></span><br><span class="line">        self.main_window.content.remove(self.image_view)</span><br><span class="line">        self.image_view = toga.ImageView(image=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        prompt = self.name_input.value</span><br><span class="line">        self.sdk.process(prompt, self.img_save_path)</span><br><span class="line"></span><br><span class="line">        image = toga.Image(self.img_save_path)</span><br><span class="line">        self.image_view = toga.ImageView(image)</span><br><span class="line">        self.main_window.content.add(self.image_view)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="keyword">return</span> SDXLApp()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    SDXLApp()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;TL;DR:&lt;br&gt;我基于 LeptonAI 和 Beeware Python 库，利用 88 行的Python，不用写一行Java代码，在手机上做了一个 SDXL text-to-image 的Demo，效果见&lt;a href=&quot;https://zhuanlan.zhihu.com/p/661358058&quot;&gt;这里&lt;/a&gt;的视频。&lt;/p&gt;
&lt;p&gt;作为一个爱折腾写Python比较多的人，我一直在想一个事情：能否将熟悉的Python技术栈的能力带到移动平台中，不用写哪些繁琐的Native开发代码，就能在移动端跑起来一个AI Demo呢？因为相比PC，移动端设备的用户数多得多，每个人都有一台手机，但并不是每个人都有一台电脑。&lt;/p&gt;
&lt;p&gt;一次偶然的机会，我发现了 &lt;a href=&quot;https://beeware.org/&quot;&gt;Beeware&lt;/a&gt;，一个目标 “Write once. Deploy everywhere.“ 的跨平台 Python 工具箱。基于 Beeware 工具箱写的 Python 程序可以在 PC，Web，Android 和 iOS 上运行，因此正是我想要的。&lt;/p&gt;
&lt;p&gt;一切听起来很美好，但实际使用时也遇到很多问题。&lt;/p&gt;</summary>
    
    
    
    
    <category term="LeptonAI" scheme="http://vra.github.io/tags/LeptonAI/"/>
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="Android" scheme="http://vra.github.io/tags/Android/"/>
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>LeptonAI 使用体验</title>
    <link href="http://vra.github.io/2023/10/14/leptonai/"/>
    <id>http://vra.github.io/2023/10/14/leptonai/</id>
    <published>2023-10-13T16:05:35.000Z</published>
    <updated>2023-12-09T02:40:12.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p><a href="https://www.lepton.ai/">LeptonAI</a>是贾扬清的创业项目，正如 slogan “Build AI The Simple Way” 所表明的，LeptonAI的目标是简化AI模型的部署。</p><p>简单来说，LeptonAI 提供了 <a href="(https://github.com/leptonai/leptonai">Python SDK</a> 和云平台。Python SDK 可以让没有AI领域知识的普通开发者调用2～3行命令就能部署一个AI模型，然后用一个curl语句或几行Python代码就能完成客户端请求；而 LeptonAI 云平台提供了CPU，GPU和存储资源，开发者可以把创建的AI模型部署到这里，就能提供公开对外的AI服务。</p><p>AI模型创建支持 HuggingFace，也就是说可以将 HuggingFace 上海量的模型集成到自己的应用中。同时 LeptonAI 也支持从 GitHub 仓库创建 AI 模型，给了开发者更多的选择。</p><p>经过一段时间的体验后发现，LeptonAI Python SDK 设计的很优雅，用起来很舒服，而云平台的操作也很丝滑，有贾扬清大神亲自操刀写代码，SDK和云平台的质量绝对是信得过的。<br><img data-src="/imgs/leptonai/20231014072616.png"></p><p>那 LeptonAI 是否解决了一些 AI 部署中的痛点问题呢？我认为是的。根据之前的经验，跑一些 AI 开源模型成本还是挺高的。</p><span id="more"></span><p>简单来说，跑一个开源模型，需要下面这些流程：</p><ol><li>需要安装Conda虚拟环境</li><li>拉取开源代码源码</li><li>安装依赖 Python 包</li><li>下载 Checkpoints</li><li>跑 Demo 命令或 UI<br>就算整个过程中没有遇到问题，整个流程走下来，跑起来一个模型至少也得半个小时。</li></ol><p>而实际情况可能更糟糕，大概率会遇到下面的问题之一或者之N：</p><ol><li>开源代码没有指定Python版本，用新/老版本的Python运行报错</li><li>开源代码没有指定CUDA版本，用新/老版本的CUDA，运行报错</li><li>当前操作系统对应的CPU和GPU组合未经测试，运行报错</li><li>编译Pytorch所用的cuDNN版本和系统现在的cuDNN版本不一致，Pytorch插件编译报错</li><li>依赖 Python 包版本冲突，需要某个特定版本才能正常工作</li><li>之前代码运行正常，更新某个包版本后运行报错</li><li> 默认 Demo Python 脚本运行报错，需要查看源码定位修复问题</li><li>某些OP不支持half精度，运行报错</li><li>某些OP不支持mps后端，运行报错</li><li>网络状况不佳导致 GitHub 拉取代码失败，HuggingFace 下载模型失败</li><li>…</li></ol><p>以 stable-diffusion-webui 这个著名的AIGC库为例，光带<code>bug-report</code>标记的issue就有3000多个：<br><img data-src="/imgs/leptonai/20231014080015.png"></p><p>随便看几个例子，都能发现上述这些问题的身影：<br><img data-src="/imgs/leptonai/20231014080629.png"><br><img data-src="/imgs/leptonai/20231014075232.png"><br>也就是说，大家在 AI 部署中遇到了非常多的问题，而不是没有问题。</p><p>因此简化 AI 的部署，是很有意义的方向，做的好的话，能减轻很多人的痛苦。根据我的经历，在使用 LeptonAI 部署模型时，我还没遇到过上面提到的这些问题（当然现在使用的次数也还比较少）。</p><p>LeptonAI 的盈利模式也很清晰，虽然 Python SDK 和 HuggingFace 上的模型都是开源的，但由于 LeptonAI 提供了云服务来部署算法，因此可以通过云服务的硬件资源（GPU，CPU和存储）来收费，在 Lepton Python SDK 将部署问题大大简化后，降低了AI应用的门槛，可能会出现更多的开发者来提供更好的创意，做出更多更棒的App，产生更加实际的业务成果。如果AI算法真的能提供实际的业务价值，那应用开发者肯定愿意将收入中的一部分给到 LeptonAI 平台</p><p>但这里也有一个问题：LeptonAI Python SDK 似乎是可以部署到任意的云计算平台的，也就是说如果阿里云、亚马逊云提供的 GPU 价格更低，那开发者可能就会迁移自己到这些平台，也就是用 LeptonAI Python SDK，但不用你的 GPU 资源。在这个角度下，LeptonAI到底能不能赢得用户，产生正向收益，还不好说。</p><p>具体到云平台的收费模式，采用 SaaS 中比较常见的基础版免费、Pro版收费的方式。基础版是每个月会给用户会赠送10美元的券，可以方便个人开发者和小的初创团队对进行前期的简单实验验证。标准版每个月100美元，还有企业可以定制：<br><img data-src="/imgs/leptonai/20231013220038.png"></p><p>从现在的信息看，LeptonAI 现阶段的目标市场应该是欧美，目标客户应该是个人开发者或者中小公司，因为定价是美元，支付采用Stripe，这些都是国内用户不太方便使用的。</p><p>其实国内使用还有另外一个影响很大的因素：HuggingFace 目前在国内网络访问不了（实在想不通有什么理由封禁HuggingFace)，上面的AI模型都是用不了的，想要部署 HuggingFace 上的模型要费一些周折。</p><p>另外 LeptonAI 最近开始公开内测了，想要试玩的朋友可以在官网注册账号申请。</p><p>在公测的<a href="https://leptonai.medium.com/build-ai-the-easy-way-2a8b68c63723">Blog</a>中，介绍了几位创始人之前的项目经历，最后也自豪地宣称他们是”make building awesome AI applications as simple as possible”最好的人选。<br><img data-src="/imgs/leptonai/20231014062556.png"></p><p>正如 Caffe 开启了深度学习框架的新时代，希望 LeptonAI 也能开启 AI 部署的新时代，未来如何发展，让我们拭目以待。</p><p>后面部分会从网站提供的功能概览、部署HuggingFace上模型、部署GitHub上自己开发的模型，以及基于Lepton云服务做一个简单的Android Demo，看看开发一个应用有多简单。</p><h2 id="2-Playground概览"><a href="#2-Playground概览" class="headerlink" title="2. Playground概览"></a>2. Playground概览</h2><p><a href="https://www.lepton.ai/playground">https://www.lepton.ai/playground</a> 提供了很多LeptonAI自部署的开源模型，如SDXL，Llama 2, Code Llama等,跟HuggingFace平台功能类似。基于LeptonAI SDK，开发者可以在自己的GPU服务器上快速地搭建这些模型服务。</p><p><img data-src="/imgs/leptonai/20231013231215.png"><br>以SDXL Inpainting为例，上传一张图像，mask掉一些区域，再结合一句Prompt，就能补全出对应的区域：</p><h2 id="3-LeptonAI-设计思路简介"><a href="#3-LeptonAI-设计思路简介" class="headerlink" title="3. LeptonAI  设计思路简介"></a>3. LeptonAI  设计思路简介</h2><p>Lepton的含义是轻子，是一个物理学概念，根据维基百科：</p><blockquote><p>轻子是一种不参与强相互作用、自旋为1/2的基本粒子。电子是最为人知的一种轻子；轻子又分为两类：“带电轻子”与“中性轻子”。带电轻子包括电子、μ子、τ子，可以与其它粒子组合成复合粒子，例如原子、电子偶素等等</p></blockquote><p>公司命名为 LeptonAI Inc，科技属性拉满。<br>LeptonAI Python SDK 的基本模块是 Photon (光子)，基本上可以理解为算法模型，如 SDXL，Llama2 都可以看作是 Photon。<br>Lepton 一个独到的地方是，将所有不同来源的算法模型统一到Photon对象中，比如HuggingFace上的一个模型可以用来初始化Photon，也可以从GitHub上的源代码来初始化Photon。</p><p>基于这种简化，所有算法模型都能通过统一的对外接口来展示，方便了模型的部署。当然为了简单统一的接口，源码内部就需要对不同来源的模型进行适配，是以内部代码的复杂度增加来换取对外接口的简洁一致。</p><p>更准确、详细的内容，请参考<a href="https://www.lepton.ai/docs">官方文档</a>。</p><h2 id="4-部署自己的服务"><a href="#4-部署自己的服务" class="headerlink" title="4. 部署自己的服务"></a>4. 部署自己的服务</h2><p>使用LeptonAI的Python leptonai 包可以进行方便的Photon创建和部署。</p><p>首先用pip 安装 leptonai 包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U leptonai</span><br></pre></td></tr></table></figure><p>输入 <code>lep --help</code> 查看是否安装成功。<br>LeptonAI 的 CLI 命令是 <code>lep</code>，包含所有你需要的功能，如 Photon 创建、部署，登录云平台等。</p><p>为了在 LeptonAI 云平台部署模型，确保你已经申请拿到了公测权限。</p><h3 id="3-1-基于HuggingFace"><a href="#3-1-基于HuggingFace" class="headerlink" title="3.1 基于HuggingFace"></a>3.1 基于HuggingFace</h3><p>官方文档展示了在 LeptonAI 云平台部署 GPT-2 的操作流程，但由于GPT-2效果一般，实际上用处不大，这里以目前效果比较好的SDXL为例来展示如何部署HuggingFace到自己的 LeptonAI 实例上。</p><p>首先是用 <code>lep photon create</code> 创建 Photon:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lep photon create --name sdxl --model hf:hotshotco/SDXL-512</span><br></pre></td></tr></table></figure><p><code>--name</code> 是自定义的 Photon 名字，<code>--model</code> 是Photo 下载模型的地址，可以是一个HuggingFace模型名字，也可以是一个GitHub地址。这里用 <code>hf:</code> 开头表示 HuggingFace 上的模型，<code>&lt;user_name&gt;/&lt;model_name&gt;</code>表示模型的路径。</p><p>这里会从 HuggingFace 上拉取模型信息，所以需要能访问 HuggingFace。国内访问hf有问题的小伙伴可以用<a href="https://gitpod.io/">GitPod</a>这个神器。</p><p>然后为了部署模型到云平台，需要登录 LeptonAI 的 dashboard，查看自己的key，复制 <code>lep login</code> 命令：<br><img data-src="/imgs/leptonai/20231014003956.png"></p><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lep login -<span class="keyword">c</span> xxx:xxxxxxxxxxxxxxxx</span><br></pre></td></tr></table></figure><p>登录成功后，就可以用 <code>lep photon push</code> 命令将本地创建好的 Photon 推送到 LeptonAI 云平台：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lep photon push --name sdxl</span><br></pre></td></tr></table></figure><p>然后在 Dashboard 网页上进行操作，对 Photon 进行部署：</p><p>部署完成后，就相当于有了一个公网可以访问的AI服务，通过下面的Python命令就可以访问这个服务：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> leptonai.client <span class="keyword">import</span> Client</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">LEPTON_API_TOKEN = <span class="string">&quot;xxxxxxxxxx&quot;</span></span><br><span class="line">client = Client(<span class="string">&quot;xx&quot;</span>, <span class="string">&quot;sdxl&quot;</span>, token=LEPTON_API_TOKEN)</span><br><span class="line"></span><br><span class="line">data = client.run(</span><br><span class="line">    num_inference_steps=<span class="number">25</span>,</span><br><span class="line">    prompt=<span class="string">&quot;a photograph of an astronaut riding a horse&quot;</span>,</span><br><span class="line">    seed=<span class="number">42</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将服务返回的 bytes 数据保存为图片</span></span><br><span class="line">image_data = io.BytesIO(data)</span><br><span class="line">image = Image.<span class="built_in">open</span>(image_data)</span><br><span class="line">image.save(<span class="string">&quot;output_image.jpg&quot;</span>)</span><br></pre></td></tr></table></figure><p>同理也可以用 Python 的 requests 包来发送请求，这样的好处是客户端就不需要安装 LeptonAI 的 Python包了，毕竟它的依赖还是比较多的，下一个例子中会展示requests的用法。</p><h3 id="3-2-基于-GitHub-部署自己的算法"><a href="#3-2-基于-GitHub-部署自己的算法" class="headerlink" title="3.2 基于 GitHub 部署自己的算法"></a>3.2 基于 GitHub 部署自己的算法</h3><p>LeptonAI 也支持从 GitHub 创建 Photons，只要你的代码类继承了 lepton的Photo基类，就都可以正常部署，对于许多想要新建自己算法的人来说是个好消息。下面用一个简单的提取图像边缘的例子来展示怎么自定义算法并且部署到 LeptonAI 云平台。</p><p>我的代码仓库在<a href="https://github.com/vra/canny-lepton-photon%EF%BC%8C%E5%AE%9E%E7%8E%B0%E5%9C%A8">https://github.com/vra/canny-lepton-photon，实现在</a> <code>canny.py</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> leptonai.photon <span class="keyword">import</span> Photon, PNGResponse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要继承 Photon 类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Canny</span>(<span class="params">Photon</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Canny 边缘检测算子&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里的依赖 Package 会在创建 Photon 时自动安装</span></span><br><span class="line">    requirement_dependency = [</span><br><span class="line">        <span class="string">&quot;opencv-python&quot;</span>,</span><br><span class="line">        <span class="string">&quot;numpy&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Pillow&quot;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用这个装饰器表示这个一个对外接口</span></span><br><span class="line"><span class="meta">    @Photon.handler(<span class="params"><span class="string">&quot;run&quot;</span></span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self, url: <span class="built_in">str</span></span>) -&gt; PNGResponse:</span></span><br><span class="line">        <span class="comment"># 将第三方库的 import 放到实际执行代码中，否则如果本地没有这些包，</span></span><br><span class="line">        <span class="comment"># 在创建 Photon 时报错</span></span><br><span class="line">        <span class="keyword">import</span> cv2</span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">        <span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取图像数据</span></span><br><span class="line">        image = np.asarray(Image.<span class="built_in">open</span>(io.BytesIO(urlopen(url).read())))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行边缘检测</span></span><br><span class="line">        edges = cv2.Canny(image, <span class="number">100</span>, <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">        edges = Image.fromarray(edges)</span><br><span class="line"></span><br><span class="line">        img_io = BytesIO()</span><br><span class="line">        edges.save(img_io, <span class="built_in">format</span>=<span class="string">&quot;PNG&quot;</span>, quality=<span class="string">&quot;keep&quot;</span>)</span><br><span class="line">        img_io.seek(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> PNGResponse(img_io)</span><br></pre></td></tr></table></figure><p>由于GitHub不再支持命令行密码登录了，所以需要在<a href="https://github.com/settings/tokens?type=beta">这里</a>创建token，然后用token替代密码来登录。<br>复制刚才创建的token，在环境变量中进行设置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> GITHUB_USER=&#123;YOUR_GITHUB_USERNAME&#125;</span><br><span class="line"><span class="built_in">export</span> GITHUB_TOKEN=&#123;THE_TOKEN_GENERATED_FROM_STEP_1&#125;</span><br></pre></td></tr></table></figure><p>然后创建 Photon，模型名字格式是 <code>py:&#123;GIT_REPO_URL&#125;:&#123;PATH_TO_SCRIPT&#125;:&#123;CLASS_NAME&#125;</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lep photon create -n canny -m py:https://github.com/vra/canny-lepton-photon:canny.py:Canny</span><br></pre></td></tr></table></figure><p>再推送到云平台：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lep photon push -n canny</span><br></pre></td></tr></table></figure><p>注意：可以多次修改代码，多次创建同名的 Photon，会产生不同的版本，然后推送最新的 Photon 到云平台。</p><p>然后在云平台通过可视化界面部署，选择部署名字，部署的资源种类等：</p><p><img data-src="/imgs/leptonai/20231014023711.png"></p><p><img data-src="/imgs/leptonai/20231014023738.png"></p><p>部署完成后，就可以用下面的Python代码来推理结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> leptonai.client <span class="keyword">import</span> Client</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">api_token = os.environ.get(<span class="string">&#x27;LEPTON_API_TOKEN&#x27;</span>)</span><br><span class="line">client = Client(<span class="string">&quot;lq87wh9y&quot;</span>, <span class="string">&quot;canny&quot;</span>, token=api_token)</span><br><span class="line"></span><br><span class="line">data = client.run(</span><br><span class="line">  url=<span class="string">&quot;https://i.stack.imgur.com/WsJGN.jpg&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">image_data = io.BytesIO(data)</span><br><span class="line">image = Image.<span class="built_in">open</span>(image_data)</span><br><span class="line">image.save(<span class="string">&quot;canny.jpg&quot;</span>)</span><br></pre></td></tr></table></figure><p>原始图和Canny结果图：<br>![[results.png)</p><p>如果不想用LeptonAI Python包，可以用 requests网络请求包来完成服务：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">api_token = os.environ.get(<span class="string">&quot;LEPTON_API_TOKEN&quot;</span>)</span><br><span class="line">url = <span class="string">&quot;https://xxxx-canny.bjz.edr.lepton.ai/run&quot;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/json&quot;</span>,</span><br><span class="line">    <span class="string">&quot;accept&quot;</span>: <span class="string">&quot;image/png&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Authorization&quot;</span>: <span class="string">f&quot;Bearer <span class="subst">&#123;api_token&#125;</span>&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data = &#123;<span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://i.stack.imgur.com/WsJGN.jpg&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(url, headers=headers, json=data)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;b.png&quot;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(response.content)</span><br></pre></td></tr></table></figure><p>或者用部署页面提供的 cURL 命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">curl -X <span class="string">&#x27;POST&#x27;</span> \</span><br><span class="line">  <span class="string">&#x27;https://xxxx-canny.bjz.edr.lepton.ai/run&#x27;</span> \</span><br><span class="line">  -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> \</span><br><span class="line">  -H <span class="string">&#x27;accept: image/png&#x27;</span> \</span><br><span class="line">  -H <span class="string">&quot;Authorization: Bearer <span class="variable">$LEPTON_API_TOKEN</span>&quot;</span> \</span><br><span class="line">  --output output.png \</span><br><span class="line">  -d <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">  &quot;url&quot;: &quot;string&quot;</span></span><br><span class="line"><span class="string">&#125;&#x27;</span></span><br></pre></td></tr></table></figure><p>也可以在网页端 Deployment 页面的 Demo 模块中进行推理，输入图片 url 即可得到结果：<br><img data-src="/imgs/leptonai/20231014023552.png"></p><p>这里的 Canny 算法这是一个简单的示例，实际中可以替换为任意的 AI 模型。</p><p>从这个简单的例子可以看出来，LeptonAI 将 AI 部署中的网站服务后端搭建给省掉了，让开发者专注于写算法部分，而且客户端调用服务的方式也很多，尽可能地简化了AI 服务的调用。</p><h2 id="4-一个Android端-AI程序Demo"><a href="#4-一个Android端-AI程序Demo" class="headerlink" title="4. 一个Android端 AI程序Demo"></a>4. 一个Android端 AI程序Demo</h2><p>为了验证 LeptonAI 对于AI App 的开发有没有提速，对于没有移动端开发的我，尝试不写一行 Java或者Object-C/Swift代码，纯粹利用 Python写一个简单的AI应用。</p><p>验证发现，利用 LeptonAI 算法平台 + Beeware Python 移动端程序开发工具， 纯Python代码在手机上运行起来了一个文生图的App。</p><p>视频如下：</p><p>源码在这里，最核心的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">An Android Application based on Python and LeptonAI!</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> toga</span><br><span class="line"><span class="keyword">from</span> toga.style <span class="keyword">import</span> Pack</span><br><span class="line"><span class="keyword">from</span> toga.style.pack <span class="keyword">import</span> COLUMN, ROW</span><br><span class="line"><span class="keyword">from</span> toga.images <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image <span class="keyword">as</span> PIL_Image</span><br><span class="line"><span class="keyword">from</span> leptonai.client <span class="keyword">import</span> Client</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AISDK</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        LEPTON_API_TOKEN = <span class="string">&quot;xxxxxxxxxxxxxxx&quot;</span></span><br><span class="line">        self.client = Client(<span class="string">&quot;xxx&quot;</span>, <span class="string">&quot;sdxl&quot;</span>, token=LEPTON_API_TOKEN)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span>(<span class="params">self, prompt, img_save_path</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;ai processing begin...&#x27;</span>)</span><br><span class="line">        data = self.client.run(</span><br><span class="line">            num_inference_steps=<span class="number">25</span>,</span><br><span class="line">            prompt=prompt,</span><br><span class="line">            seed=<span class="number">42</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        image_data = io.BytesIO(data)</span><br><span class="line">        image = PIL_Image.<span class="built_in">open</span>(image_data)</span><br><span class="line">        image.save(img_save_path)</span><br><span class="line">        <span class="built_in">print</span>(image.size)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;ai processing done&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sdxlandroid</span>(<span class="params">toga.App</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">startup</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">        self.sdk = AISDK()</span><br><span class="line">        self.img_save_path = <span class="string">&#x27;./tmp.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line">        main_box = toga.Box(style=Pack(direction=COLUMN))</span><br><span class="line"></span><br><span class="line">        name_label = toga.Label(</span><br><span class="line">            <span class="string">&quot;Your prompt: &quot;</span>,</span><br><span class="line">            style=Pack(padding=(<span class="number">0</span>, <span class="number">5</span>))</span><br><span class="line">        )</span><br><span class="line">        self.name_input = toga.TextInput(style=Pack(flex=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        name_box = toga.Box(style=Pack(direction=ROW, padding=<span class="number">5</span>))</span><br><span class="line">        name_box.add(name_label)</span><br><span class="line">        name_box.add(self.name_input)</span><br><span class="line"></span><br><span class="line">        button = toga.Button(</span><br><span class="line">            <span class="string">&quot;Generate Image&quot;</span>,</span><br><span class="line">            on_press=self.run_aigc,</span><br><span class="line">            style=Pack(padding=<span class="number">5</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        main_box.add(name_box)</span><br><span class="line">        main_box.add(button)</span><br><span class="line"></span><br><span class="line">        self.main_window = toga.MainWindow(title=self.formal_name)</span><br><span class="line">        self.main_window.content = main_box</span><br><span class="line">        self.main_window.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_aigc</span>(<span class="params">self, widget</span>):</span></span><br><span class="line">        prompt = self.name_input.value</span><br><span class="line">        self.sdk.process(prompt, self.img_save_path)</span><br><span class="line"></span><br><span class="line">        image = Image(self.img_save_path)</span><br><span class="line">        image_view = toga.ImageView(image)</span><br><span class="line">        self.main_window.content.add(image_view)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="keyword">return</span> sdxlandroid()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>可以看到，用不到100行的 Python 代码就能做出一个 AIGC 的简易手机端 App！</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1. 背景&quot;&gt;&lt;/a&gt;1. 背景&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.lepton.ai/&quot;&gt;LeptonAI&lt;/a&gt;是贾扬清的创业项目，正如 slogan “Build AI The Simple Way” 所表明的，LeptonAI的目标是简化AI模型的部署。&lt;/p&gt;
&lt;p&gt;简单来说，LeptonAI 提供了 &lt;a href=&quot;(https://github.com/leptonai/leptonai&quot;&gt;Python SDK&lt;/a&gt; 和云平台。Python SDK 可以让没有AI领域知识的普通开发者调用2～3行命令就能部署一个AI模型，然后用一个curl语句或几行Python代码就能完成客户端请求；而 LeptonAI 云平台提供了CPU，GPU和存储资源，开发者可以把创建的AI模型部署到这里，就能提供公开对外的AI服务。&lt;/p&gt;
&lt;p&gt;AI模型创建支持 HuggingFace，也就是说可以将 HuggingFace 上海量的模型集成到自己的应用中。同时 LeptonAI 也支持从 GitHub 仓库创建 AI 模型，给了开发者更多的选择。&lt;/p&gt;
&lt;p&gt;经过一段时间的体验后发现，LeptonAI Python SDK 设计的很优雅，用起来很舒服，而云平台的操作也很丝滑，有贾扬清大神亲自操刀写代码，SDK和云平台的质量绝对是信得过的。&lt;br&gt;&lt;img data-src=&quot;/imgs/leptonai/20231014072616.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;那 LeptonAI 是否解决了一些 AI 部署中的痛点问题呢？我认为是的。根据之前的经验，跑一些 AI 开源模型成本还是挺高的。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="pip" scheme="http://vra.github.io/tags/pip/"/>
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="Pytorch" scheme="http://vra.github.io/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Language Modeling Is Compression 论文阅读</title>
    <link href="http://vra.github.io/2023/10/03/language-modeling-is-compression/"/>
    <id>http://vra.github.io/2023/10/03/language-modeling-is-compression/</id>
    <published>2023-10-03T04:57:30.000Z</published>
    <updated>2023-12-09T01:43:43.000Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>这是DeepMind 最近发表的一篇论文，题目翻译成中文是“语言模型即压缩”，是一个很简单但也有分量的观点。更有趣的是，论文中作者发现，自己的预训练大模型Chinchilla 70B只在文本训练集上训练后，在ImagNet 图像Patch上压缩率能达到43.4%，优于PNG算法的压缩率58.5%，在LibrSpeech语音数据集上，压缩率达到16.4%，优于语音压缩算法FLAC30.3%的压缩率。</p><p>说实话看到这个结论我还是很震惊的。一个是文本数据上训练的模型居然也能用于图像和语音的压缩，而且压缩率还高于常用的领域特定的压缩算法。</p><p>除了震惊，还很好奇大模型是怎么被当作压缩器来压缩文本图像和语音数据的。在好奇心的驱使下，我读了几遍这篇论文。</p><span id="more"></span><p>除了上面的结论外，论文中作者提供了许多有启迪的观点，比如：</p><ol><li>通过推导，说明了压缩最优即是预测最优，即压缩和预测的等价性</li><li>传统压缩算法的特点是上下文很长（比如gzip 32K字节），可以利用具有丰富信息的上下文来进行压缩算法的设计，而Transformer结构的语言模型则是上下文很短（比如2048字节），通过大量参数来调节压缩的结果</li><li>Transformer模型是基于tokenizer压缩的数据进行训练的，因此tokenizer也是一种压缩器</li><li>从最右压缩算法的角度来看，语言模型作为压缩器，最优的模型大小是和数据强绑定的，不可能无限扩大，从这个角度一定程度给出了LLM的理论上限</li><li>传统压缩算法也能当作一种生成模型，给语言模型的研究带来新的可能性</li></ol><p>在这篇论文阅读文章中，我尝试就这些结论和几个关心的问题给出自己的解读，如有不准确之处，欢迎指出。</p><h2 id="2-为什么说压缩即预测"><a href="#2-为什么说压缩即预测" class="headerlink" title="2. 为什么说压缩即预测"></a>2. 为什么说压缩即预测</h2><p>作者首先是介绍了信息论的一些基本理论，然后结合语言模型的优化目标，引出了压缩最优即是预测最优。具体过程下面展开。</p><p>对一个序列（如“AIXI”），无损压缩的过程是将其转换为一种高效的表示（如01序列），同时能从该表示序列中恢复出原始的数据。无损压缩其实是一种编码过程。</p><p>对于一个离散随机变量，其信息熵表示为H(X)，计算公式为H(X) = - ∑ P(x) log2 P(x)，其中，P(x)表示随机变量X取值为x的概率。</p><p>根据香农的信息熵理论，对于最优的编码方案，平均编码长度L至少应该不小于信息熵H(X)。当编码长度等于信息熵时，即L = H(X)，编码达到了最优。</p><p>算术编码 (Arithmetic Coding) 是一种高效的无损压缩算法，是一种最优编码。概括来说，它将每个编码区间按照待编码序列的概率分布来进行划分，对于出现概率更大的序列元素，赋予更大的编码区间。下图演示了算数编码的过程，可以看到将4字节的输入编码为了7bit的输出，压缩率为7/(4x8) = 21.8%。具体算术编码的算法细节可以参考<a href="https://zhuanlan.zhihu.com/p/390684936">这篇文章</a>。<br><img data-src="/imgs/language_modeling_is_compression/20231003142114.png"></p><p>实际情况中，表示随机变量X取值为x的概率P(x)经常是难以获得的，因此采用P^来近似。基于这种近似，最优编码的期望长度如下，实际是一种交叉熵的表示。<br><img data-src="/imgs/language_modeling_is_compression/20231003142945.png"><br>这个表达式也刚好是现有语言模型的loss优化目标，也就是说，最小化压缩率等价于最小化语言模型的loss，即最优压缩即最优语言模型，从而证明了压缩和预测的等价性。</p><p>总结来说，这一部分根据压缩的最优目标和语言模型Loss的一致性，推导出两者其实是等价的。</p><h2 id="3-语言模型如何进行压缩"><a href="#3-语言模型如何进行压缩" class="headerlink" title="3. 语言模型如何进行压缩"></a>3. 语言模型如何进行压缩</h2><h3 id="3-1-压缩算法选择"><a href="#3-1-压缩算法选择" class="headerlink" title="3.1 压缩算法选择"></a>3.1 压缩算法选择</h3><p>对于传统压缩算法，作者选择了两个通用的压缩算法和两个特定模态的压缩算法。通用的压缩算法选择的是gzip和LZMA2。gzip相信大家都不陌生，用过linux的人都应该见过。而LZMA2是7z使用的压缩算法。这两者都算是久经考验的使用广泛的生产环境压缩算法，虽然不见得是最前沿的。</p><p>图像领域的压缩算法是PNG，语音领域的压缩算法是FLAC，两者都是无损压缩算法。</p><p>作为对比的语言模型选择了两个系列的大小不同的模型，比较小的是vanilla的只有decoder的 Transformer结构，在enwiki8上预训练（注意没有在别的数据上finetune)。比较大的是DeepMind之前提出的Chinchilla 系列模型，也只在文本上训练，没有见过图像和语音数据。</p><h3 id="3-2-数据设置"><a href="#3-2-数据设置" class="headerlink" title="3.2 数据设置"></a>3.2 数据设置</h3><p>数据集的选择也是很有技巧。首先是对于传统压缩算法和LLM，应用场景大相径庭，该怎么选择测试集进行合理的比较呢。</p><p>对于语言模型，由于常见模型输入context是2048，因此选择了把数据切分成N段2048字节的数据，来进行测试。<br>对于传统压缩算法，显然是可以将整个数据都作为整体进行压缩的，而切分成小块进行压缩是对结果有损害的。所以作者对传统压缩算法采用了两种处理方式。</p><p>为了方便比较不同模态的数据，作者都是构造总大小为1GB的数据集。</p><p>文本数据，作者采用了截止到2006年3月3日的维基百科英文预料的前1e8个字节，作为enwiki8，前1e9字节作为enwiki9。可以看到，enwiki8是包含在enwiki9中的，而enwiki9刚好是1G Bytes。</p><p>图像数据采用的是ImageNet数据集，选择了488821张图片，每张图片选择32x64的patch，然后转换为灰度图(每个像素刚好1个字节），刚好是488821x32x64=1G Bytes。</p><p>语音数据是从LibriSpeech数据集中选择的，每段2048字节，选择了488821段，也是刚好1G字节。</p><h3 id="3-3-如何进行数据压缩"><a href="#3-3-如何进行数据压缩" class="headerlink" title="3.3 如何进行数据压缩"></a>3.3 如何进行数据压缩</h3><p>对于语言模型进行数据压缩的细节，论文没有描述，我只能说一下我的猜测。作者采用算术编码的方式，根据给定每个输入后模型的输出的概率，利用概率大小划分编码空间，对序列中所有输入元素执行完一次推理，得到最终的编码结果。</p><h3 id="3-4-压缩结果"><a href="#3-4-压缩结果" class="headerlink" title="3.4 压缩结果"></a>3.4 压缩结果</h3><p>下面的表格展示了整体的压缩结果。<br>chunk size 无穷大表示不进行数据的切分，也就是传统压缩算法的压缩过程。<br>压缩率等于压缩后文件大小/原始文件大小，其中Raw压缩率没有考虑模型尺寸大小，二调整后的压缩率将模型的参数量也计算进了压缩后的文件大小中，由于语言模型参数量很大，因此调整后的压缩率甚至超过了一百。</p><p><img data-src="/imgs/language_modeling_is_compression/20231003010543.png"></p><p>从这个表格中可以得出一些结论：</p><ol><li>传统压缩算法经过分块，确实有比较大的性能下降，说明大的上下文信息对压缩至关重要</li><li>针对图像设计的压缩算法PNG在语音数据上LibrSpeech也能得到较好的压缩结果</li><li>LZMA2通用压缩算法在LibriSpeech数据上压缩率优于语音特定的FLAC算法，还是有点出乎意料</li><li>由于传统压缩算法没有模型参数，因此调整后压缩率等于Raw压缩率</li><li>在enwik8上训练的Transformer模型，增大参数后在同模态的enwiki9上效果会变好，而在不同模态的图片和语音数据上，增大参数量到一定程度后效果反而下降</li><li>而Chinchilla语言模型参数量从1B到7B再到70B，压缩性能都是在稳步提升，与Vanilla Transformer呈现不同的现象</li></ol><p>下面图中是在enwiki8上预训练到不同参数量的Transformer在enwiki7/8/9上的压缩率，其中压缩率是调整后压缩率，也就是考虑了参数量。<br><img data-src="/imgs/language_modeling_is_compression/20231003091610.png"><br>从这里可以看出一些结论：</p><ol><li>随着参数量的增大，压缩性能都下降了（表现为压缩率增大），这是因为参数量的增大对结果的影响比较大</li><li>对于大的数据集(enwiki9)，更多参数的模型压缩性能更好，但更大的模型在小的数据集上效果比较差，也就是说从压缩的角度看，模型的Scaling，与测试集的大小有关</li></ol><p>为了对比不同压缩算法的生成能力，作者比较了gzip和Chinchilla 70B 在文本，图像和语音上的生成能力，具体来说，给出前面的的内容（文本是前1948个字节预测最后100个字节，图像和语音是给出前面一半预测后面一半），让算法来预测后面一半的内容。注意这个生成过程和压缩过程是不同的。</p><p>文本预测结果如下<br><img data-src="/imgs/language_modeling_is_compression/20231003092610.png"><br>可以看到语言模型预测的结果更连贯，而gzip预测的结果则没有明显的含义</p><p>语音预测上，gzip的预测更自然一些，而语言模型的预测则进入了循环，类似大家使用代码补全模型时经常会遇到的循环一样，不知道是不是同样的效应。<br><img data-src="/imgs/language_modeling_is_compression/20231003092857.png"></p><p>图像生成方面，语言模型和gzip都是从已知的图像信息中预测颜色值，不过两者的分布还是挺不一样的，gzip像是水平和竖直方向规律的噪声，而语言模型则明显的更能体验row-wise的效应。<br><img data-src="/imgs/language_modeling_is_compression/20231003093107.png"></p><p>另外一个实验是关于序列长度对压缩性能的影响。可以看到增大序列长度，三个任务上所有压缩算法的压缩性能都提升了。而Chinchilla 的压缩性能最好。<br><img data-src="/imgs/language_modeling_is_compression/20231003134503.png"><br>理论上来讲，gzip，lzma等传统压缩算法在序列长度增大后继续增大压缩性能，直到达到非切块场景的效果。</p><p>然后作者还比较了在Vanilla Transformer结构上，采用不同tokenization方法在3个不同参数的网络上的Raw压缩率。可以看到，对于比较小的网络(200K),增大tokenization的词汇大小能提高压缩性能，但针对大模型，增大词汇量压缩性能变差，可以看到tokenization的选择对结果还是有挺大影响的。<br><img data-src="/imgs/language_modeling_is_compression/20231003135231.png"></p><h2 id="5-疑问，思考和总结"><a href="#5-疑问，思考和总结" class="headerlink" title="5. 疑问，思考和总结"></a>5. 疑问，思考和总结</h2><p>看论文是有一个直观的疑问：使用大模型时，对同一个问题，输出的结果每次都是不一样的，怎么能保证解压缩时的结果的一致性呢？</p><p>其实大模型每次给出不同结果，是因为处理过程中故意增加了随机性，意图给出更丰富多样的回答。如果去掉随机性，得到的结果是完全一样的，比如哪个token的输出概率最大都是确定的。</p><p>还有这篇文章对于语言模型进行压缩的细节描述的太少了，甚至连一个示例图片或者示例伪代码都没有，导致读论文时很难想象这部分的细节，比如几个疑问，在文本上训练的语言模型是怎么在图像和语音任务上使用的，实际中怎么将语言模型当作算术编码来使用，等等。不知道作者为什么这么处理。</p><p>我觉得这篇论文可能会给相关研究方向带来新的想象空间，比如，文本语料上训练的模型也能在图像和音频数据上获得很好的压缩率，那是不是对于不同模态的输入，也可以设计同一个LLM来统一不同的任务呢？</p><p>再比如，既然像gzip这种传统压缩算法也能作为一个生成模型，而且运行成本远小于现有的LLM，那能否基于这种算法进行AI模型的演进呢？也许是一个更实用的方向。</p><p>总之这篇论文从压缩算法的角度提出了很多有意思有启发的观点，希望能对AI的研究产生新的前进推力。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h2&gt;&lt;p&gt;这是DeepMind 最近发表的一篇论文，题目翻译成中文是“语言模型即压缩”，是一个很简单但也有分量的观点。更有趣的是，论文中作者发现，自己的预训练大模型Chinchilla 70B只在文本训练集上训练后，在ImagNet 图像Patch上压缩率能达到43.4%，优于PNG算法的压缩率58.5%，在LibrSpeech语音数据集上，压缩率达到16.4%，优于语音压缩算法FLAC30.3%的压缩率。&lt;/p&gt;
&lt;p&gt;说实话看到这个结论我还是很震惊的。一个是文本数据上训练的模型居然也能用于图像和语音的压缩，而且压缩率还高于常用的领域特定的压缩算法。&lt;/p&gt;
&lt;p&gt;除了震惊，还很好奇大模型是怎么被当作压缩器来压缩文本图像和语音数据的。在好奇心的驱使下，我读了几遍这篇论文。&lt;/p&gt;</summary>
    
    
    
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="Paper Reading" scheme="http://vra.github.io/tags/Paper-Reading/"/>
    
  </entry>
  
  <entry>
    <title>Numpy set_printoptions函数用法</title>
    <link href="http://vra.github.io/2023/09/30/numpy-printoptions/"/>
    <id>http://vra.github.io/2023/09/30/numpy-printoptions/</id>
    <published>2023-09-30T04:57:30.000Z</published>
    <updated>2023-09-30T04:58:30.000Z</updated>
    
    <content type="html"><![CDATA[<p>Numpy是Python中常用的数值计算库，我们经常需要用到Numpy来打印数值，查看结果。为了能精确地控制Numpy打印的信息，Numpy提供了<code>set_printoptions</code> <a href="https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html">函数</a>，包含数个参数，能满足数值打印的需要。</p><p>这里以iPython中操作作为示例，从浅入深，一步步地探索set_printoptions提供的功能，如何来满足我们的打印需求。</p><span id="more"></span><h3 id="precision"><a href="#precision" class="headerlink" title="precision"></a>precision</h3><p>首先用Numpy创建一个float64 类型的np.ndarray，并打印数值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: a = np.random.rand(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">0.63039295</span> <span class="number">0.09185505</span> <span class="number">0.02203224</span>]</span><br></pre></td></tr></table></figure><p>可以看到输出的float数组保留了8位小数位，这是因为Numpy默认的设置是显示8位小数位。<br>如果只想显示2位小数位，该怎么操作呢？<br>这时候就需要用到<code>set_printoptions</code>的<code>precsion</code>的选项了，它就是用来控制显示的小数位：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: np.set_printoptions(precision=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">0.6304</span> <span class="number">0.0919</span> <span class="number">0.022</span> ]</span><br></pre></td></tr></table></figure><p>可以看到通过设置precsion=4，显示的数组输出保留4位小数。</p><p>⚠️需要注意的是，这个设置对float类型的数值无效：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">14</span>]: a = np.random.rand()</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: <span class="built_in">type</span>(a)</span><br><span class="line">Out[<span class="number">15</span>]: <span class="built_in">float</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">16</span>]: <span class="built_in">print</span>(a)</span><br><span class="line"><span class="number">0.40944018143470295</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">17</span>]: np.set_printoptions(precision=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: <span class="built_in">print</span>(a)</span><br><span class="line"><span class="number">0.40944018143470295</span></span><br></pre></td></tr></table></figure><p>所以使用时注意类型是<code>np.ndarray</code>还是<code>float</code>。</p><h3 id="suppress"><a href="#suppress" class="headerlink" title="suppress"></a>suppress</h3><p>假设我们需要获取一组很小的数值，并且需要显示结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">2</span>]: a = np.random.rand(<span class="number">3</span>) * <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">9.49522547e-06</span> <span class="number">4.55101001e-06</span> <span class="number">4.01284118e-06</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以看到打印时用了科学计数法。<br>有没有办法不使用科学计数法呢，<code>set_printoptions</code>提供了<code>suppress</code>参数，将其设置为<code>True</code>，就会禁用科学计数法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: np.set_printoptions(suppress=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">0.0000095</span>  <span class="number">0.00000455</span> <span class="number">0.00000401</span>]</span><br></pre></td></tr></table></figure><p><code>suppress</code> 参数有一个例外情况，就是对于整数部分大于8位的，即使设置<code>suppress=True</code> ，仍然会显示科学计数法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">8</span>]: a = np.random.rand(<span class="number">3</span>) * <span class="number">1e10</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">9.35772525e+09</span> <span class="number">4.14513333e+09</span> <span class="number">4.59176775e+09</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: a = np.random.rand(<span class="number">3</span>) * <span class="number">1e8</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">37984517.91633694</span> <span class="number">87748330.34519586</span> <span class="number">21101693.42416701</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: a = np.random.rand(<span class="number">3</span>) * <span class="number">1e9</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">4.46826342e+08</span> <span class="number">5.17327105e+08</span> <span class="number">9.07218130e+08</span>]</span><br></pre></td></tr></table></figure><p>那有没有办法解决这个问题呢？这里就需要用到<code>set_printoptions</code> 提供的另一个参数<code>formatter</code>。</p><h3 id="formatter"><a href="#formatter" class="headerlink" title="formatter"></a>formatter</h3><p><code>formatter</code>接受一个dict类型的参数，其中dict的key表示参数的类型，而dict的value则是一个函数或者format字符串，表示如何对对应的类型进行打印。</p><p>举个简单的例子，我想在所有float类型的数组的每个元素后面加一个字母<code>f</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">21</span>]: a = np.random.rand(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">22</span>]: np.set_printoptions(precision=<span class="number">4</span>, formatter=&#123;<span class="string">&#x27;float&#x27;</span>: <span class="keyword">lambda</span> x: <span class="built_in">str</span>(x) + <span class="string">&#x27;f&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">23</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">0.6925034861246904</span>f <span class="number">0.0613911477046164</span>f <span class="number">0.3348313234151774</span>f]</span><br></pre></td></tr></table></figure><p><code>formatter</code>参数是个dict，key是“float”，表示对float类型的数组进行操作，value是一个lambda函数，将输入转换为str字符串再加一个f。</p><p>在这里也可以看到，np.float64数组中元素的实际长度是16位小数。默认显示的8位数值只是它的一个近似。</p><p>除了lamda函数外，也可以用Python的format格式函数来作为<code>formatter</code>参数dict的value：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">33</span>]: a = np.random.rand(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">34</span>]: np.set_printoptions(formatter=&#123;<span class="string">&#x27;float&#x27;</span>: <span class="string">&#x27;&#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">35</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">0.70</span> <span class="number">0.91</span> <span class="number">0.82</span>]</span><br></pre></td></tr></table></figure><p>可以看到，这里可以用f-string和format函数中使用的语法格式，对于用惯f-string的小伙伴来说，以这种方式来控制显示格式简直太舒服了。</p><p>关于python format的语法，可以参考我之前写的<a href="https://zhuanlan.zhihu.com/p/632687543">教程</a>。</p><p>另外需要注意，设置<code>formatter</code>后，会覆盖<code>precision</code>参数，也就是显示多少位数以<code>formatter</code>中设置为准:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">25</span>]: a = np.random.rand(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">26</span>]: np.set_printoptions(precision=<span class="number">4</span>, formatter=&#123;<span class="string">&#x27;float&#x27;</span>: <span class="keyword">lambda</span> x: <span class="built_in">str</span>(x) + <span class="string">&#x27;f&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">27</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">0.1604388367489663</span>f <span class="number">0.6047908263355061</span>f <span class="number">0.1645621828526913</span>f]</span><br></pre></td></tr></table></figure><p>根据Numpy 文档，<code>formatter</code>支持的类型包括下面这些：</p><ul><li>‘bool’</li><li>‘int’</li><li>‘timedelta’ : a <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.timedelta64" title="numpy.timedelta64"><code>numpy.timedelta64</code></a></li><li>‘datetime’ : a <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.datetime64" title="numpy.datetime64"><code>numpy.datetime64</code></a></li><li>‘float’</li><li>‘longfloat’ : 128-bit floats</li><li>‘complexfloat’</li><li>‘longcomplexfloat’ : composed of two 128-bit floats</li><li>‘numpystr’ : types <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.bytes_" title="numpy.bytes_"><code>numpy.bytes_</code></a> and <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.str_" title="numpy.str_"><code>numpy.str_</code></a></li><li>‘object’ : <em>np.object</em>_ arrays</li><li>‘all’ : sets all types</li><li>‘int_kind’ : sets ‘int’</li><li>‘float_kind’ : sets ‘float’ and ‘longfloat’</li><li>‘complex_kind’ : sets ‘complexfloat’ and ‘longcomplexfloat’</li><li>‘str_kind’ : sets ‘numpystr</li></ul><p>好了，说了这么多，那回到上面的问题，到底该怎么控制整数位大于8的float数组不用科学计数法呢？有了formatter参数，就很简单了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">36</span>]: a = np.random.rand(<span class="number">3</span>) * <span class="number">1e10</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">37</span>]: np.set_printoptions(formatter=&#123;<span class="string">&#x27;float&#x27;</span>: <span class="string">&#x27;&#123;:.8f&#125;&#x27;</span>.<span class="built_in">format</span>&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">38</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">7694883457.28612423</span> <span class="number">864845466.08411431</span> <span class="number">6505022487.23314571</span>]</span><br></pre></td></tr></table></figure><p>使用format格式语言轻松完成。</p><p>有些时候，数组中的元素长度各不相同，打印时要么对不齐不好查看，要么自动转换为科学计数法也不好分析，利用<code>formatter</code>能够显示对齐的数值，大大方便了数据查看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: a = np.array(</span><br><span class="line">   ...: [[<span class="number">1</span>,  -<span class="number">1000</span>, <span class="number">2222222222.33333333</span>],</span><br><span class="line">   ...: [<span class="number">233</span>, <span class="number">240.03333333333333333333333333</span>, <span class="number">8.0</span>],</span><br><span class="line">   ...: [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]]</span><br><span class="line">   ...: )</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[[ <span class="number">1.00000000e+00</span> -<span class="number">1.00000000e+03</span>  <span class="number">2.22222222e+09</span>]</span><br><span class="line"> [ <span class="number">2.33000000e+02</span>  <span class="number">2.40033333e+02</span>  <span class="number">8.00000000e+00</span>]</span><br><span class="line"> [ <span class="number">1.00000000e+00</span>  <span class="number">2.00000000e+00</span>  <span class="number">3.00000000e+00</span>]]</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: np.set_printoptions(formatter=&#123;<span class="string">&#x27;float&#x27;</span>: <span class="string">&#x27;&#123;:&gt;20.8f&#125;&#x27;</span>.<span class="built_in">format</span>&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[[          <span class="number">1.00000000</span>       -<span class="number">1000.00000000</span>  <span class="number">2222222222.33333349</span>]</span><br><span class="line"> [        <span class="number">233.00000000</span>         <span class="number">240.03333333</span>           <span class="number">8.00000000</span>]</span><br><span class="line"> [          <span class="number">1.00000000</span>           <span class="number">2.00000000</span>           <span class="number">3.00000000</span>]]</span><br></pre></td></tr></table></figure><p>这里利用了<code>&gt;N</code>的format语法，向右对齐。</p><h3 id="threshold和edgeitems"><a href="#threshold和edgeitems" class="headerlink" title="threshold和edgeitems"></a>threshold和edgeitems</h3><p>假如我们有一个很大的数组(1024x4)，打印时默认只显示开始三行和最后三行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: a = np.random.rand(<span class="number">1024</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[[<span class="number">0.5159347</span>  <span class="number">0.06396333</span> <span class="number">0.18446106</span> <span class="number">0.06163127</span>]</span><br><span class="line"> [<span class="number">0.96894042</span> <span class="number">0.278889</span>   <span class="number">0.25117021</span> <span class="number">0.9757328</span> ]</span><br><span class="line"> [<span class="number">0.42980522</span> <span class="number">0.44724705</span> <span class="number">0.89322128</span> <span class="number">0.19697129</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">0.31956847</span> <span class="number">0.4790065</span>  <span class="number">0.45595315</span> <span class="number">0.98816687</span>]</span><br><span class="line"> [<span class="number">0.35240443</span> <span class="number">0.44400784</span> <span class="number">0.76815952</span> <span class="number">0.18499155</span>]</span><br><span class="line"> [<span class="number">0.33888548</span> <span class="number">0.50811964</span> <span class="number">0.32341108</span> <span class="number">0.98617324</span>]]</span><br></pre></td></tr></table></figure><p>这是因为Numpy默认设置，当数组的元素个数大于1000时，就会只显示开头和结尾部分。</p><p>如果想多显示一些数据，看更多内容，该怎么操作呢？<br><code>set_printoptions</code>提供了<code>threshold</code>参数，用于控制多少个元素后显示部分，另一个参数<code>edgeitems</code>,控制显示缩略部分的行数。</p><p>因此可以修改这两个参数，修改显示效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: np.set_printoptions(edgeitems=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[[<span class="number">0.5159347</span>  <span class="number">0.06396333</span> <span class="number">0.18446106</span> <span class="number">0.06163127</span>]</span><br><span class="line"> [<span class="number">0.96894042</span> <span class="number">0.278889</span>   <span class="number">0.25117021</span> <span class="number">0.9757328</span> ]</span><br><span class="line"> [<span class="number">0.42980522</span> <span class="number">0.44724705</span> <span class="number">0.89322128</span> <span class="number">0.19697129</span>]</span><br><span class="line"> [<span class="number">0.41831831</span> <span class="number">0.32864348</span> <span class="number">0.9599147</span>  <span class="number">0.04244498</span>]</span><br><span class="line"> [<span class="number">0.17307071</span> <span class="number">0.70541496</span> <span class="number">0.12485861</span> <span class="number">0.68987846</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">0.36880553</span> <span class="number">0.66404444</span> <span class="number">0.12623872</span> <span class="number">0.32754608</span>]</span><br><span class="line"> [<span class="number">0.53076768</span> <span class="number">0.76770867</span> <span class="number">0.36680954</span> <span class="number">0.58596153</span>]</span><br><span class="line"> [<span class="number">0.31956847</span> <span class="number">0.4790065</span>  <span class="number">0.45595315</span> <span class="number">0.98816687</span>]</span><br><span class="line"> [<span class="number">0.35240443</span> <span class="number">0.44400784</span> <span class="number">0.76815952</span> <span class="number">0.18499155</span>]</span><br><span class="line"> [<span class="number">0.33888548</span> <span class="number">0.50811964</span> <span class="number">0.32341108</span> <span class="number">0.98617324</span>]]</span><br></pre></td></tr></table></figure><h3 id="linewidth"><a href="#linewidth" class="headerlink" title="linewidth"></a>linewidth</h3><p>linewidth参数用来控制一行显示多少个字符，默认是75，通过修改这个参数，能在一行显示更多数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">n [<span class="number">3</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: a = np.random.rand(<span class="number">1024</span>,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: np.set_printoptions(precision=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[[<span class="number">0.6151590922948798</span> <span class="number">0.8394381715187383</span> <span class="number">0.1287492144726177</span></span><br><span class="line">  <span class="number">0.432486748198503</span>  <span class="number">0.008210600687992</span>  <span class="number">0.5251777687645207</span>]</span><br><span class="line"> [<span class="number">0.8986836534319551</span> <span class="number">0.5275521098334796</span> <span class="number">0.1275787604074625</span></span><br><span class="line">  <span class="number">0.2088067024068581</span> <span class="number">0.9728215202746345</span> <span class="number">0.0222310180458779</span>]</span><br><span class="line"> [<span class="number">0.1919751621010076</span> <span class="number">0.7593251629630882</span> <span class="number">0.2216025287318845</span></span><br><span class="line">  <span class="number">0.1693395870716256</span> <span class="number">0.0447174013709218</span> <span class="number">0.2669167788671162</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">0.2056367250351134</span> <span class="number">0.1961953658298233</span> <span class="number">0.6844119224272207</span></span><br><span class="line">  <span class="number">0.396808314963211</span>  <span class="number">0.2270659358855954</span> <span class="number">0.1694468143457141</span>]</span><br><span class="line"> [<span class="number">0.0404784779577213</span> <span class="number">0.977932794679906</span>  <span class="number">0.319154876583544</span></span><br><span class="line">  <span class="number">0.6301954893143036</span> <span class="number">0.4533581710958777</span> <span class="number">0.4980767389069806</span>]</span><br><span class="line"> [<span class="number">0.5722796781670568</span> <span class="number">0.8683487818109435</span> <span class="number">0.819417328117305</span></span><br><span class="line">  <span class="number">0.5286251921005498</span> <span class="number">0.2252964609019765</span> <span class="number">0.7439441509500194</span>]]</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: np.set_printoptions(linewidth=<span class="number">150</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[[<span class="number">0.6151590922948798</span> <span class="number">0.8394381715187383</span> <span class="number">0.1287492144726177</span> <span class="number">0.432486748198503</span>  <span class="number">0.008210600687992</span>  <span class="number">0.5251777687645207</span>]</span><br><span class="line"> [<span class="number">0.8986836534319551</span> <span class="number">0.5275521098334796</span> <span class="number">0.1275787604074625</span> <span class="number">0.2088067024068581</span> <span class="number">0.9728215202746345</span> <span class="number">0.0222310180458779</span>]</span><br><span class="line"> [<span class="number">0.1919751621010076</span> <span class="number">0.7593251629630882</span> <span class="number">0.2216025287318845</span> <span class="number">0.1693395870716256</span> <span class="number">0.0447174013709218</span> <span class="number">0.2669167788671162</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">0.2056367250351134</span> <span class="number">0.1961953658298233</span> <span class="number">0.6844119224272207</span> <span class="number">0.396808314963211</span>  <span class="number">0.2270659358855954</span> <span class="number">0.1694468143457141</span>]</span><br><span class="line"> [<span class="number">0.0404784779577213</span> <span class="number">0.977932794679906</span>  <span class="number">0.319154876583544</span>  <span class="number">0.6301954893143036</span> <span class="number">0.4533581710958777</span> <span class="number">0.4980767389069806</span>]</span><br><span class="line"> [<span class="number">0.5722796781670568</span> <span class="number">0.8683487818109435</span> <span class="number">0.819417328117305</span>  <span class="number">0.5286251921005498</span> <span class="number">0.2252964609019765</span> <span class="number">0.7439441509500194</span>]]</span><br></pre></td></tr></table></figure><p>可以看到，增加linewidth到150后，以前一行显示不了的数据现在可以在一行上显示了。</p><h3 id="nanstr和infstr"><a href="#nanstr和infstr" class="headerlink" title="nanstr和infstr"></a>nanstr和infstr</h3><p>nanstr和infstr参数用来控制nan和inf数值的显示字符，默认是<code>nan</code>和<code>inf</code>，如果好奇想修改的话，可以设置对应的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">12</span>]: a = np.array([np.nan, np.inf])</span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[nan inf]</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: np.set_printoptions(nanstr=<span class="string">&#x27;非数&#x27;</span>, infstr=<span class="string">&#x27;∞&#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[非数  ∞]</span><br></pre></td></tr></table></figure><p>有点好玩，但建议别修改，不然别人不知道你在do what 🤷</p><h3 id="sign"><a href="#sign" class="headerlink" title="sign"></a>sign</h3><p>sign参数用来控制每个数字前显示的符号，默认是<code>-</code>,也就是只有负数前面显示减号。如果是<code>+</code>，则在正数前面添加加号。如果是<code> </code>,则在正数前面添加空格：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: a = np.random.rand(<span class="number">3</span>) - <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[ <span class="number">0.44889495</span> -<span class="number">0.25608263</span> -<span class="number">0.23228835</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: np.set_printoptions(sign=<span class="string">&#x27;+&#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[+<span class="number">0.44889495</span> -<span class="number">0.25608263</span> -<span class="number">0.23228835</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: np.set_printoptions(sign=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[ <span class="number">0.44889495</span> -<span class="number">0.25608263</span> -<span class="number">0.23228835</span>]</span><br></pre></td></tr></table></figure><h3 id="恢复默认配置"><a href="#恢复默认配置" class="headerlink" title="恢复默认配置"></a>恢复默认配置</h3><p>如何恢复默认配置呢？可以这么设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.set_printoptions(edgeitems=<span class="number">3</span>, infstr=<span class="string">&#x27;inf&#x27;</span>,linewidth=<span class="number">75</span>, nanstr=<span class="string">&#x27;nan&#x27;</span>, precision=<span class="number">8</span>,suppress=<span class="literal">False</span>, threshold=<span class="number">1000</span>, formatter=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="使用with语句"><a href="#使用with语句" class="headerlink" title="使用with语句"></a>使用with语句</h3><p>通过使用with语句，可以临时修改打印配置项，在退出with语句的时候恢复默认配置，这样也减少侵入式地修改，避免造成不必要的后果。<br>在使用with语句时，需要将<code>np.set_printoptions</code> 替换为<code>np.printoptions</code>，也就是去掉函数中的<code>set_</code>前缀，函数的所有参数都一样。使用例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">6</span>]: a = np.random.rand(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: <span class="keyword">with</span> np.printoptions(formatter=&#123;<span class="string">&#x27;float&#x27;</span>: <span class="keyword">lambda</span> x: <span class="built_in">str</span>(x)+<span class="string">&#x27;f&#x27;</span>&#125;):</span><br><span class="line">   ...:     <span class="built_in">print</span>(a)</span><br><span class="line">   ...:</span><br><span class="line">[<span class="number">0.24752544521208586</span>f <span class="number">0.01852100917834376</span>f <span class="number">0.9358432384604951</span>f]</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">0.24752544521208586</span>f <span class="number">0.01852100917834376</span>f <span class="number">0.9358432384604951</span>f]</span><br></pre></td></tr></table></figure><p>这就是<code>set_printoptions</code>几乎全部的参数了，更多用法，参考官方<a href="https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html">文档</a>。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Numpy是Python中常用的数值计算库，我们经常需要用到Numpy来打印数值，查看结果。为了能精确地控制Numpy打印的信息，Numpy提供了&lt;code&gt;set_printoptions&lt;/code&gt; &lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html&quot;&gt;函数&lt;/a&gt;，包含数个参数，能满足数值打印的需要。&lt;/p&gt;
&lt;p&gt;这里以iPython中操作作为示例，从浅入深，一步步地探索set_printoptions提供的功能，如何来满足我们的打印需求。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="Numpy" scheme="http://vra.github.io/tags/Numpy/"/>
    
  </entry>
  
  <entry>
    <title>Code Llama 解读系列1-论文阅读</title>
    <link href="http://vra.github.io/2023/09/29/code-llama-1/"/>
    <id>http://vra.github.io/2023/09/29/code-llama-1/</id>
    <published>2023-09-29T02:17:04.000Z</published>
    <updated>2023-09-29T02:40:12.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Code Llama 是 Meta 基于 Llama 2 的代码生成AI模型， 在同类开源模型中取得比较好的结果。这里计划写3篇系列文章，从论文细节、代码使用、效果实测方面对 Code Llama 进行解读，欢迎关注我了解后续文章。</p></blockquote><h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h3><p>2023年8月24日，Meta 开源了基于 <a href="https://github.com/facebookresearch/llama">Llama 2</a>) 通用 LLM 的代码生成系列模型 <a href="https://github.com/facebookresearch/codellama">Code Llama</a>)，支持Python, C++, Java, PHP, TypeScript, C# 和 Bash 编程语言，而且支持学术研究和商业使用。</p><p>另外 Code Llama 官方代码只提供了一些简单的使用示例，没有提供生产环境可用的 VSCode 等 工具的插件，搜索了一下也没找到简单易用的第三方开发的插件。相信很快就会有人做出来的。如果你有看到基于 Code Llama 的 VSCode 或者 Vim 插件，欢迎评论指教。</p><span id="more"></span><p>一些链接：</p><ul><li>代码仓库: <a href="https://github.com/facebookresearch/codellama">https://github.com/facebookresearch/codellama</a></li><li>论文PDF: <a href="https://scontent-sjc3-1.xx.fbcdn.net/v/t39.2365-6/369856151_1754812304950972_1159666448927483931_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=wURKmnWKaloAX-JEHAz&_nc_ht=scontent-sjc3-1.xx&oh=00_AfBOeTPJWHrxyxjNs4TLPACB4M7xQIwQcM5SMRMzDo8uCg&oe=64EEAC4F">链接</a></li><li>Meta AI 博客文章：<a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/">链接</a></li></ul><h3 id="2-数据说明"><a href="#2-数据说明" class="headerlink" title="2. 数据说明"></a>2. 数据说明</h3><p>这篇论文中提到了几个不同的数据，有一些数据的构造还是挺巧妙的，这里列出来，希望能给大家一些启发。</p><h4 id="2-1-2T-token-dataset"><a href="#2-1-2T-token-dataset" class="headerlink" title="2.1 2T token dataset"></a>2.1 2T token dataset</h4><p>首先是 Llama 2 的训练数据，虽然不是这篇论文的贡献，但因为 Code Llama 模型都是从 Llama 2 初始化的，所以 这些代码生成的特化模型也都是见过这些数据的，包含它们中的知识。</p><p>Llama 2 是使用 2T token 数据训练的，其中代码相关的部分有80B token，占比只有4%。</p><h4 id="2-2-500B-token-dataset"><a href="#2-2-500B-token-dataset" class="headerlink" title="2.2 500B token dataset"></a>2.2 500B token dataset</h4><p>这篇论文先是提出了通用的 500B tokens 数据集, 85% 都是关于代码的，所以可以认为这个500B 就是一个代码数据集。</p><h4 id="2-3-100B-token-Python-dataset"><a href="#2-3-100B-token-Python-dataset" class="headerlink" title="2.3 100B token Python dataset"></a>2.3 100B token Python dataset</h4><p>除了通用的 500B token 代码数据集，为了提高对 Python 代码的生成能力，论文又提出了 100B token Python dataset。</p><h4 id="2-4-RLHF-V5-dataset"><a href="#2-4-RLHF-V5-dataset" class="headerlink" title="2.4 RLHF V5 dataset"></a>2.4 RLHF V5 dataset</h4><p>这是 Llama 2 论文中使用的人工纠正的数据集，是为了让代码可以更好的对应回答提问者的指令，也为了生成更安全的代码（可以理解成对生成的代码中某些危险的代码进行过滤？）</p><h4 id="2-5-self-instruct-5B-token-dataset"><a href="#2-5-self-instruct-5B-token-dataset" class="headerlink" title="2.5 self-instruct 5B token dataset"></a>2.5 self-instruct 5B token dataset</h4><p>self-instruct 是一个生成代码数据集，通过给llama2提问代码任务，得到它的结果，作为gt。 会不会存在错误答案？这里论文设计了一个很精巧的方案来构造生成代码数据：</p><ol><li>让 LLama 2 70B 大模型设计 Easy 和 Medium 难度的编程题，每次出50道题目，要求不重复，且可以用一个单独的Python函数来实现。总共得到52000个去重的问题。下面是提示词和一些回答的例子：<br><img data-src="/imgs/code_llama_1/20230826223518.png"></li><li>对上面得到的每个问题，用 Code Llama 7B 模型生成 单元测试代码和10个解题的代码，然后在解题代码上运行单元测试，将第一个通过单元测试的代码加入到 self-instruct 数据集中。</li></ol><p>这是一种很巧妙的设计，通过单元测试来判断代码的对错，能够做到完全自动化地构造数据。</p><p>当然如果单元测试代码本身错，那可能会将错误的解题代码加入到训练集中。而根据<a href="https://arxiv.org/abs/2308.02312">这篇论文</a>)的分析，作为最强的 LLM，ChatGPT 生成的代码错误率为52%。所以有理由认为  Code Llama 7B 生成的单元测试代码也会有错误，因此 self-instruct 不是一个完美的数据集。</p><h3 id="3-训练策略"><a href="#3-训练策略" class="headerlink" title="3. 训练策略"></a>3. 训练策略</h3><p>论文中针对不同的模型，尝试了不同的训练策略，整体来说是和数据集比较匹配的。</p><h4 id="3-1-从头训练-vs-Finetune"><a href="#3-1-从头训练-vs-Finetune" class="headerlink" title="3.1 从头训练 vs Finetune"></a>3.1 从头训练 vs Finetune</h4><p>论文中实验发现，采用通用llm (Llama 2)初始化，再在code数据集上finetune比在code数据集上从头训练效果要好。但同时也发现，只使用 Llama 2 模型来做代码生成，效果比 Llama 2 + Code 数据集训练要差，可见 2T token  pretrain + 500B token finetune才是做通用代码生成的最好选择。</p><h4 id="3-2-代码补全功能"><a href="#3-2-代码补全功能" class="headerlink" title="3.2 代码补全功能"></a>3.2 代码补全功能</h4><p>上面的基本训练策略中只会给定前面的代码，补全或预测后面的代码，但在有些常见，是已知前面和后面的代码，给出中间的代码，比如docstring的生成，就需要知道前面的内容(函数的名字和参数)和后面的内容（函数的具体实现），才能给出比较准确的函数说明docstring。这种任务模式论文中称之为补全 (Infilling)。</p><p>这种需求跟 LLM 预测下一个 token 的任务模式是不同的，因此需要对训练模式进行改造。总体来说，论文采用了 Casual Mask 的模式来训练网络，也就是将训练序列中间的一部分移动到最后，让网络来预测这部分内容。具体来说，将训练中的token分割为前缀、中间部分和后缀部分，分割位置利用均匀分布来确定。训练时以一半的概率喂前缀-后缀-中间（PSM）格式 token 序列，一半的概率喂后缀-前缀-中间（SPM）格式的 token 序列。</p><h4 id="3-3-长上下文输入微调"><a href="#3-3-长上下文输入微调" class="headerlink" title="3.3 长上下文输入微调"></a>3.3 长上下文输入微调</h4><p>Llama 2 模型的最长 token 数目为4096，对于代码生成任务来说，还是比较小，比如分析整个仓库中的代码，可能很容易超出限制。因此 Code Llama 在 finetune 阶段将 token 数从4096 提升到16384，提升了4倍。</p><p>位置embedding 采用旋转位置embedding, query 和 key vector都是 Rxn的一个线性组合，而R是一个块对角矩阵，也就是只有对角线和附近的4个值非零，每个位置i处的R公式如下：<br><img data-src="/imgs/code_llama_1/20230827002151.png"></p><p><img data-src="/imgs/code_llama_1/20230827002347.png"><br>d为总的token 维度。</p><h4 id="3-4-指令finetune"><a href="#3-4-指令finetune" class="headerlink" title="3.4 指令finetune"></a>3.4 指令finetune</h4><p>这部分也是为了生成更安全的代码，也更好地针对提问者的问题进行更人性化的回答。个人理解，这部分本身在策略上没有太多trick，核心是数据的构造和采集。</p><h3 id="4-模型说明"><a href="#4-模型说明" class="headerlink" title="4. 模型说明"></a>4. 模型说明</h3><p>基于几种数据和几个训练策略，就能得到不同的模型。<br>Code Llama 系列包含三大类模型，每类模型包含 7B, 13B 和 34B 三种参数大小，共9个模型。</p><p><img data-src="/imgs/code_llama_1/20230826222457.png"></p><p>第一类是Code Llama 通用代码生成模型，采用 Llama 2 的模型参数初始化，在 500B token 数据集上训练。其中 7B 和 13B 模型还进行了代码补全数据集上的训练，适用于 IDE 中实时的代码补全，而 34B 因为速度问题，并不适合实时补全，更适合作为编程助手。</p><p>第二类是 Code Llama-Python，这是针对 Python 专门优化的模型，在 500B 通用数据训练的基础上，又在额外的 100B Python 数据集上进行了finetune。</p><p>第三类是 Code. Llama-Instruct，在 Code Llama 通用模型基础上，增加了在 RLHF V5 和 self-instruct 数据集上的 finetune 过程，可以生成更符合指令需求的代码。</p><h3 id="5-结果对比"><a href="#5-结果对比" class="headerlink" title="5. 结果对比"></a>5. 结果对比</h3><p>论文中比较了非常多测试集上的指标，太多反而不知道模型的效果到底怎么样，所以这里也不列出来了。下面放的是博客文章中的和别的模型的对比表格，反而比较简洁，可以做一个大致的对比。</p><p><img data-src="/imgs/code_llama_1/20230826213734.png"></p><p>当然根据我之前的 LLMs 使用经验，实际使用时的智能感受貌似不能很好地和 Benchmark 上的结果对应起来，相差几个点对最终结果的提升有多大不太好说。 所以 Code Llama 具体使用体验如何，留待下一篇文章来分析。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Code Llama 是 Meta 基于 Llama 2 的代码生成AI模型， 在同类开源模型中取得比较好的结果。这里计划写3篇系列文章，从论文细节、代码使用、效果实测方面对 Code Llama 进行解读，欢迎关注我了解后续文章。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1. 背景&quot;&gt;&lt;/a&gt;1. 背景&lt;/h3&gt;&lt;p&gt;2023年8月24日，Meta 开源了基于 &lt;a href=&quot;https://github.com/facebookresearch/llama&quot;&gt;Llama 2&lt;/a&gt;) 通用 LLM 的代码生成系列模型 &lt;a href=&quot;https://github.com/facebookresearch/codellama&quot;&gt;Code Llama&lt;/a&gt;)，支持Python, C++, Java, PHP, TypeScript, C# 和 Bash 编程语言，而且支持学术研究和商业使用。&lt;/p&gt;
&lt;p&gt;另外 Code Llama 官方代码只提供了一些简单的使用示例，没有提供生产环境可用的 VSCode 等 工具的插件，搜索了一下也没找到简单易用的第三方开发的插件。相信很快就会有人做出来的。如果你有看到基于 Code Llama 的 VSCode 或者 Vim 插件，欢迎评论指教。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="Deep Learning" scheme="http://vra.github.io/tags/Deep-Learning/"/>
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="Code Llama" scheme="http://vra.github.io/tags/Code-Llama/"/>
    
  </entry>
  
  <entry>
    <title>FastViT 论文阅读</title>
    <link href="http://vra.github.io/2023/09/01/fastvit/"/>
    <id>http://vra.github.io/2023/09/01/fastvit/</id>
    <published>2023-09-01T07:56:52.000Z</published>
    <updated>2023-09-29T02:15:14.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p>论文地址：<a href="https://arxiv.org/abs/2303.14189">arxiv</a><br>代码地址：<a href="https://github.com/apple/ml-fastvit">ml-fastvit</a></p><p>FastViT 是苹果公司在 ICCV 2023上发表的网络结构设计的论文，在速度和精度上取得比较好的折衷，速度上既能和MobileOne这种轻量级网络匹敌，精度上也不输PoolFormer、ConvNeXt等比较新的大网络结构。</p><span id="more"></span><p><img data-src="/imgs/fastvit/20230901135142.png"></p><p>这是网络整体的结构图：<br><img data-src="/imgs/fastvit/20230901155048.png"><br>整体还是分成Stem和4个Stage，以及最后的输出Head。可以看到所有结构都在推理时进行了重参数化，保证只有一个网络分支。虽然叫ViT，但网络的核心还是由Conv层组成。</p><p>整个网络的的大部分模块是以MobileOne 的核心 MobileOneBlock 打底的，所以说是 MobileOne V2 也不为过。</p><p>比较有意思的是，FastVit 这篇论文的作者列表、作者顺序都和 MobileOne 一模一样！<br><img data-src="/imgs/fastvit/20230901154317.png"><br><img data-src="/imgs/fastvit/20230901154255.png"></p><p>所以可以说，FastViT 是 MobileOne 框架的延续，核心是在推理的时候保证只有一条网络分支，提升网络的推理速度。</p><p>具体来说，为了提升效果，网络设计上参考了比较新的 ConvMixer 结构。然后为了保证能够重参数化，将其中的非线性层省略掉，去掉残差模块。为了缓解 Self-Attention 模块计算量太大的问题，在浅层特征图比较大的情况下，采用 Large Kernel，也就是7x7 Kernel Size 的Conv网络。</p><p>下面依次对网络的几个核心模块进行说明。</p><h3 id="2-RepMixer"><a href="#2-RepMixer" class="headerlink" title="2. RepMixer"></a>2. RepMixer</h3><p>ConvMixer 提出了用Conv网络替代ViT网络的方法，在效果上超越了ViT方法。</p><p>已有的一些方法已经验证，Skip-Connection因为会有额外的内存访问开销，因此会显著增加网络延迟，如果能合并Skip-Connection，对于网络的加速会有很帮助。注意论文中的Skip-Connection其实指的是类似残差模块中的两个分支相加的操作（如下图），而不是更常见的Encoder和Decoder之间的跳层连接。<br><img data-src="/imgs/fastvit/20230901143032.png"></p><p>FastViT利用了 ConvMixer 网络结构优异的性能，同时为了能够在推理时进行重参数化，对 ConvMixer 进行了几个修改：</p><ol><li>去掉非线性层，否则没法进行重参数化</li><li>将BN放在DepthWiseConv之前</li><li>在推理时合并 Skip-Connection，用来加速推理。</li></ol><p>具体代码实现时，训练时采用了2个MobileOneBlock，分别表示mixer和normal，与原始输入x相加；推理的时候去掉残差相加，直接转换为一个MobileOne模块：<br><img data-src="/imgs/fastvit/20230901144427.png"></p><h3 id="3-训练时过参数化"><a href="#3-训练时过参数化" class="headerlink" title="3. 训练时过参数化"></a>3. 训练时过参数化</h3><p>过参数化是指训练的时候将结构相同的网络模块重复多遍，通过增加模型的复杂度来提点。在推理的时候，再通过重参数化trick将多个分支的结构合并到一个分支来提速。下面是过参数化的示意图（图片来自<a href="https://zhuanlan.zhihu.com/p/560894077">这里</a>):<br><img data-src="/imgs/fastvit/20230901150703.png"></p><p>MobileOne 论文中就采用了过参数模块，验证可以提高网络的学习能力。</p><p>在这篇论文中，为了提速，先是将普通的 KxK 的Conv修改为DepthWise KxK 的 Conv + 1x1 PointWise 的 Conv层，发现在提速后精度下降，例如论文中 Table 1 所示，这步修改后耗时从 1.58ms 下降到 1.26ms，但精度也从78.5下降到78.0:<br><img data-src="/imgs/fastvit/20230901151134.png"></p><p>为了弥补这一步造成的精度损失，作者叠加了上面提到的训练时重参数化的trick，保证速度不变的情况下，效果超过了之前的方法，从78.0上升到78.9。</p><p>当然这部分的结构优化其实比较”水”，是现有的两个工作的简单组合……</p><h3 id="4-Large-Kernel"><a href="#4-Large-Kernel" class="headerlink" title="4. Large Kernel"></a>4. Large Kernel</h3><p>由于Transformer结构的核心模块是Self-Attention模块，而且已经被无数实验验证具有强大的特征提取能力。<br>但Self-Attention的计算量很大，要做到手机上实时难度不小。</p><p>作者认为，Self-Attention 效果好跟它有很大的感受野有关系。而普通 Conv 层通过增加 Kernel. Size，也能达到提高感受野的效果。</p><p>因此最终网络结构设计上，在每个Stage开始的时候，采用 7x7 的 MobileOneBlock。7x7 的 Kernel Size 也是通过实验试出来的。</p><p>为了既能跟MobileOne这种轻量级网络对比，又能在 ImageNet 上和别的模型一较高下，论文中提出了7个 Fast-ViT的变种，各个变种的设置如下：<br><img data-src="/imgs/fastvit/20230901153635.png"></p><h3 id="5-实验"><a href="#5-实验" class="headerlink" title="5. 实验"></a>5. 实验</h3><p>对比实验在 ImageNet-1K 分类任务、COCO 物体检测，ADE20K 语义分割等标准任务上进行了对比</p><p><img data-src="/imgs/fastvit/20230901153843.png"><br><img data-src="/imgs/fastvit/20230901153923.png"><br><img data-src="/imgs/fastvit/20230901153936.png"><br>另外这篇论文还比较了FastVit在3D手重建这个下游任务上的效果，也是比MobRecon这些端侧实时的方法效果更好，当然还是刷不过MeshGraphormer等基于HRNet Backbone的模型。<br><img data-src="/imgs/fastvit/20230901135311.png"></p><h3 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h3><p>整个论文是比较实用的，没有太多自己的原创性的点子，更多的是将一些现有的网络结构设计思想融合进MobileOne的推理时单分支的网络结构中来。</p><p>另外一个值得注意的事情是，论文中给出的Mobile Latency都很低，像 FastVit-MA36 7.9G 的FLOPS，移动端延迟4.5毫秒。但要明白这是用iPhone 12 Pro Max上使用CoreML来测试的，本身iPhone 12 Pro Max 采用的A14芯片很强，而且CoreML针对苹果的硬件有专门的优化，所以在安卓机器或者低端一些的iPhone 上，采用别的推理引擎（如ONNX， MNN， TCNN）进行推理时，很有可能达不到这么高的速度，所以像 FastVit-MA36这种FLOPS 约为8G的模型在手机上用起来还是需要验证的。</p><p>总之对于想试用 FastViT 的小伙伴来说，用就完了，代码已经开源，也不存在复现的问题，直接用起来，好用就加入到自己的任务中，效果比较差或者速度有瓶颈抛弃即可。</p><p>另外 FastViT 的代码实现很简洁优雅，阅读起来很舒服，后面有空可以写一篇代码阅读的文章，欢迎感兴趣的小伙伴关注、点赞和评论区留言～</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h3&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/2303.14189&quot;&gt;arxiv&lt;/a&gt;&lt;br&gt;代码地址：&lt;a href=&quot;https://github.com/apple/ml-fastvit&quot;&gt;ml-fastvit&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;FastViT 是苹果公司在 ICCV 2023上发表的网络结构设计的论文，在速度和精度上取得比较好的折衷，速度上既能和MobileOne这种轻量级网络匹敌，精度上也不输PoolFormer、ConvNeXt等比较新的大网络结构。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Deep Learning" scheme="http://vra.github.io/tags/Deep-Learning/"/>
    
    <category term="Pytorch" scheme="http://vra.github.io/tags/Pytorch/"/>
    
    <category term="论文阅读" scheme="http://vra.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>一个简单好用的Python并行函数</title>
    <link href="http://vra.github.io/2023/08/12/python-parallel-function/"/>
    <id>http://vra.github.io/2023/08/12/python-parallel-function/</id>
    <published>2023-08-12T15:43:21.000Z</published>
    <updated>2023-08-12T15:44:23.000Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h3><p>用Python跑有大量数据的任务的时候，启用多进程加速效果明显。但因为我之前在使用Python的多进程库时总遇到卡住的问题，后来对这块避而远之，总是用别的方法来加速。最近发现OpenMMLab的一些库提供了多进程并行的函数功能，简单好用。比如一个简单的toy例子，OpenCV读图像，resize然后保存，在8个CPU核的 Mac 上，加速比能达到3.4倍(45ms vs 13ms)，也就是以前要跑3个多小时的任务，现在1个小时就能搞定，省了不少时间，更多实际例子也证明了这个函数的加速效果，还是挺实用的。这里写个教程，希望也能方便到别的有同样需要的人，当然同类型的库应该也有很多，这里只是取一瓢饮。</p><span id="more"></span><h3 id="2-函数实现"><a href="#2-函数实现" class="headerlink" title="2. 函数实现"></a>2. 函数实现</h3><p>具体实现是<a href="https://github.com/open-mmlab/mmengine">mmengine</a>中的<a href="https://github.com/open-mmlab/mmengine/blob/main/mmengine/utils/progressbar.py#L109">track_parallel_progress</a>函数，它底层也是调用了Python系统库的<a href="https://docs.python.org/3/library/multiprocessing.html">multiprocessing</a>，进行多进程加速脚本的运行。所以原理上来说我们也可以不用这个函数，自己写multiprocessing调用代码。但mmengine的这个封装，给我们省去了写multiprocessing比较复杂的调度代码的时间，拿来直接用还是能加速代码的开发节奏。</p><p>大致的调用框架:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="keyword">import</span> mmengine</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mmengine_track_func</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="comment"># wraps的作用是将装饰器的信息都传递给被装饰的函数，</span></span><br><span class="line">    <span class="comment"># 参考：https://stackoverflow.com/a/309000</span></span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapped_func</span>(<span class="params">args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> func(*args)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapped_func</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@mmengine_track_func</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">your_func</span>(<span class="params">arg1, arg2</span>):</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进程数</span></span><br><span class="line">NUM_PROC = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造调用参数</span></span><br><span class="line">params = [(arg1, arg2) <span class="keyword">for</span> arg1, arg2 <span class="keyword">in</span> <span class="built_in">zip</span>(arg1_list, arg2_list)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用mmengine封装好的多进程函数</span></span><br><span class="line">results = mmengine.track_parallel_progress(your_func, params, nproc=NUM_PROC)</span><br></pre></td></tr></table></figure><p>使用时需要先 <code>pip install mmengine</code>来安装依赖库 mmengine。</p><p>然后这里构造了一个装饰器<code>mmengine_track_func</code>，对实际调用的函数<code>your_func</code>进行封装。其中用到了functools中的wraps函数，它的作用是将装饰器的信息都传递给被装饰的函数，具体例子可以参考这个<a href="https://stackoverflow.com/a/309000">回答</a>。</p><p>实际使用时<code>mmengine_track_func</code> 不需要修改，直接采用这种形式。</p><p>然后是设置进程数，构造你自己函数的参数，再调用<code>mmengine.track_parallel_progress</code> 即可，它的必需的三个参数分别是:</p><ol><li>你的函数名</li><li>函数参数list</li><li>设置的进程数</li></ol><p>别的非必需参数可以参考<a href="https://github.com/open-mmlab/mmengine/blob/main/mmengine/utils/progressbar.py#L109">源码</a>。</p><h3 id="3-toy-例子"><a href="#3-toy-例子" class="headerlink" title="3. toy 例子"></a>3. toy 例子</h3><p>这里举一个简单的伪造例子，读取本地某个目录下的png图像，将它们都缩放到200x200，再保存到本地。完整代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> mmengine</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mmengine_track_func</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="comment"># wraps的作用是将装饰器的信息都传递给被装饰的函数，</span></span><br><span class="line">    <span class="comment"># 参考：https://stackoverflow.com/a/309000</span></span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapped_func</span>(<span class="params">args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> func(*args)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapped_func</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@mmengine_track_func</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">idx, img_path</span>):</span></span><br><span class="line">    img = cv2.imread(img_path)</span><br><span class="line">    img = cv2.resize(img, (<span class="number">200</span>, <span class="number">200</span>))</span><br><span class="line"></span><br><span class="line">    op = <span class="string">f&quot;<span class="subst">&#123;idx&#125;</span>.jpg&quot;</span></span><br><span class="line">    cv2.imwrite(op, img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 获取所有图片路径</span></span><br><span class="line">    img_paths = glob.glob(<span class="string">&quot;/path/to/folder/*.png&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试开多线程版本，耗时 13ms</span></span><br><span class="line">    params = [(idx, img_path) <span class="keyword">for</span> idx, img_path <span class="keyword">in</span> <span class="built_in">enumerate</span>(img_paths)]</span><br><span class="line">    mmengine.track_parallel_progress(run, params, nproc=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试不开多线程版本，耗时45ms</span></span><br><span class="line">    t0 = time.time()</span><br><span class="line">    <span class="keyword">for</span> idx, ip <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(img_paths)):</span><br><span class="line">        run.__wrapped__(idx, ip)</span><br><span class="line">    t1 = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;time:&quot;</span>, t1 - t0)</span><br></pre></td></tr></table></figure><p>这里有一个小的Python知识点：可以通过<code>func.__wrapped__</code> 属性来获取 <em>被装饰的函数</em> 对应的原始函数。</p><p>输出结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;] 4000/4000, 316.3 task/s, elapsed: 13s, ETA:     0s</span><br><span class="line">4000it [00:45, 88.84it/s]</span><br><span class="line">time: 45.0268120765686</span><br></pre></td></tr></table></figure><p>可以看到耗时从45ms下降到13ms，加速比3.4倍。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1. 背景&quot;&gt;&lt;/a&gt;1. 背景&lt;/h3&gt;&lt;p&gt;用Python跑有大量数据的任务的时候，启用多进程加速效果明显。但因为我之前在使用Python的多进程库时总遇到卡住的问题，后来对这块避而远之，总是用别的方法来加速。最近发现OpenMMLab的一些库提供了多进程并行的函数功能，简单好用。比如一个简单的toy例子，OpenCV读图像，resize然后保存，在8个CPU核的 Mac 上，加速比能达到3.4倍(45ms vs 13ms)，也就是以前要跑3个多小时的任务，现在1个小时就能搞定，省了不少时间，更多实际例子也证明了这个函数的加速效果，还是挺实用的。这里写个教程，希望也能方便到别的有同样需要的人，当然同类型的库应该也有很多，这里只是取一瓢饮。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="mmengine" scheme="http://vra.github.io/tags/mmengine/"/>
    
  </entry>
  
</feed>
