<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yunfeng&#39;s Simple Blog</title>
  
  <subtitle>Love, Life, Linux</subtitle>
  <link href="http://vra.github.io/atom.xml" rel="self"/>
  
  <link href="http://vra.github.io/"/>
  <updated>2023-12-09T12:22:48.572Z</updated>
  <id>http://vra.github.io/</id>
  
  <author>
    <name>Yunfeng Wang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>talkGPT4All 2.5-更多模型以及更加真实的TTS</title>
    <link href="http://vra.github.io/2023/11/22/talkgpt4all-2.5/"/>
    <id>http://vra.github.io/2023/11/22/talkgpt4all-2.5/</id>
    <published>2023-11-22T04:57:30.000Z</published>
    <updated>2023-12-09T12:22:48.572Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p><a href="https://link.zhihu.com/?target=https://github.com/vra/talkGPT4All">talkGPT4All</a>是基于<a href="https://link.zhihu.com/?target=https://gpt4all.io/index.html">GPT4All</a>的一个语音聊天程序，运行在本地CPU上，支持Linux，Mac和Windows。它利用OpenAI的Whisper模型将用户输入的语音转换为文本，再调用GPT4All的语言模型得到回答文本，最后利用文本转语音(TTS)的程序将回答文本朗读出来。</p><p>今年4、5月份的时候，我发布了talkGPT4All 1.0版本和2.0版本，链接见下：</p><p><a href="https://zhuanlan.zhihu.com/p/618826760">talkGPT4All: 基于GPT4All的智能语音聊天程序</a><br><a href="https://zhuanlan.zhihu.com/p/632592897">talkGPT4All 2.0:现在支持8个语言模型了</a></p><p>大家反馈最大的问题是TTS太机械了，听着很难受（具体可以看前面两篇文章的评论区）。而最近TTS领域的进展很多，例如很受欢迎的 coqui-ai的<a href="https://github.com/coqui-ai/TTS">TTS</a> 库，提供了TTS、声音克隆和声音变换的功能。上周末尝试了一下，发现内置了一些开箱即用的TTS模型，刚好可以集成到 talkGPT4All 中，解决目前采用的 <a href="https://pypi.org/project/pyttsx3/">pyttsx3</a>合成声音太机械的问题。</p><span id="more"></span><p>另外查看 GPT4All 的文档，从2.5.0开始，之前的.bin 格式的模型文件不再支持，只支持.gguf 格式的模型。因此我也是将上游仓库的更新合并进来，修改一下 talkGPT4All 的接口。</p><p>由于GPT4All 是从2.5.0开始不兼容.bin 格式老模型的，是一个很大的 break change。为了统一，我将更新后的 talkGPT4All 版本也命名为 2.5.0。</p><p>2.5.0版本效果视频见<a href="https://zhuanlan.zhihu.com/p/668275615">这里</a>。</p><h3 id="2-如何使用"><a href="#2-如何使用" class="headerlink" title="2. 如何使用"></a>2. 如何使用</h3><p>如果想直接使用的话，采用pip安装talkGPT4All包即可：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install talkgpt4all</span><br></pre></td></tr></table></figure><p>安装完后进入聊天：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">talkgpt4ll </span><br></pre></td></tr></table></figure><p>talkGPT4All 现在支持15个模型，可以通过-m 来切换你想用的GPT模型，所有模型列表见 3.2章节。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">talkgpt4all -m gpt4all-13b-snoozy-q4_0.gguf</span><br></pre></td></tr></table></figure><h3 id="3-实现细节"><a href="#3-实现细节" class="headerlink" title="3. 实现细节"></a>3. 实现细节</h3><p>这里重点讲一下此次更新中涉及到的两个点：coqui-ai/TTS如何使用以及GPT4All 2.5.0以后如何调用GPT模型。</p><h4 id="3-1-coqui-ai-TTS使用"><a href="#3-1-coqui-ai-TTS使用" class="headerlink" title="3.1 coqui-ai/TTS使用"></a>3.1 coqui-ai/TTS使用</h4><p>直接使用pip install TTS 即可安装 coqui-ai/TTS包，里面包含了很多功能，这里只简单展示如何调用一个现有的TTS模型。</p><p>首先列出所有的TTS模型：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> TTS.api <span class="keyword">import</span> TTS</span><br><span class="line"><span class="built_in">print</span>(TTS().list_models()) </span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;tts_models/multilingual/multi-dataset/xtts_v2&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/multilingual/multi-dataset/xtts_v1.1&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/multilingual/multi-dataset/your_tts&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/multilingual/multi-dataset/bark&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/bg/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/cs/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/da/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/et/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/ga/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ek1/tacotron2&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/tacotron2-DDC&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/tacotron2-DDC_ph&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/glow-tts&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/speedy-speech&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/tacotron2-DCA&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/vits--neon&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/fast_pitch&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/overflow&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/ljspeech/neural_hmm&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/vctk/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/vctk/fast_pitch&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/sam/tacotron-DDC&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/blizzard2013/capacitron-t2-c50&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/blizzard2013/capacitron-t2-c150_v2&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/multi-dataset/tortoise-v2&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/en/jenny/jenny&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/es/mai/tacotron2-DDC&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/es/css10/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/fr/mai/tacotron2-DDC&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/fr/css10/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/uk/mai/glow-tts&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/uk/mai/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/zh-CN/baker/tacotron2-DDC-GST&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/nl/mai/tacotron2-DDC&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/nl/css10/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/de/thorsten/tacotron2-DCA&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/de/thorsten/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/de/thorsten/tacotron2-DDC&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/de/css10/vits-neon&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/ja/kokoro/tacotron2-DDC&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/tr/common-voice/glow-tts&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/it/mai_female/glow-tts&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/it/mai_female/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/it/mai_male/glow-tts&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/it/mai_male/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/ewe/openbible/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/hau/openbible/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/lin/openbible/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/tw_akuapem/openbible/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/tw_asante/openbible/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/yor/openbible/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/hu/css10/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/el/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/fi/css10/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/hr/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/lt/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/lv/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/mt/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/pl/mai_female/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/pt/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/ro/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/sk/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/sl/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/sv/cv/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/ca/custom/vits&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/fa/custom/glow-tts&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/bn/custom/vits-male&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/bn/custom/vits-female&#x27;</span>,</span><br><span class="line"><span class="string">&#x27;tts_models/be/common-voice/glow-tts&#x27;</span></span><br></pre></td></tr></table></figure><p>我从英文(‘en’)的 TTS 模型中挑选了一个听起来比较好的 <code>tts_models/en/ljspeech/glow-tts</code>, 作为 talkGPT4All的默认 TTS，调用方式如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> TTS.api <span class="keyword">import</span> TTS</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化TTS模型</span></span><br><span class="line">tts = TTS(model_name=<span class="string">&quot;tts_models/en/ljspeech/glow-tts&quot;</span>, progress_bar=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者用离线下载的模型路径</span></span><br><span class="line">tts = TTS(model_path=<span class="string">&quot;/path/to/model&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合成文本对应的音频并保存到文件</span></span><br><span class="line">tts.tts_to_file(text=<span class="string">&quot;Hello there&quot;</span>, file_path=<span class="string">&quot;hello.wav&quot;</span>)</span><br></pre></td></tr></table></figure><p>如果因为网络原因模型在Python代码中下载不了，可以手动下载模型，然后指定TTS初始化中的model_path 为模型的本地路径。</p><h4 id="3-2-GPT4All-2-5-0以后模型的调用"><a href="#3-2-GPT4All-2-5-0以后模型的调用" class="headerlink" title="3.2 GPT4All 2.5.0以后模型的调用"></a>3.2 GPT4All 2.5.0以后模型的调用</h4><p>gguf 格式的模型目前有15个，各有特点：</p><p><img data-src="https://picx.zhimg.com/80/v2-be3555b71a240b52bbc48865090126cc_1440w.png?source=d16d100b"></p><p>所有模型的详细信息在<a href="https://github.com/nomic-ai/gpt4all/blob/a328f9ed3fdf238835429dd45940850724d0a652/gpt4all-chat/metadata/models2.json#L145">这里</a>，下面我列出所有支持的模型，方便命令行调用时参考：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">mistral-7b-openorca.Q4_0.gguf</span><br><span class="line">mistral-7b-instruct-v0.1.Q4_0.gguf</span><br><span class="line">gpt4all-falcon-q4_0.gguf</span><br><span class="line">orca-2-7b.Q4_0.gguf</span><br><span class="line">orca-2-13b.Q4_0.gguf</span><br><span class="line">wizardlm-13b-v1.2.Q4_0.gguf</span><br><span class="line">nous-hermes-llama2-13b.Q4_0.gguf</span><br><span class="line">gpt4all-13b-snoozy-q4_0.gguf</span><br><span class="line">mpt-7b-chat-merges-q4_0.gguf</span><br><span class="line">orca-mini-3b-gguf2-q4_0.gguf</span><br><span class="line">replit-code-v1_5-3b-q4_0.gguf</span><br><span class="line">starcoder-q4_0.gguf</span><br><span class="line">rift-coder-v0-7b-q4_0.gguf</span><br><span class="line">all-MiniLM-L6-v2-f16.gguf</span><br><span class="line">em_german_mistral_v01.Q4_0.gguf</span><br></pre></td></tr></table></figure><p>而 GPT4All chat 模式的调用方式也发生了变化，新版本需要这么调用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">gpt_model = GPT4All(<span class="string">&quot;mistral-7b-openorca.Q4_0.gguf&quot;</span>, allow_download=<span class="literal">True</span>)       </span><br><span class="line"><span class="keyword">with</span> gpt_model.chat_session():</span><br><span class="line">    answer = gpt_model.generate(prompt=<span class="string">&quot;hello&quot;</span>)</span><br></pre></td></tr></table></figure><p>需要显式地创建<code>chat_session</code> context manager。</p><h3 id="4-总结"><a href="#4-总结" class="headerlink" title="4. 总结"></a>4. 总结</h3><p>上面就是这次更新的主要内容，总的来说就是采用了更自然的TTS，更新代码以支持 GPT4All最新的break change。</p><p>欢迎大家试用、反馈bug。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://link.zhihu.com/?target=https://github.com/vra/talkGPT4All&quot;&gt;talkGPT4All&lt;/a&gt;是基于&lt;a href=&quot;https://link.zhihu.com/?target=https://gpt4all.io/index.html&quot;&gt;GPT4All&lt;/a&gt;的一个语音聊天程序，运行在本地CPU上，支持Linux，Mac和Windows。它利用OpenAI的Whisper模型将用户输入的语音转换为文本，再调用GPT4All的语言模型得到回答文本，最后利用文本转语音(TTS)的程序将回答文本朗读出来。&lt;/p&gt;
&lt;p&gt;今年4、5月份的时候，我发布了talkGPT4All 1.0版本和2.0版本，链接见下：&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/618826760&quot;&gt;talkGPT4All: 基于GPT4All的智能语音聊天程序&lt;/a&gt;&lt;br&gt;&lt;a href=&quot;https://zhuanlan.zhihu.com/p/632592897&quot;&gt;talkGPT4All 2.0:现在支持8个语言模型了&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;大家反馈最大的问题是TTS太机械了，听着很难受（具体可以看前面两篇文章的评论区）。而最近TTS领域的进展很多，例如很受欢迎的 coqui-ai的&lt;a href=&quot;https://github.com/coqui-ai/TTS&quot;&gt;TTS&lt;/a&gt; 库，提供了TTS、声音克隆和声音变换的功能。上周末尝试了一下，发现内置了一些开箱即用的TTS模型，刚好可以集成到 talkGPT4All 中，解决目前采用的 &lt;a href=&quot;https://pypi.org/project/pyttsx3/&quot;&gt;pyttsx3&lt;/a&gt;合成声音太机械的问题。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="pip" scheme="http://vra.github.io/tags/pip/"/>
    
    <category term="Whisper" scheme="http://vra.github.io/tags/Whisper/"/>
    
    <category term="GPT4All" scheme="http://vra.github.io/tags/GPT4All/"/>
    
  </entry>
  
  <entry>
    <title>AI小实验：大语言模型能否帮助我们理解古诗？</title>
    <link href="http://vra.github.io/2023/11/13/can-ai-explain-ancient-chinese-poetry/"/>
    <id>http://vra.github.io/2023/11/13/can-ai-explain-ancient-chinese-poetry/</id>
    <published>2023-11-13T04:57:30.000Z</published>
    <updated>2023-12-09T12:07:50.848Z</updated>
    
    <content type="html"><![CDATA[<p>昨天在读龚自珍《己亥杂诗》的时候，看到一句“千秋名教吾谁愧？愧读羲之誓墓文”，怎么想都想不明白这句什么意思。<br><img data-src="/imgs/ai_explain_poetry/000.jpg"><br>突发奇想，既然大语言模型进展突飞猛进，能否帮助我来解读这句诗是什么意思呢？</p><span id="more"></span><p>因此打开手机上的文心一言，讯飞星火、通义千问和智谱清言，向它们提问：“千秋名教吾谁愧？愧读羲之誓墓文。怎么理解”，各个App的回复如下：<br><img data-src="/imgs/ai_explain_poetry/004.png"><br><img data-src="/imgs/ai_explain_poetry/005.png"><br><img data-src="/imgs/ai_explain_poetry/007.jpg"><br><img data-src="/imgs/ai_explain_poetry/006.jpg"><br>可以看到四个App给出了完全不同的回答，其中有两个是明显的胡编乱造了，因为作者和出处都不对。</p><p>忽略掉作者和出处的错误，作者为什么要“愧”呢？四个App给出的答案也不一样，分别是：</p><ul><li>敬佩先贤，自愧不如</li><li>敬仰王羲之书法，但对自己的文学水平和自信</li><li>愧对王羲之临终时守护和传承传统文化的情操</li><li>感慨自己的水平无法与王羲之相提并论</li></ul><p>由于各个App给出了完全不同的回答，而我也不知道正确解释是什么，因此我又用传统的搜索引擎来搜索同样的问题，尝试了微信搜索、微信读书搜索、百度搜索和谷歌搜索。</p><p>与之前搜索古诗的经验不同，这句诗在搜索引擎上很少有解释。之前搜索古诗时，总会找到现代文的翻译，因此意思很容易就能搞懂。但或许由于这句诗实在太生僻了，网上找不到任何的完整的现代文解释。中途我甚至在怀疑：难道我需要去图书馆查找专业资料才能搞懂这句诗的意思吗？</p><p>最终在百度搜索上找到了“羲之誓墓”这个典故的含义：<br><img data-src="/imgs/ai_explain_poetry/003.jpg"><br>所以“羲之誓墓”含义是辞官归隐，隐约能明白作者意思，大概是”后悔误入尘网中，一去三十年”的感觉。<br>然后在谷歌搜索往下翻，找到了一片杭州日报纪念龚自珍的文章，里面提到了这句诗的含义：<br><img data-src="/imgs/ai_explain_poetry/001.jpg"></p><p>总体来说意思就是：王羲之曾在父母墓前发誓不再做官，而我为了在外做官十四年没有给母亲扫墓，真的是羞愧不已，枉读了羲之誓墓的文章。</p><p>所以结论是：至少在生僻的古诗上面，大模型还不能作为一个专家来帮我解读诗词的含义，在搜索引擎中进行信息检索和筛选还是有必要的。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;昨天在读龚自珍《己亥杂诗》的时候，看到一句“千秋名教吾谁愧？愧读羲之誓墓文”，怎么想都想不明白这句什么意思。&lt;br&gt;&lt;img data-src=&quot;/imgs/ai_explain_poetry/000.jpg&quot;&gt;&lt;br&gt;突发奇想，既然大语言模型进展突飞猛进，能否帮助我来解读这句诗是什么意思呢？&lt;/p&gt;</summary>
    
    
    
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="通义千问" scheme="http://vra.github.io/tags/%E9%80%9A%E4%B9%89%E5%8D%83%E9%97%AE/"/>
    
    <category term="文心一言" scheme="http://vra.github.io/tags/%E6%96%87%E5%BF%83%E4%B8%80%E8%A8%80/"/>
    
  </entry>
  
  <entry>
    <title>关于LLaVA-Plus 的一些思考</title>
    <link href="http://vra.github.io/2023/11/12/llava-plus/"/>
    <id>http://vra.github.io/2023/11/12/llava-plus/</id>
    <published>2023-11-12T04:57:30.000Z</published>
    <updated>2023-12-09T11:57:23.040Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://llava-vl.github.io/llava-plus">LLaVA-Plus</a> 是LLAVA团队最近放出来的LMM工作，对LLaVA进行了改进升级，相比LLaVA对输入图像只能进行文本回答的情况，LLaVA-PLUS则包含相当丰富的功能：</p><ul><li>可以调用SD生成与输入类似的图像</li><li>可以对图像进行编辑，例如调用Instruct pix2pix在图像上放置一只动物</li><li>可以对图像进行物体检测，分割，Cpation，OCR，打标签等多模态处理的功能</li><li>还可以调用外部知识来对未知的信息进行检索</li><li>支持用户交互，如对用户点击的区域进行实例分割</li><li>对图像进行美化，然后生成可以发布到社交媒体上的文案</li></ul><p>那么LMM是怎么获得到这么多的多模态能力的呢？论文中提出了一个叫<code>Skill Repository</code> 的概念，就是一些AI 子任务的能力和对应的模型，利用这个Skill Repository来完成丰富的功能。也就是说LLaVA-Plus将用户输入的任务通过进行拆分，然后调用合适的子任务模型来实现，再对结果进行一定的处理返回给用户。</p><span id="more"></span><p>具体的Skill Repository 包括下面这些：<br><img data-src="/imgs/llava_plus/20231112094107.png"></p><p>其实会发现，这种思路跟Visual ChatGPT是很类似的，不过与Visual ChatGPT不同的是，LLaVA-Plus没有调用外部的大语言模型ChatGPT，而是将LLM部分融合进了统一的网络结构中。这样的好处是图像特征在整个对话过程中都是有感知的，而Visual ChatGPT的方案则只在调用子模型的时候有图像感知，语言模型部分并不知道图像的特征，毕竟那会的ChatGPT还无法理解图像。</p><p>我觉得这种思路是LMM模型最可行的方案，即语言模型部分理解用户的要求，得到需要调用能力的列表，再调用对应的多模态模型，将多模态模型的输出进行总结，以自然语言的形式返回给用户。<br>这样的好处也是非常明显的：</p><ul><li>将子任务模型与LMM模型解耦，只要增加自己子任务的模型，就能支持用户的输入要求</li><li>每个子任务模型解决自己的特定的任务，结果肯定是最好的，而不是用一个什么都想做但都做的不是最好的模型</li><li>子任务可以利用现有的开源模型，降低整个系统学习的难度，避免了重复工作</li></ul><p>我觉得LLaVA-Plus对AI应用的进一步涌现很有促进作用。首先是这个方向有很多有意思的东西可以来做着玩了。<br>比如自动发朋友圈/微博/Ins/Twitter的Bot，可以将用户拍的照片进行美化，提高分辨率，然后自动生成I文案并发送出去。更发散一点，AI可以有自己的朋友圈了。</p><p>还有自动标注数据集的工具，所有类型的标注都自动来标注，甚至可以利用不同模型之间的一致性对标注质量进行提高。</p><p>另一方面，包含语言模型和子任务模型的LMMs也许真的会让CV和AI离普通人更近，因为自然语言的接口相比之前的计算机语言的接口要更易用。也许未来我们真的不需要单独的子任务模型了，通过LLaVA-Plus就可以用自然语言调用这些模型，甚至未来这些子任务模型我们可能都感知不到了，毕竟对用户来说，只是希望解决问题，而不关系底层用的是检测模型还是分割模型。</p><p><img data-src="/imgs/llava_plus/20231112093744.png"></p><p><img data-src="/imgs/llava_plus/20231112095143.png"><img data-src="/imgs/llava_plus/20231112095628.png"></p><p>另一个有意思的结果是，利用LLaVA-Plus可以对文生图的过程进行改进，就像WALLE-3利用ChatGPT来生成更好的Prompt一样，LLaVA-Plus也可以对用户输入的提示词进行优化，得到更适合SD的提示词：<br><img data-src="/imgs/llava_plus/20231112132234.png"></p><p>最后对论文的大致思路进行一个总结，也是比较粗糙，具体细节看论文吧。<br>作者提出了一种通用的多模态任务的问答形式：<br><img data-src="/imgs/llava_plus/20231112122754.png"></p><p>Iq是问题图像输入，Xq是问题文本输入，Xanswer是回答输出。</p><p>形式看似简单，但要看到这种统一形式的重要意义，利用统一的形式定义，可以将大量的不同子任务训练数据组织到一起，为LMM强大功能奠定基础。</p><p>为了得到准确的Xanswer，需要借助 Skill Repository里面的工具，得到Xskill_result，再得到Xanswer，<img data-src="/imgs/llava_plus/20231112094913.png"><br><img data-src="/imgs/llava_plus/20231112122933.png"><br>为了能够找到输入任务对应的模型并得到输出，作者设置了”thoughts”, “actions”和“value”三个阶段的， 分别进行输入到子模型的拆分、子模型调用API和参数，以及子模型的输出。</p><p>下面是一个具体调用的例子：<br><img data-src="/imgs/llava_plus/20231112123129.png"></p><p>在训练数据的构造方面也比较有意思。<br>为了利用LLaVA没有thoughts-actions-value过程的数据，作者添加了“空白”的thoughts-actions-value占位符：<br><img data-src="/imgs/llava_plus/20231112131254.png"></p><p>为了增加问题的多样性，让GPT4来改写问题：<img data-src="/imgs/llava_plus/20231112131711.png"></p><p>根据caption 数据，让ChatGPT/GPT4来提问题，构造训练数据，这里的提示词工程挺有意思，有些trick在里面，可以细看一下：<br><img data-src="/imgs/llava_plus/20231112131934.png"></p><p>论文附录中有很多例子，可以参考。</p><p>Online功能体验地址： <a href="https://llavaplus.ngrok.io/">LLaVA-Plus (llavaplus.ngrok.io)</a></p>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://llava-vl.github.io/llava-plus&quot;&gt;LLaVA-Plus&lt;/a&gt; 是LLAVA团队最近放出来的LMM工作，对LLaVA进行了改进升级，相比LLaVA对输入图像只能进行文本回答的情况，LLaVA-PLUS则包含相当丰富的功能：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;可以调用SD生成与输入类似的图像&lt;/li&gt;
&lt;li&gt;可以对图像进行编辑，例如调用Instruct pix2pix在图像上放置一只动物&lt;/li&gt;
&lt;li&gt;可以对图像进行物体检测，分割，Cpation，OCR，打标签等多模态处理的功能&lt;/li&gt;
&lt;li&gt;还可以调用外部知识来对未知的信息进行检索&lt;/li&gt;
&lt;li&gt;支持用户交互，如对用户点击的区域进行实例分割&lt;/li&gt;
&lt;li&gt;对图像进行美化，然后生成可以发布到社交媒体上的文案&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那么LMM是怎么获得到这么多的多模态能力的呢？论文中提出了一个叫&lt;code&gt;Skill Repository&lt;/code&gt; 的概念，就是一些AI 子任务的能力和对应的模型，利用这个Skill Repository来完成丰富的功能。也就是说LLaVA-Plus将用户输入的任务通过进行拆分，然后调用合适的子任务模型来实现，再对结果进行一定的处理返回给用户。&lt;/p&gt;</summary>
    
    
    
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="Deep Learning" scheme="http://vra.github.io/tags/Deep-Learning/"/>
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="LMM" scheme="http://vra.github.io/tags/LMM/"/>
    
    <category term="CV" scheme="http://vra.github.io/tags/CV/"/>
    
  </entry>
  
  <entry>
    <title>Bing Brush-Python代码和命令行中调用必应 DALL·E 3文生图模型</title>
    <link href="http://vra.github.io/2023/11/04/bing-brush/"/>
    <id>http://vra.github.io/2023/11/04/bing-brush/</id>
    <published>2023-11-04T15:22:03.000Z</published>
    <updated>2023-12-09T03:13:56.389Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-说明"><a href="#1-说明" class="headerlink" title="1. 说明"></a>1. 说明</h3><p>今早看到一个好玩的项目，利用Bing Image Creator 来生成每日诗词的图像，研究了一下，发现有人提供了<a href="https://github.com/acheong08/BingImageCreator">BingImageCreator</a>仓库来调用Bing的API在代码中生成图像，但还需要下载源码，没有提供cli，cookie怎么获取也没有讲太细。</p><p>因此我基于这个仓库，做了一些精简和封装，提供了一个可以直接pip安装的工具<a href="https://github.com/vra/bing_brush">bing_brush</a>, 获取cookie后可以直接命令行调用。</p><span id="more"></span><p><img data-src="https://pic1.zhimg.com/80/v2-4d60e7c55a9388e56903c58fd3b1432f_1440w.png?source=d16d100b"></p><p>整体流程很简单：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">pip install bing_brush</span><br><span class="line"><span class="comment"># 获取bing.com的cookie，见下文</span></span><br><span class="line">bing_brush -c cookie.txt -p <span class="string">&#x27;a cute panda eating bamboos&#x27;</span> -o output_folder</span><br></pre></td></tr></table></figure><p>就会output_folder 下生成4张图像：</p><p><img data-src="https://pica.zhimg.com/80/v2-9f13f504f3431d1018acd2bf8d3a7241_1440w.png?source=d16d100b"></p><p>源码：<a href="https://github.com/vra/bing_brush">vra/bing_brush (github.com)</a><br>欢迎Watch, Star, Fork 和Contribute！</p><h3 id="2-cookie获取"><a href="#2-cookie获取" class="headerlink" title="2. cookie获取"></a>2. cookie获取</h3><p>整个过程中稍微有些繁琐的是获取cookie，详细操作见下。</p><p>首先打开 <a href="https://www.bing.com/images/create">https://www.bing.com/images/create</a></p><p>如果访问不了的话，那这个工具也没法使用，因此确保这个页面可以正常打开。</p><p><img data-src="https://picx.zhimg.com/80/v2-aa9f3e5f8e645d02f9ad174fa11a0f50_1440w.jpeg?source=d16d100b"></p><p>然后按F12，打开开发者页面，然后刷新页面，会看到很多请求，选择任一类型为xhr的请求，点击前面的lianjie：</p><p><img data-src="https://picx.zhimg.com/80/v2-173983dcdd28069d41dce7af3f2d61eb_1440w.jpeg?source=d16d100b"></p><p>进入详情页面后，往下翻找到Cookie 部分，将对应的右边的复制到cookie.txt即可，后面-c 指定这个路径就行。</p><p><img data-src="https://picx.zhimg.com/80/v2-cd8f0e48096c0b990a931764610bf5ad_1440w.jpeg?source=d16d100b"></p><h3 id="3-使用流程"><a href="#3-使用流程" class="headerlink" title="3. 使用流程"></a>3. 使用流程</h3><p>pip安装bing_brush，并且获取cookie后，就可以用一条命令来运行图像生成：</p><p>bing_brush -c cookie.txt -p ‘a cute panda eating bamboos’ -o output_folder</p><p>然后就可以发挥你的创意来在命令行跑图了。</p><h3 id="4-Python代码中使用"><a href="#4-Python代码中使用" class="headerlink" title="4. Python代码中使用"></a>4. Python代码中使用</h3><p>pip 安装后，也可以在Python代码中使用 Bing Brush:</p><p>from bing_brush import BingBrush</p><p>brush = BingBrush(cookie=’cookie.txt’)<br>brush.process(prompt=’a cute panda eating bamboos’, out_folder=’output_folder’)</p><h3 id="5-彩蛋"><a href="#5-彩蛋" class="headerlink" title="5. 彩蛋"></a>5. 彩蛋</h3><p>这个项目的Logo也是用Bing生成的，prompt如下：</p><blockquote><p>A minimalist logo vector image, square-shaped, with a magical brush implemented in Python language in the center, colorful, digital art</p></blockquote><p>画出了三张logo，最后选择第三张作为项目的Logo</p><p><img data-src="https://picx.zhimg.com/80/v2-28772285cca47864cbd9a6bf396a6bb1_1440w.png?source=d16d100b"></p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-说明&quot;&gt;&lt;a href=&quot;#1-说明&quot; class=&quot;headerlink&quot; title=&quot;1. 说明&quot;&gt;&lt;/a&gt;1. 说明&lt;/h3&gt;&lt;p&gt;今早看到一个好玩的项目，利用Bing Image Creator 来生成每日诗词的图像，研究了一下，发现有人提供了&lt;a href=&quot;https://github.com/acheong08/BingImageCreator&quot;&gt;BingImageCreator&lt;/a&gt;仓库来调用Bing的API在代码中生成图像，但还需要下载源码，没有提供cli，cookie怎么获取也没有讲太细。&lt;/p&gt;
&lt;p&gt;因此我基于这个仓库，做了一些精简和封装，提供了一个可以直接pip安装的工具&lt;a href=&quot;https://github.com/vra/bing_brush&quot;&gt;bing_brush&lt;/a&gt;, 获取cookie后可以直接命令行调用。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="pip" scheme="http://vra.github.io/tags/pip/"/>
    
    <category term="DALL·E" scheme="http://vra.github.io/tags/DALL%C2%B7E/"/>
    
    <category term="Bing" scheme="http://vra.github.io/tags/Bing/"/>
    
  </entry>
  
  <entry>
    <title>使用 Python 88 行代码写一个简易的 Android AI 程序</title>
    <link href="http://vra.github.io/2023/10/14/android-ai-app-in-88-lines-of-python-code/"/>
    <id>http://vra.github.io/2023/10/14/android-ai-app-in-88-lines-of-python-code/</id>
    <published>2023-10-14T15:22:03.000Z</published>
    <updated>2023-12-09T02:52:12.255Z</updated>
    
    <content type="html"><![CDATA[<p>TL;DR:<br>我基于 LeptonAI 和 Beeware Python 库，利用 88 行的Python，不用写一行Java代码，在手机上做了一个 SDXL text-to-image 的Demo，效果见<a href="https://zhuanlan.zhihu.com/p/661358058">这里</a>的视频。</p><p>作为一个爱折腾写Python比较多的人，我一直在想一个事情：能否将熟悉的Python技术栈的能力带到移动平台中，不用写哪些繁琐的Native开发代码，就能在移动端跑起来一个AI Demo呢？因为相比PC，移动端设备的用户数多得多，每个人都有一台手机，但并不是每个人都有一台电脑。</p><p>一次偶然的机会，我发现了 <a href="https://beeware.org/">Beeware</a>，一个目标 “Write once. Deploy everywhere.“ 的跨平台 Python 工具箱。基于 Beeware 工具箱写的 Python 程序可以在 PC，Web，Android 和 iOS 上运行，因此正是我想要的。</p><p>一切听起来很美好，但实际使用时也遇到很多问题。</p><span id="more"></span><p>首先是 Beeware 在移动端支持的 Python 包有限，比如像对 Pytorch 的支持就有问题 (可以import但运行时报错)，所以手机本地没法直接运行 Pytorch AI模型，至少我没有跑通。</p><p>另一个是 Beeware 工具链中的 GUI 库 toga 太简单了，一些复杂的功能实现不了，比如网络推理时加一个显示在窗口最顶层的转圈的特效。所以只能做一些比较toy的小的项目，没法做真正可以用的产品。</p><p>所以不想写繁琐的 Natvie代码的话，另一个选择可能就是写 基于小程序的 Web 代码了，至少小程序的UI功能还是很齐全的。</p><p>Anyway，虽然有这些约束，但还是可以用 Beeware 做一些简单的 Python Demo，比如这里我就结合 <a href="https://www.lepton.ai/">LeptonAI</a>和 Beeware，一行 Android 开发的都不用写，总共利用 88 行的 Python 代码，做出来了一个简单的 SDXL text-to-image Android 端 Demo。</p><p>首先说说一下服务端。SDXL 部署在 LeptonAI 的云平台上，提供公网可访问的 AI 服务。关于 LeptonAI 的使用和 SDXL 的部署，可以参考我这篇<a href="https://zhuanlan.zhihu.com/p/661243511">文章</a>。简单来说安装 LeptonAI Python SDK 后，使用下面的三条命令创建模型镜像，然后在 LeptonAI 的云平台进行部署:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建镜像</span></span><br><span class="line">lep photon create --name sdxl --model hf:hotshotco/SDXL-512</span><br><span class="line"></span><br><span class="line"><span class="comment"># 登录云平台</span></span><br><span class="line">lep login -c xxx:xxxxxxxxxxxxxxxx</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推送镜像到云平台</span></span><br><span class="line">lep photon push --name sdxl</span><br></pre></td></tr></table></figure><p>客户端就是这个App， 整体功能很简陋，用户在输入框填入提示词，点击生成图片的按钮后，代码读取用户输入，构造网络请求，然后将 text-to-image 生成的图像返回给客户端，客户端进行解析后再展示。</p><p>开发流程是先在 Mac 上调试代码，成功后再进行一些微调，就能跑到手机上。</p><p>具体来说，整个过程中用到的 Beeware 命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 交互式地构建项目目录</span></span><br><span class="line">briefcase new</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在Mac上调试代码</span></span><br><span class="line">briefcase dev</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建 Android 开发环境，会自动在命令行下载NDK等</span></span><br><span class="line">briefcase create android </span><br><span class="line"></span><br><span class="line"><span class="comment"># 编译代码，生成 APK文件</span></span><br><span class="line">briefcase build android</span><br></pre></td></tr></table></figure><p><code>briefcase</code> 是 Beeware 工具箱中用来将 Python 代码转换为 Native 应用的工具。</p><p>在 Mac 上运行正常，往手机上微调过程中，也有一些细节要注意。</p><p>首先是需要将依赖包写入到<code>pyproject.toml</code>中的<code>requires</code> 字段中，Mac上可能因为已经提前安装了一些第三方包而在使用时没有报错，但在移动端使用时需要将所有用到的包都加入到apk中。</p><p>由于 Beeware 貌似不支持 requests 包，所以需要将 比较简洁的 requests 请求方式修改为基于系统库的<code>urllib.request</code> 请求方式。</p><p>由于Android环境没有环境变量，因此需要将原先代码中读取环境变量中的TOKEN的代码去掉，这里采用了不太科学的方法，直接将TOKEN写死在代码中。</p><p>Python 代码更新有时候不会生效，需要手动删除 Build 目录再执行 <code>briefcase build android</code>的命令。</p><p>最后也将 88 行代码列出来，完整代码仓库在<a href="https://github.com/vra/sdxl-python-app">这里</a>，感兴趣的小伙伴可以自己玩玩。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">An Application based on Python and LeptonAI!</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image <span class="keyword">as</span> PIL_Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> toga</span><br><span class="line"><span class="keyword">from</span> toga.style <span class="keyword">import</span> Pack</span><br><span class="line"><span class="keyword">from</span> toga.style.pack <span class="keyword">import</span> COLUMN, ROW</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AISDK</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="comment"># Android 端没法用环境变量，这里只能将 TOKEN 写死在代码中</span></span><br><span class="line">        api_token = <span class="string">&quot;xxxxxxxxxxxx&quot;</span></span><br><span class="line">        self.url = <span class="string">&quot;https://xxx-sdxl-deploy.bjz.edr.lepton.ai/run&quot;</span></span><br><span class="line">        self.headers = &#123;</span><br><span class="line">            <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/json&quot;</span>,</span><br><span class="line">            <span class="string">&quot;accept&quot;</span>: <span class="string">&quot;image/png&quot;</span>,</span><br><span class="line">            <span class="string">&quot;Authorization&quot;</span>: <span class="string">f&quot;Bearer <span class="subst">&#123;api_token&#125;</span>&quot;</span>,</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span>(<span class="params">self, prompt, img_save_path</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;ai processing begin...&quot;</span>)</span><br><span class="line">        data = &#123;<span class="string">&quot;num_inference_steps&quot;</span>: <span class="number">25</span>, <span class="string">&quot;prompt&quot;</span>: prompt, <span class="string">&quot;seed&quot;</span>: <span class="number">42</span>&#125;</span><br><span class="line">        req = urllib.request.Request(self.url, headers=self.headers, data=json.dumps(data).encode(<span class="string">&#x27;utf-8&#x27;</span>))</span><br><span class="line">        response = urllib.request.urlopen(req)</span><br><span class="line">        res = response.read()</span><br><span class="line"></span><br><span class="line">        image_data = io.BytesIO(res)</span><br><span class="line">        image = PIL_Image.<span class="built_in">open</span>(image_data)</span><br><span class="line">        image.save(img_save_path)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;ai processing done&quot;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SDXLApp</span>(<span class="params">toga.App</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">startup</span>(<span class="params">self</span>):</span></span><br><span class="line">        self.sdk = AISDK()</span><br><span class="line">        self.img_save_path = os.path.join(os.path.dirname(__file__), <span class="string">&quot;aigc_img.jpg&quot;</span>)</span><br><span class="line"></span><br><span class="line">        main_box = toga.Box(style=Pack(direction=COLUMN))</span><br><span class="line"></span><br><span class="line">        name_label = toga.Label(<span class="string">&quot;Your prompt: &quot;</span>, style=Pack(padding=(<span class="number">0</span>, <span class="number">5</span>)))</span><br><span class="line">        self.name_input = toga.TextInput(style=Pack(flex=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        name_box = toga.Box(style=Pack(direction=ROW, padding=<span class="number">5</span>))</span><br><span class="line">        name_box.add(name_label)</span><br><span class="line">        name_box.add(self.name_input)</span><br><span class="line"></span><br><span class="line">        button = toga.Button(</span><br><span class="line">            <span class="string">&quot;Generate Image&quot;</span>, on_press=self.run_aigc, style=Pack(padding=<span class="number">5</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        main_box.add(name_box)</span><br><span class="line">        main_box.add(button)</span><br><span class="line"></span><br><span class="line">        <span class="built_in">print</span>(self.img_save_path)</span><br><span class="line">        self.image = toga.Image(self.img_save_path)</span><br><span class="line">        self.image_view = toga.ImageView(self.image)</span><br><span class="line"></span><br><span class="line">        self.main_window = toga.MainWindow(title=self.formal_name)</span><br><span class="line">        self.main_window.content = main_box</span><br><span class="line">        self.main_window.content.add(self.image_view)</span><br><span class="line">        self.main_window.show()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_aigc</span>(<span class="params">self, widget</span>):</span></span><br><span class="line">        <span class="comment"># 清除已有结果</span></span><br><span class="line">        self.main_window.content.remove(self.image_view)</span><br><span class="line">        self.image_view = toga.ImageView(image=<span class="literal">None</span>)</span><br><span class="line"></span><br><span class="line">        prompt = self.name_input.value</span><br><span class="line">        self.sdk.process(prompt, self.img_save_path)</span><br><span class="line"></span><br><span class="line">        image = toga.Image(self.img_save_path)</span><br><span class="line">        self.image_view = toga.ImageView(image)</span><br><span class="line">        self.main_window.content.add(self.image_view)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="keyword">return</span> SDXLApp()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    SDXLApp()</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;TL;DR:&lt;br&gt;我基于 LeptonAI 和 Beeware Python 库，利用 88 行的Python，不用写一行Java代码，在手机上做了一个 SDXL text-to-image 的Demo，效果见&lt;a href=&quot;https://zhuanlan.zhihu.com/p/661358058&quot;&gt;这里&lt;/a&gt;的视频。&lt;/p&gt;
&lt;p&gt;作为一个爱折腾写Python比较多的人，我一直在想一个事情：能否将熟悉的Python技术栈的能力带到移动平台中，不用写哪些繁琐的Native开发代码，就能在移动端跑起来一个AI Demo呢？因为相比PC，移动端设备的用户数多得多，每个人都有一台手机，但并不是每个人都有一台电脑。&lt;/p&gt;
&lt;p&gt;一次偶然的机会，我发现了 &lt;a href=&quot;https://beeware.org/&quot;&gt;Beeware&lt;/a&gt;，一个目标 “Write once. Deploy everywhere.“ 的跨平台 Python 工具箱。基于 Beeware 工具箱写的 Python 程序可以在 PC，Web，Android 和 iOS 上运行，因此正是我想要的。&lt;/p&gt;
&lt;p&gt;一切听起来很美好，但实际使用时也遇到很多问题。&lt;/p&gt;</summary>
    
    
    
    
    <category term="LeptonAI" scheme="http://vra.github.io/tags/LeptonAI/"/>
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="Android" scheme="http://vra.github.io/tags/Android/"/>
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
  </entry>
  
  <entry>
    <title>LeptonAI 使用体验</title>
    <link href="http://vra.github.io/2023/10/14/leptonai/"/>
    <id>http://vra.github.io/2023/10/14/leptonai/</id>
    <published>2023-10-13T16:05:35.000Z</published>
    <updated>2023-12-09T02:40:12.330Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h2><p><a href="https://www.lepton.ai/">LeptonAI</a>是贾扬清的创业项目，正如 slogan “Build AI The Simple Way” 所表明的，LeptonAI的目标是简化AI模型的部署。</p><p>简单来说，LeptonAI 提供了 <a href="(https://github.com/leptonai/leptonai">Python SDK</a> 和云平台。Python SDK 可以让没有AI领域知识的普通开发者调用2～3行命令就能部署一个AI模型，然后用一个curl语句或几行Python代码就能完成客户端请求；而 LeptonAI 云平台提供了CPU，GPU和存储资源，开发者可以把创建的AI模型部署到这里，就能提供公开对外的AI服务。</p><p>AI模型创建支持 HuggingFace，也就是说可以将 HuggingFace 上海量的模型集成到自己的应用中。同时 LeptonAI 也支持从 GitHub 仓库创建 AI 模型，给了开发者更多的选择。</p><p>经过一段时间的体验后发现，LeptonAI Python SDK 设计的很优雅，用起来很舒服，而云平台的操作也很丝滑，有贾扬清大神亲自操刀写代码，SDK和云平台的质量绝对是信得过的。<br><img data-src="/imgs/leptonai/20231014072616.png"></p><p>那 LeptonAI 是否解决了一些 AI 部署中的痛点问题呢？我认为是的。根据之前的经验，跑一些 AI 开源模型成本还是挺高的。</p><span id="more"></span><p>简单来说，跑一个开源模型，需要下面这些流程：</p><ol><li>需要安装Conda虚拟环境</li><li>拉取开源代码源码</li><li>安装依赖 Python 包</li><li>下载 Checkpoints</li><li>跑 Demo 命令或 UI<br>就算整个过程中没有遇到问题，整个流程走下来，跑起来一个模型至少也得半个小时。</li></ol><p>而实际情况可能更糟糕，大概率会遇到下面的问题之一或者之N：</p><ol><li>开源代码没有指定Python版本，用新/老版本的Python运行报错</li><li>开源代码没有指定CUDA版本，用新/老版本的CUDA，运行报错</li><li>当前操作系统对应的CPU和GPU组合未经测试，运行报错</li><li>编译Pytorch所用的cuDNN版本和系统现在的cuDNN版本不一致，Pytorch插件编译报错</li><li>依赖 Python 包版本冲突，需要某个特定版本才能正常工作</li><li>之前代码运行正常，更新某个包版本后运行报错</li><li> 默认 Demo Python 脚本运行报错，需要查看源码定位修复问题</li><li>某些OP不支持half精度，运行报错</li><li>某些OP不支持mps后端，运行报错</li><li>网络状况不佳导致 GitHub 拉取代码失败，HuggingFace 下载模型失败</li><li>…</li></ol><p>以 stable-diffusion-webui 这个著名的AIGC库为例，光带<code>bug-report</code>标记的issue就有3000多个：<br><img data-src="/imgs/leptonai/20231014080015.png"></p><p>随便看几个例子，都能发现上述这些问题的身影：<br><img data-src="/imgs/leptonai/20231014080629.png"><br><img data-src="/imgs/leptonai/20231014075232.png"><br>也就是说，大家在 AI 部署中遇到了非常多的问题，而不是没有问题。</p><p>因此简化 AI 的部署，是很有意义的方向，做的好的话，能减轻很多人的痛苦。根据我的经历，在使用 LeptonAI 部署模型时，我还没遇到过上面提到的这些问题（当然现在使用的次数也还比较少）。</p><p>LeptonAI 的盈利模式也很清晰，虽然 Python SDK 和 HuggingFace 上的模型都是开源的，但由于 LeptonAI 提供了云服务来部署算法，因此可以通过云服务的硬件资源（GPU，CPU和存储）来收费，在 Lepton Python SDK 将部署问题大大简化后，降低了AI应用的门槛，可能会出现更多的开发者来提供更好的创意，做出更多更棒的App，产生更加实际的业务成果。如果AI算法真的能提供实际的业务价值，那应用开发者肯定愿意将收入中的一部分给到 LeptonAI 平台</p><p>但这里也有一个问题：LeptonAI Python SDK 似乎是可以部署到任意的云计算平台的，也就是说如果阿里云、亚马逊云提供的 GPU 价格更低，那开发者可能就会迁移自己到这些平台，也就是用 LeptonAI Python SDK，但不用你的 GPU 资源。在这个角度下，LeptonAI到底能不能赢得用户，产生正向收益，还不好说。</p><p>具体到云平台的收费模式，采用 SaaS 中比较常见的基础版免费、Pro版收费的方式。基础版是每个月会给用户会赠送10美元的券，可以方便个人开发者和小的初创团队对进行前期的简单实验验证。标准版每个月100美元，还有企业可以定制：<br><img data-src="/imgs/leptonai/20231013220038.png"></p><p>从现在的信息看，LeptonAI 现阶段的目标市场应该是欧美，目标客户应该是个人开发者或者中小公司，因为定价是美元，支付采用Stripe，这些都是国内用户不太方便使用的。</p><p>其实国内使用还有另外一个影响很大的因素：HuggingFace 目前在国内网络访问不了（实在想不通有什么理由封禁HuggingFace)，上面的AI模型都是用不了的，想要部署 HuggingFace 上的模型要费一些周折。</p><p>另外 LeptonAI 最近开始公开内测了，想要试玩的朋友可以在官网注册账号申请。</p><p>在公测的<a href="https://leptonai.medium.com/build-ai-the-easy-way-2a8b68c63723">Blog</a>中，介绍了几位创始人之前的项目经历，最后也自豪地宣称他们是”make building awesome AI applications as simple as possible”最好的人选。<br><img data-src="/imgs/leptonai/20231014062556.png"></p><p>正如 Caffe 开启了深度学习框架的新时代，希望 LeptonAI 也能开启 AI 部署的新时代，未来如何发展，让我们拭目以待。</p><p>后面部分会从网站提供的功能概览、部署HuggingFace上模型、部署GitHub上自己开发的模型，以及基于Lepton云服务做一个简单的Android Demo，看看开发一个应用有多简单。</p><h2 id="2-Playground概览"><a href="#2-Playground概览" class="headerlink" title="2. Playground概览"></a>2. Playground概览</h2><p><a href="https://www.lepton.ai/playground">https://www.lepton.ai/playground</a> 提供了很多LeptonAI自部署的开源模型，如SDXL，Llama 2, Code Llama等,跟HuggingFace平台功能类似。基于LeptonAI SDK，开发者可以在自己的GPU服务器上快速地搭建这些模型服务。</p><p><img data-src="/imgs/leptonai/20231013231215.png"><br>以SDXL Inpainting为例，上传一张图像，mask掉一些区域，再结合一句Prompt，就能补全出对应的区域：</p><h2 id="3-LeptonAI-设计思路简介"><a href="#3-LeptonAI-设计思路简介" class="headerlink" title="3. LeptonAI  设计思路简介"></a>3. LeptonAI  设计思路简介</h2><p>Lepton的含义是轻子，是一个物理学概念，根据维基百科：</p><blockquote><p>轻子是一种不参与强相互作用、自旋为1/2的基本粒子。电子是最为人知的一种轻子；轻子又分为两类：“带电轻子”与“中性轻子”。带电轻子包括电子、μ子、τ子，可以与其它粒子组合成复合粒子，例如原子、电子偶素等等</p></blockquote><p>公司命名为 LeptonAI Inc，科技属性拉满。<br>LeptonAI Python SDK 的基本模块是 Photon (光子)，基本上可以理解为算法模型，如 SDXL，Llama2 都可以看作是 Photon。<br>Lepton 一个独到的地方是，将所有不同来源的算法模型统一到Photon对象中，比如HuggingFace上的一个模型可以用来初始化Photon，也可以从GitHub上的源代码来初始化Photon。</p><p>基于这种简化，所有算法模型都能通过统一的对外接口来展示，方便了模型的部署。当然为了简单统一的接口，源码内部就需要对不同来源的模型进行适配，是以内部代码的复杂度增加来换取对外接口的简洁一致。</p><p>更准确、详细的内容，请参考<a href="https://www.lepton.ai/docs">官方文档</a>。</p><h2 id="4-部署自己的服务"><a href="#4-部署自己的服务" class="headerlink" title="4. 部署自己的服务"></a>4. 部署自己的服务</h2><p>使用LeptonAI的Python leptonai 包可以进行方便的Photon创建和部署。</p><p>首先用pip 安装 leptonai 包：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install -U leptonai</span><br></pre></td></tr></table></figure><p>输入 <code>lep --help</code> 查看是否安装成功。<br>LeptonAI 的 CLI 命令是 <code>lep</code>，包含所有你需要的功能，如 Photon 创建、部署，登录云平台等。</p><p>为了在 LeptonAI 云平台部署模型，确保你已经申请拿到了公测权限。</p><h3 id="3-1-基于HuggingFace"><a href="#3-1-基于HuggingFace" class="headerlink" title="3.1 基于HuggingFace"></a>3.1 基于HuggingFace</h3><p>官方文档展示了在 LeptonAI 云平台部署 GPT-2 的操作流程，但由于GPT-2效果一般，实际上用处不大，这里以目前效果比较好的SDXL为例来展示如何部署HuggingFace到自己的 LeptonAI 实例上。</p><p>首先是用 <code>lep photon create</code> 创建 Photon:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lep photon create --name sdxl --model hf:hotshotco/SDXL-512</span><br></pre></td></tr></table></figure><p><code>--name</code> 是自定义的 Photon 名字，<code>--model</code> 是Photo 下载模型的地址，可以是一个HuggingFace模型名字，也可以是一个GitHub地址。这里用 <code>hf:</code> 开头表示 HuggingFace 上的模型，<code>&lt;user_name&gt;/&lt;model_name&gt;</code>表示模型的路径。</p><p>这里会从 HuggingFace 上拉取模型信息，所以需要能访问 HuggingFace。国内访问hf有问题的小伙伴可以用<a href="https://gitpod.io/">GitPod</a>这个神器。</p><p>然后为了部署模型到云平台，需要登录 LeptonAI 的 dashboard，查看自己的key，复制 <code>lep login</code> 命令：<br><img data-src="/imgs/leptonai/20231014003956.png"></p><figure class="highlight llvm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lep login -<span class="keyword">c</span> xxx:xxxxxxxxxxxxxxxx</span><br></pre></td></tr></table></figure><p>登录成功后，就可以用 <code>lep photon push</code> 命令将本地创建好的 Photon 推送到 LeptonAI 云平台：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lep photon push --name sdxl</span><br></pre></td></tr></table></figure><p>然后在 Dashboard 网页上进行操作，对 Photon 进行部署：</p><p>部署完成后，就相当于有了一个公网可以访问的AI服务，通过下面的Python命令就可以访问这个服务：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> leptonai.client <span class="keyword">import</span> Client</span><br><span class="line"></span><br><span class="line"> </span><br><span class="line">LEPTON_API_TOKEN = <span class="string">&quot;xxxxxxxxxx&quot;</span></span><br><span class="line">client = Client(<span class="string">&quot;xx&quot;</span>, <span class="string">&quot;sdxl&quot;</span>, token=LEPTON_API_TOKEN)</span><br><span class="line"></span><br><span class="line">data = client.run(</span><br><span class="line">    num_inference_steps=<span class="number">25</span>,</span><br><span class="line">    prompt=<span class="string">&quot;a photograph of an astronaut riding a horse&quot;</span>,</span><br><span class="line">    seed=<span class="number">42</span>,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将服务返回的 bytes 数据保存为图片</span></span><br><span class="line">image_data = io.BytesIO(data)</span><br><span class="line">image = Image.<span class="built_in">open</span>(image_data)</span><br><span class="line">image.save(<span class="string">&quot;output_image.jpg&quot;</span>)</span><br></pre></td></tr></table></figure><p>同理也可以用 Python 的 requests 包来发送请求，这样的好处是客户端就不需要安装 LeptonAI 的 Python包了，毕竟它的依赖还是比较多的，下一个例子中会展示requests的用法。</p><h3 id="3-2-基于-GitHub-部署自己的算法"><a href="#3-2-基于-GitHub-部署自己的算法" class="headerlink" title="3.2 基于 GitHub 部署自己的算法"></a>3.2 基于 GitHub 部署自己的算法</h3><p>LeptonAI 也支持从 GitHub 创建 Photons，只要你的代码类继承了 lepton的Photo基类，就都可以正常部署，对于许多想要新建自己算法的人来说是个好消息。下面用一个简单的提取图像边缘的例子来展示怎么自定义算法并且部署到 LeptonAI 云平台。</p><p>我的代码仓库在<a href="https://github.com/vra/canny-lepton-photon%EF%BC%8C%E5%AE%9E%E7%8E%B0%E5%9C%A8">https://github.com/vra/canny-lepton-photon，实现在</a> <code>canny.py</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> io <span class="keyword">import</span> BytesIO</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">from</span> urllib.request <span class="keyword">import</span> urlopen</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> leptonai.photon <span class="keyword">import</span> Photon, PNGResponse</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要继承 Photon 类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Canny</span>(<span class="params">Photon</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;Canny 边缘检测算子&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里的依赖 Package 会在创建 Photon 时自动安装</span></span><br><span class="line">    requirement_dependency = [</span><br><span class="line">        <span class="string">&quot;opencv-python&quot;</span>,</span><br><span class="line">        <span class="string">&quot;numpy&quot;</span>,</span><br><span class="line">        <span class="string">&quot;Pillow&quot;</span>,</span><br><span class="line">    ]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 用这个装饰器表示这个一个对外接口</span></span><br><span class="line"><span class="meta">    @Photon.handler(<span class="params"><span class="string">&quot;run&quot;</span></span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">self, url: <span class="built_in">str</span></span>) -&gt; PNGResponse:</span></span><br><span class="line">        <span class="comment"># 将第三方库的 import 放到实际执行代码中，否则如果本地没有这些包，</span></span><br><span class="line">        <span class="comment"># 在创建 Photon 时报错</span></span><br><span class="line">        <span class="keyword">import</span> cv2</span><br><span class="line">        <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">        <span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 读取图像数据</span></span><br><span class="line">        image = np.asarray(Image.<span class="built_in">open</span>(io.BytesIO(urlopen(url).read())))</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 进行边缘检测</span></span><br><span class="line">        edges = cv2.Canny(image, <span class="number">100</span>, <span class="number">200</span>)</span><br><span class="line"></span><br><span class="line">        edges = Image.fromarray(edges)</span><br><span class="line"></span><br><span class="line">        img_io = BytesIO()</span><br><span class="line">        edges.save(img_io, <span class="built_in">format</span>=<span class="string">&quot;PNG&quot;</span>, quality=<span class="string">&quot;keep&quot;</span>)</span><br><span class="line">        img_io.seek(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> PNGResponse(img_io)</span><br></pre></td></tr></table></figure><p>由于GitHub不再支持命令行密码登录了，所以需要在<a href="https://github.com/settings/tokens?type=beta">这里</a>创建token，然后用token替代密码来登录。<br>复制刚才创建的token，在环境变量中进行设置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> GITHUB_USER=&#123;YOUR_GITHUB_USERNAME&#125;</span><br><span class="line"><span class="built_in">export</span> GITHUB_TOKEN=&#123;THE_TOKEN_GENERATED_FROM_STEP_1&#125;</span><br></pre></td></tr></table></figure><p>然后创建 Photon，模型名字格式是 <code>py:&#123;GIT_REPO_URL&#125;:&#123;PATH_TO_SCRIPT&#125;:&#123;CLASS_NAME&#125;</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lep photon create -n canny -m py:https://github.com/vra/canny-lepton-photon:canny.py:Canny</span><br></pre></td></tr></table></figure><p>再推送到云平台：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">lep photon push -n canny</span><br></pre></td></tr></table></figure><p>注意：可以多次修改代码，多次创建同名的 Photon，会产生不同的版本，然后推送最新的 Photon 到云平台。</p><p>然后在云平台通过可视化界面部署，选择部署名字，部署的资源种类等：</p><p><img data-src="/imgs/leptonai/20231014023711.png"></p><p><img data-src="/imgs/leptonai/20231014023738.png"></p><p>部署完成后，就可以用下面的Python代码来推理结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> leptonai.client <span class="keyword">import</span> Client</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">api_token = os.environ.get(<span class="string">&#x27;LEPTON_API_TOKEN&#x27;</span>)</span><br><span class="line">client = Client(<span class="string">&quot;lq87wh9y&quot;</span>, <span class="string">&quot;canny&quot;</span>, token=api_token)</span><br><span class="line"></span><br><span class="line">data = client.run(</span><br><span class="line">  url=<span class="string">&quot;https://i.stack.imgur.com/WsJGN.jpg&quot;</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">image_data = io.BytesIO(data)</span><br><span class="line">image = Image.<span class="built_in">open</span>(image_data)</span><br><span class="line">image.save(<span class="string">&quot;canny.jpg&quot;</span>)</span><br></pre></td></tr></table></figure><p>原始图和Canny结果图：<br>![[results.png)</p><p>如果不想用LeptonAI Python包，可以用 requests网络请求包来完成服务：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">api_token = os.environ.get(<span class="string">&quot;LEPTON_API_TOKEN&quot;</span>)</span><br><span class="line">url = <span class="string">&quot;https://xxxx-canny.bjz.edr.lepton.ai/run&quot;</span></span><br><span class="line">headers = &#123;</span><br><span class="line">    <span class="string">&quot;Content-Type&quot;</span>: <span class="string">&quot;application/json&quot;</span>,</span><br><span class="line">    <span class="string">&quot;accept&quot;</span>: <span class="string">&quot;image/png&quot;</span>,</span><br><span class="line">    <span class="string">&quot;Authorization&quot;</span>: <span class="string">f&quot;Bearer <span class="subst">&#123;api_token&#125;</span>&quot;</span>,</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">data = &#123;<span class="string">&quot;url&quot;</span>: <span class="string">&quot;https://i.stack.imgur.com/WsJGN.jpg&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">response = requests.post(url, headers=headers, json=data)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&quot;b.png&quot;</span>, <span class="string">&quot;wb&quot;</span>) <span class="keyword">as</span> file:</span><br><span class="line">    file.write(response.content)</span><br></pre></td></tr></table></figure><p>或者用部署页面提供的 cURL 命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">curl -X <span class="string">&#x27;POST&#x27;</span> \</span><br><span class="line">  <span class="string">&#x27;https://xxxx-canny.bjz.edr.lepton.ai/run&#x27;</span> \</span><br><span class="line">  -H <span class="string">&#x27;Content-Type: application/json&#x27;</span> \</span><br><span class="line">  -H <span class="string">&#x27;accept: image/png&#x27;</span> \</span><br><span class="line">  -H <span class="string">&quot;Authorization: Bearer <span class="variable">$LEPTON_API_TOKEN</span>&quot;</span> \</span><br><span class="line">  --output output.png \</span><br><span class="line">  -d <span class="string">&#x27;&#123;</span></span><br><span class="line"><span class="string">  &quot;url&quot;: &quot;string&quot;</span></span><br><span class="line"><span class="string">&#125;&#x27;</span></span><br></pre></td></tr></table></figure><p>也可以在网页端 Deployment 页面的 Demo 模块中进行推理，输入图片 url 即可得到结果：<br><img data-src="/imgs/leptonai/20231014023552.png"></p><p>这里的 Canny 算法这是一个简单的示例，实际中可以替换为任意的 AI 模型。</p><p>从这个简单的例子可以看出来，LeptonAI 将 AI 部署中的网站服务后端搭建给省掉了，让开发者专注于写算法部分，而且客户端调用服务的方式也很多，尽可能地简化了AI 服务的调用。</p><h2 id="4-一个Android端-AI程序Demo"><a href="#4-一个Android端-AI程序Demo" class="headerlink" title="4. 一个Android端 AI程序Demo"></a>4. 一个Android端 AI程序Demo</h2><p>为了验证 LeptonAI 对于AI App 的开发有没有提速，对于没有移动端开发的我，尝试不写一行 Java或者Object-C/Swift代码，纯粹利用 Python写一个简单的AI应用。</p><p>验证发现，利用 LeptonAI 算法平台 + Beeware Python 移动端程序开发工具， 纯Python代码在手机上运行起来了一个文生图的App。</p><p>视频如下：</p><p>源码在这里，最核心的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">An Android Application based on Python and LeptonAI!</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">import</span> toga</span><br><span class="line"><span class="keyword">from</span> toga.style <span class="keyword">import</span> Pack</span><br><span class="line"><span class="keyword">from</span> toga.style.pack <span class="keyword">import</span> COLUMN, ROW</span><br><span class="line"><span class="keyword">from</span> toga.images <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> io</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image <span class="keyword">as</span> PIL_Image</span><br><span class="line"><span class="keyword">from</span> leptonai.client <span class="keyword">import</span> Client</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">AISDK</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span>(<span class="params">self</span>):</span></span><br><span class="line">        LEPTON_API_TOKEN = <span class="string">&quot;xxxxxxxxxxxxxxx&quot;</span></span><br><span class="line">        self.client = Client(<span class="string">&quot;xxx&quot;</span>, <span class="string">&quot;sdxl&quot;</span>, token=LEPTON_API_TOKEN)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">process</span>(<span class="params">self, prompt, img_save_path</span>):</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;ai processing begin...&#x27;</span>)</span><br><span class="line">        data = self.client.run(</span><br><span class="line">            num_inference_steps=<span class="number">25</span>,</span><br><span class="line">            prompt=prompt,</span><br><span class="line">            seed=<span class="number">42</span>,</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        image_data = io.BytesIO(data)</span><br><span class="line">        image = PIL_Image.<span class="built_in">open</span>(image_data)</span><br><span class="line">        image.save(img_save_path)</span><br><span class="line">        <span class="built_in">print</span>(image.size)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;ai processing done&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sdxlandroid</span>(<span class="params">toga.App</span>):</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">startup</span>(<span class="params">self</span>):</span></span><br><span class="line"></span><br><span class="line">        self.sdk = AISDK()</span><br><span class="line">        self.img_save_path = <span class="string">&#x27;./tmp.jpg&#x27;</span></span><br><span class="line"></span><br><span class="line">        main_box = toga.Box(style=Pack(direction=COLUMN))</span><br><span class="line"></span><br><span class="line">        name_label = toga.Label(</span><br><span class="line">            <span class="string">&quot;Your prompt: &quot;</span>,</span><br><span class="line">            style=Pack(padding=(<span class="number">0</span>, <span class="number">5</span>))</span><br><span class="line">        )</span><br><span class="line">        self.name_input = toga.TextInput(style=Pack(flex=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        name_box = toga.Box(style=Pack(direction=ROW, padding=<span class="number">5</span>))</span><br><span class="line">        name_box.add(name_label)</span><br><span class="line">        name_box.add(self.name_input)</span><br><span class="line"></span><br><span class="line">        button = toga.Button(</span><br><span class="line">            <span class="string">&quot;Generate Image&quot;</span>,</span><br><span class="line">            on_press=self.run_aigc,</span><br><span class="line">            style=Pack(padding=<span class="number">5</span>)</span><br><span class="line">        )</span><br><span class="line"></span><br><span class="line">        main_box.add(name_box)</span><br><span class="line">        main_box.add(button)</span><br><span class="line"></span><br><span class="line">        self.main_window = toga.MainWindow(title=self.formal_name)</span><br><span class="line">        self.main_window.content = main_box</span><br><span class="line">        self.main_window.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">run_aigc</span>(<span class="params">self, widget</span>):</span></span><br><span class="line">        prompt = self.name_input.value</span><br><span class="line">        self.sdk.process(prompt, self.img_save_path)</span><br><span class="line"></span><br><span class="line">        image = Image(self.img_save_path)</span><br><span class="line">        image_view = toga.ImageView(image)</span><br><span class="line">        self.main_window.content.add(image_view)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    <span class="keyword">return</span> sdxlandroid()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p>可以看到，用不到100行的 Python 代码就能做出一个 AIGC 的简易手机端 App！</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1. 背景&quot;&gt;&lt;/a&gt;1. 背景&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://www.lepton.ai/&quot;&gt;LeptonAI&lt;/a&gt;是贾扬清的创业项目，正如 slogan “Build AI The Simple Way” 所表明的，LeptonAI的目标是简化AI模型的部署。&lt;/p&gt;
&lt;p&gt;简单来说，LeptonAI 提供了 &lt;a href=&quot;(https://github.com/leptonai/leptonai&quot;&gt;Python SDK&lt;/a&gt; 和云平台。Python SDK 可以让没有AI领域知识的普通开发者调用2～3行命令就能部署一个AI模型，然后用一个curl语句或几行Python代码就能完成客户端请求；而 LeptonAI 云平台提供了CPU，GPU和存储资源，开发者可以把创建的AI模型部署到这里，就能提供公开对外的AI服务。&lt;/p&gt;
&lt;p&gt;AI模型创建支持 HuggingFace，也就是说可以将 HuggingFace 上海量的模型集成到自己的应用中。同时 LeptonAI 也支持从 GitHub 仓库创建 AI 模型，给了开发者更多的选择。&lt;/p&gt;
&lt;p&gt;经过一段时间的体验后发现，LeptonAI Python SDK 设计的很优雅，用起来很舒服，而云平台的操作也很丝滑，有贾扬清大神亲自操刀写代码，SDK和云平台的质量绝对是信得过的。&lt;br&gt;&lt;img data-src=&quot;/imgs/leptonai/20231014072616.png&quot;&gt;&lt;/p&gt;
&lt;p&gt;那 LeptonAI 是否解决了一些 AI 部署中的痛点问题呢？我认为是的。根据之前的经验，跑一些 AI 开源模型成本还是挺高的。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="pip" scheme="http://vra.github.io/tags/pip/"/>
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="Pytorch" scheme="http://vra.github.io/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>Language Modeling Is Compression 论文阅读</title>
    <link href="http://vra.github.io/2023/10/03/language-modeling-is-compression/"/>
    <id>http://vra.github.io/2023/10/03/language-modeling-is-compression/</id>
    <published>2023-10-03T04:57:30.000Z</published>
    <updated>2023-12-09T01:43:43.635Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>这是DeepMind 最近发表的一篇论文，题目翻译成中文是“语言模型即压缩”，是一个很简单但也有分量的观点。更有趣的是，论文中作者发现，自己的预训练大模型Chinchilla 70B只在文本训练集上训练后，在ImagNet 图像Patch上压缩率能达到43.4%，优于PNG算法的压缩率58.5%，在LibrSpeech语音数据集上，压缩率达到16.4%，优于语音压缩算法FLAC30.3%的压缩率。</p><p>说实话看到这个结论我还是很震惊的。一个是文本数据上训练的模型居然也能用于图像和语音的压缩，而且压缩率还高于常用的领域特定的压缩算法。</p><p>除了震惊，还很好奇大模型是怎么被当作压缩器来压缩文本图像和语音数据的。在好奇心的驱使下，我读了几遍这篇论文。</p><span id="more"></span><p>除了上面的结论外，论文中作者提供了许多有启迪的观点，比如：</p><ol><li>通过推导，说明了压缩最优即是预测最优，即压缩和预测的等价性</li><li>传统压缩算法的特点是上下文很长（比如gzip 32K字节），可以利用具有丰富信息的上下文来进行压缩算法的设计，而Transformer结构的语言模型则是上下文很短（比如2048字节），通过大量参数来调节压缩的结果</li><li>Transformer模型是基于tokenizer压缩的数据进行训练的，因此tokenizer也是一种压缩器</li><li>从最右压缩算法的角度来看，语言模型作为压缩器，最优的模型大小是和数据强绑定的，不可能无限扩大，从这个角度一定程度给出了LLM的理论上限</li><li>传统压缩算法也能当作一种生成模型，给语言模型的研究带来新的可能性</li></ol><p>在这篇论文阅读文章中，我尝试就这些结论和几个关心的问题给出自己的解读，如有不准确之处，欢迎指出。</p><h2 id="2-为什么说压缩即预测"><a href="#2-为什么说压缩即预测" class="headerlink" title="2. 为什么说压缩即预测"></a>2. 为什么说压缩即预测</h2><p>作者首先是介绍了信息论的一些基本理论，然后结合语言模型的优化目标，引出了压缩最优即是预测最优。具体过程下面展开。</p><p>对一个序列（如“AIXI”），无损压缩的过程是将其转换为一种高效的表示（如01序列），同时能从该表示序列中恢复出原始的数据。无损压缩其实是一种编码过程。</p><p>对于一个离散随机变量，其信息熵表示为H(X)，计算公式为H(X) = - ∑ P(x) log2 P(x)，其中，P(x)表示随机变量X取值为x的概率。</p><p>根据香农的信息熵理论，对于最优的编码方案，平均编码长度L至少应该不小于信息熵H(X)。当编码长度等于信息熵时，即L = H(X)，编码达到了最优。</p><p>算术编码 (Arithmetic Coding) 是一种高效的无损压缩算法，是一种最优编码。概括来说，它将每个编码区间按照待编码序列的概率分布来进行划分，对于出现概率更大的序列元素，赋予更大的编码区间。下图演示了算数编码的过程，可以看到将4字节的输入编码为了7bit的输出，压缩率为7/(4x8) = 21.8%。具体算术编码的算法细节可以参考<a href="https://zhuanlan.zhihu.com/p/390684936">这篇文章</a>。<br><img data-src="/imgs/language_modeling_is_compression/20231003142114.png"></p><p>实际情况中，表示随机变量X取值为x的概率P(x)经常是难以获得的，因此采用P^来近似。基于这种近似，最优编码的期望长度如下，实际是一种交叉熵的表示。<br><img data-src="/imgs/language_modeling_is_compression/20231003142945.png"><br>这个表达式也刚好是现有语言模型的loss优化目标，也就是说，最小化压缩率等价于最小化语言模型的loss，即最优压缩即最优语言模型，从而证明了压缩和预测的等价性。</p><p>总结来说，这一部分根据压缩的最优目标和语言模型Loss的一致性，推导出两者其实是等价的。</p><h2 id="3-语言模型如何进行压缩"><a href="#3-语言模型如何进行压缩" class="headerlink" title="3. 语言模型如何进行压缩"></a>3. 语言模型如何进行压缩</h2><h3 id="3-1-压缩算法选择"><a href="#3-1-压缩算法选择" class="headerlink" title="3.1 压缩算法选择"></a>3.1 压缩算法选择</h3><p>对于传统压缩算法，作者选择了两个通用的压缩算法和两个特定模态的压缩算法。通用的压缩算法选择的是gzip和LZMA2。gzip相信大家都不陌生，用过linux的人都应该见过。而LZMA2是7z使用的压缩算法。这两者都算是久经考验的使用广泛的生产环境压缩算法，虽然不见得是最前沿的。</p><p>图像领域的压缩算法是PNG，语音领域的压缩算法是FLAC，两者都是无损压缩算法。</p><p>作为对比的语言模型选择了两个系列的大小不同的模型，比较小的是vanilla的只有decoder的 Transformer结构，在enwiki8上预训练（注意没有在别的数据上finetune)。比较大的是DeepMind之前提出的Chinchilla 系列模型，也只在文本上训练，没有见过图像和语音数据。</p><h3 id="3-2-数据设置"><a href="#3-2-数据设置" class="headerlink" title="3.2 数据设置"></a>3.2 数据设置</h3><p>数据集的选择也是很有技巧。首先是对于传统压缩算法和LLM，应用场景大相径庭，该怎么选择测试集进行合理的比较呢。</p><p>对于语言模型，由于常见模型输入context是2048，因此选择了把数据切分成N段2048字节的数据，来进行测试。<br>对于传统压缩算法，显然是可以将整个数据都作为整体进行压缩的，而切分成小块进行压缩是对结果有损害的。所以作者对传统压缩算法采用了两种处理方式。</p><p>为了方便比较不同模态的数据，作者都是构造总大小为1GB的数据集。</p><p>文本数据，作者采用了截止到2006年3月3日的维基百科英文预料的前1e8个字节，作为enwiki8，前1e9字节作为enwiki9。可以看到，enwiki8是包含在enwiki9中的，而enwiki9刚好是1G Bytes。</p><p>图像数据采用的是ImageNet数据集，选择了488821张图片，每张图片选择32x64的patch，然后转换为灰度图(每个像素刚好1个字节），刚好是488821x32x64=1G Bytes。</p><p>语音数据是从LibriSpeech数据集中选择的，每段2048字节，选择了488821段，也是刚好1G字节。</p><h3 id="3-3-如何进行数据压缩"><a href="#3-3-如何进行数据压缩" class="headerlink" title="3.3 如何进行数据压缩"></a>3.3 如何进行数据压缩</h3><p>对于语言模型进行数据压缩的细节，论文没有描述，我只能说一下我的猜测。作者采用算术编码的方式，根据给定每个输入后模型的输出的概率，利用概率大小划分编码空间，对序列中所有输入元素执行完一次推理，得到最终的编码结果。</p><h3 id="3-4-压缩结果"><a href="#3-4-压缩结果" class="headerlink" title="3.4 压缩结果"></a>3.4 压缩结果</h3><p>下面的表格展示了整体的压缩结果。<br>chunk size 无穷大表示不进行数据的切分，也就是传统压缩算法的压缩过程。<br>压缩率等于压缩后文件大小/原始文件大小，其中Raw压缩率没有考虑模型尺寸大小，二调整后的压缩率将模型的参数量也计算进了压缩后的文件大小中，由于语言模型参数量很大，因此调整后的压缩率甚至超过了一百。</p><p><img data-src="/imgs/language_modeling_is_compression/20231003010543.png"></p><p>从这个表格中可以得出一些结论：</p><ol><li>传统压缩算法经过分块，确实有比较大的性能下降，说明大的上下文信息对压缩至关重要</li><li>针对图像设计的压缩算法PNG在语音数据上LibrSpeech也能得到较好的压缩结果</li><li>LZMA2通用压缩算法在LibriSpeech数据上压缩率优于语音特定的FLAC算法，还是有点出乎意料</li><li>由于传统压缩算法没有模型参数，因此调整后压缩率等于Raw压缩率</li><li>在enwik8上训练的Transformer模型，增大参数后在同模态的enwiki9上效果会变好，而在不同模态的图片和语音数据上，增大参数量到一定程度后效果反而下降</li><li>而Chinchilla语言模型参数量从1B到7B再到70B，压缩性能都是在稳步提升，与Vanilla Transformer呈现不同的现象</li></ol><p>下面图中是在enwiki8上预训练到不同参数量的Transformer在enwiki7/8/9上的压缩率，其中压缩率是调整后压缩率，也就是考虑了参数量。<br><img data-src="/imgs/language_modeling_is_compression/20231003091610.png"><br>从这里可以看出一些结论：</p><ol><li>随着参数量的增大，压缩性能都下降了（表现为压缩率增大），这是因为参数量的增大对结果的影响比较大</li><li>对于大的数据集(enwiki9)，更多参数的模型压缩性能更好，但更大的模型在小的数据集上效果比较差，也就是说从压缩的角度看，模型的Scaling，与测试集的大小有关</li></ol><p>为了对比不同压缩算法的生成能力，作者比较了gzip和Chinchilla 70B 在文本，图像和语音上的生成能力，具体来说，给出前面的的内容（文本是前1948个字节预测最后100个字节，图像和语音是给出前面一半预测后面一半），让算法来预测后面一半的内容。注意这个生成过程和压缩过程是不同的。</p><p>文本预测结果如下<br><img data-src="/imgs/language_modeling_is_compression/20231003092610.png"><br>可以看到语言模型预测的结果更连贯，而gzip预测的结果则没有明显的含义</p><p>语音预测上，gzip的预测更自然一些，而语言模型的预测则进入了循环，类似大家使用代码补全模型时经常会遇到的循环一样，不知道是不是同样的效应。<br><img data-src="/imgs/language_modeling_is_compression/20231003092857.png"></p><p>图像生成方面，语言模型和gzip都是从已知的图像信息中预测颜色值，不过两者的分布还是挺不一样的，gzip像是水平和竖直方向规律的噪声，而语言模型则明显的更能体验row-wise的效应。<br><img data-src="/imgs/language_modeling_is_compression/20231003093107.png"></p><p>另外一个实验是关于序列长度对压缩性能的影响。可以看到增大序列长度，三个任务上所有压缩算法的压缩性能都提升了。而Chinchilla 的压缩性能最好。<br><img data-src="/imgs/language_modeling_is_compression/20231003134503.png"><br>理论上来讲，gzip，lzma等传统压缩算法在序列长度增大后继续增大压缩性能，直到达到非切块场景的效果。</p><p>然后作者还比较了在Vanilla Transformer结构上，采用不同tokenization方法在3个不同参数的网络上的Raw压缩率。可以看到，对于比较小的网络(200K),增大tokenization的词汇大小能提高压缩性能，但针对大模型，增大词汇量压缩性能变差，可以看到tokenization的选择对结果还是有挺大影响的。<br><img data-src="/imgs/language_modeling_is_compression/20231003135231.png"></p><h2 id="5-疑问，思考和总结"><a href="#5-疑问，思考和总结" class="headerlink" title="5. 疑问，思考和总结"></a>5. 疑问，思考和总结</h2><p>看论文是有一个直观的疑问：使用大模型时，对同一个问题，输出的结果每次都是不一样的，怎么能保证解压缩时的结果的一致性呢？</p><p>其实大模型每次给出不同结果，是因为处理过程中故意增加了随机性，意图给出更丰富多样的回答。如果去掉随机性，得到的结果是完全一样的，比如哪个token的输出概率最大都是确定的。</p><p>还有这篇文章对于语言模型进行压缩的细节描述的太少了，甚至连一个示例图片或者示例伪代码都没有，导致读论文时很难想象这部分的细节，比如几个疑问，在文本上训练的语言模型是怎么在图像和语音任务上使用的，实际中怎么将语言模型当作算术编码来使用，等等。不知道作者为什么这么处理。</p><p>我觉得这篇论文可能会给相关研究方向带来新的想象空间，比如，文本语料上训练的模型也能在图像和音频数据上获得很好的压缩率，那是不是对于不同模态的输入，也可以设计同一个LLM来统一不同的任务呢？</p><p>再比如，既然像gzip这种传统压缩算法也能作为一个生成模型，而且运行成本远小于现有的LLM，那能否基于这种算法进行AI模型的演进呢？也许是一个更实用的方向。</p><p>总之这篇论文从压缩算法的角度提出了很多有意思有启发的观点，希望能对AI的研究产生新的前进推力。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h2&gt;&lt;p&gt;这是DeepMind 最近发表的一篇论文，题目翻译成中文是“语言模型即压缩”，是一个很简单但也有分量的观点。更有趣的是，论文中作者发现，自己的预训练大模型Chinchilla 70B只在文本训练集上训练后，在ImagNet 图像Patch上压缩率能达到43.4%，优于PNG算法的压缩率58.5%，在LibrSpeech语音数据集上，压缩率达到16.4%，优于语音压缩算法FLAC30.3%的压缩率。&lt;/p&gt;
&lt;p&gt;说实话看到这个结论我还是很震惊的。一个是文本数据上训练的模型居然也能用于图像和语音的压缩，而且压缩率还高于常用的领域特定的压缩算法。&lt;/p&gt;
&lt;p&gt;除了震惊，还很好奇大模型是怎么被当作压缩器来压缩文本图像和语音数据的。在好奇心的驱使下，我读了几遍这篇论文。&lt;/p&gt;</summary>
    
    
    
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="Paper Reading" scheme="http://vra.github.io/tags/Paper-Reading/"/>
    
  </entry>
  
  <entry>
    <title>Numpy set_printoptions函数用法</title>
    <link href="http://vra.github.io/2023/09/30/numpy-printoptions/"/>
    <id>http://vra.github.io/2023/09/30/numpy-printoptions/</id>
    <published>2023-09-30T04:57:30.000Z</published>
    <updated>2023-09-30T04:58:30.177Z</updated>
    
    <content type="html"><![CDATA[<p>Numpy是Python中常用的数值计算库，我们经常需要用到Numpy来打印数值，查看结果。为了能精确地控制Numpy打印的信息，Numpy提供了<code>set_printoptions</code> <a href="https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html">函数</a>，包含数个参数，能满足数值打印的需要。</p><p>这里以iPython中操作作为示例，从浅入深，一步步地探索set_printoptions提供的功能，如何来满足我们的打印需求。</p><span id="more"></span><h3 id="precision"><a href="#precision" class="headerlink" title="precision"></a>precision</h3><p>首先用Numpy创建一个float64 类型的np.ndarray，并打印数值：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: a = np.random.rand(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">0.63039295</span> <span class="number">0.09185505</span> <span class="number">0.02203224</span>]</span><br></pre></td></tr></table></figure><p>可以看到输出的float数组保留了8位小数位，这是因为Numpy默认的设置是显示8位小数位。<br>如果只想显示2位小数位，该怎么操作呢？<br>这时候就需要用到<code>set_printoptions</code>的<code>precsion</code>的选项了，它就是用来控制显示的小数位：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: np.set_printoptions(precision=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">0.6304</span> <span class="number">0.0919</span> <span class="number">0.022</span> ]</span><br></pre></td></tr></table></figure><p>可以看到通过设置precsion=4，显示的数组输出保留4位小数。</p><p>⚠️需要注意的是，这个设置对float类型的数值无效：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">14</span>]: a = np.random.rand()</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: <span class="built_in">type</span>(a)</span><br><span class="line">Out[<span class="number">15</span>]: <span class="built_in">float</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">16</span>]: <span class="built_in">print</span>(a)</span><br><span class="line"><span class="number">0.40944018143470295</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">17</span>]: np.set_printoptions(precision=<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">18</span>]: <span class="built_in">print</span>(a)</span><br><span class="line"><span class="number">0.40944018143470295</span></span><br></pre></td></tr></table></figure><p>所以使用时注意类型是<code>np.ndarray</code>还是<code>float</code>。</p><h3 id="suppress"><a href="#suppress" class="headerlink" title="suppress"></a>suppress</h3><p>假设我们需要获取一组很小的数值，并且需要显示结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">2</span>]: a = np.random.rand(<span class="number">3</span>) * <span class="number">1e-5</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">9.49522547e-06</span> <span class="number">4.55101001e-06</span> <span class="number">4.01284118e-06</span>]</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>可以看到打印时用了科学计数法。<br>有没有办法不使用科学计数法呢，<code>set_printoptions</code>提供了<code>suppress</code>参数，将其设置为<code>True</code>，就会禁用科学计数法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: np.set_printoptions(suppress=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">0.0000095</span>  <span class="number">0.00000455</span> <span class="number">0.00000401</span>]</span><br></pre></td></tr></table></figure><p><code>suppress</code> 参数有一个例外情况，就是对于整数部分大于8位的，即使设置<code>suppress=True</code> ，仍然会显示科学计数法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">8</span>]: a = np.random.rand(<span class="number">3</span>) * <span class="number">1e10</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">9.35772525e+09</span> <span class="number">4.14513333e+09</span> <span class="number">4.59176775e+09</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: a = np.random.rand(<span class="number">3</span>) * <span class="number">1e8</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">37984517.91633694</span> <span class="number">87748330.34519586</span> <span class="number">21101693.42416701</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">12</span>]: a = np.random.rand(<span class="number">3</span>) * <span class="number">1e9</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">4.46826342e+08</span> <span class="number">5.17327105e+08</span> <span class="number">9.07218130e+08</span>]</span><br></pre></td></tr></table></figure><p>那有没有办法解决这个问题呢？这里就需要用到<code>set_printoptions</code> 提供的另一个参数<code>formatter</code>。</p><h3 id="formatter"><a href="#formatter" class="headerlink" title="formatter"></a>formatter</h3><p><code>formatter</code>接受一个dict类型的参数，其中dict的key表示参数的类型，而dict的value则是一个函数或者format字符串，表示如何对对应的类型进行打印。</p><p>举个简单的例子，我想在所有float类型的数组的每个元素后面加一个字母<code>f</code>:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">21</span>]: a = np.random.rand(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">22</span>]: np.set_printoptions(precision=<span class="number">4</span>, formatter=&#123;<span class="string">&#x27;float&#x27;</span>: <span class="keyword">lambda</span> x: <span class="built_in">str</span>(x) + <span class="string">&#x27;f&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">23</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">0.6925034861246904</span>f <span class="number">0.0613911477046164</span>f <span class="number">0.3348313234151774</span>f]</span><br></pre></td></tr></table></figure><p><code>formatter</code>参数是个dict，key是“float”，表示对float类型的数组进行操作，value是一个lambda函数，将输入转换为str字符串再加一个f。</p><p>在这里也可以看到，np.float64数组中元素的实际长度是16位小数。默认显示的8位数值只是它的一个近似。</p><p>除了lamda函数外，也可以用Python的format格式函数来作为<code>formatter</code>参数dict的value：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">33</span>]: a = np.random.rand(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">34</span>]: np.set_printoptions(formatter=&#123;<span class="string">&#x27;float&#x27;</span>: <span class="string">&#x27;&#123;:.2f&#125;&#x27;</span>.<span class="built_in">format</span>&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">35</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">0.70</span> <span class="number">0.91</span> <span class="number">0.82</span>]</span><br></pre></td></tr></table></figure><p>可以看到，这里可以用f-string和format函数中使用的语法格式，对于用惯f-string的小伙伴来说，以这种方式来控制显示格式简直太舒服了。</p><p>关于python format的语法，可以参考我之前写的<a href="https://zhuanlan.zhihu.com/p/632687543">教程</a>。</p><p>另外需要注意，设置<code>formatter</code>后，会覆盖<code>precision</code>参数，也就是显示多少位数以<code>formatter</code>中设置为准:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">25</span>]: a = np.random.rand(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">26</span>]: np.set_printoptions(precision=<span class="number">4</span>, formatter=&#123;<span class="string">&#x27;float&#x27;</span>: <span class="keyword">lambda</span> x: <span class="built_in">str</span>(x) + <span class="string">&#x27;f&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">27</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">0.1604388367489663</span>f <span class="number">0.6047908263355061</span>f <span class="number">0.1645621828526913</span>f]</span><br></pre></td></tr></table></figure><p>根据Numpy 文档，<code>formatter</code>支持的类型包括下面这些：</p><ul><li>‘bool’</li><li>‘int’</li><li>‘timedelta’ : a <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.timedelta64" title="numpy.timedelta64"><code>numpy.timedelta64</code></a></li><li>‘datetime’ : a <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.datetime64" title="numpy.datetime64"><code>numpy.datetime64</code></a></li><li>‘float’</li><li>‘longfloat’ : 128-bit floats</li><li>‘complexfloat’</li><li>‘longcomplexfloat’ : composed of two 128-bit floats</li><li>‘numpystr’ : types <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.bytes_" title="numpy.bytes_"><code>numpy.bytes_</code></a> and <a href="https://numpy.org/doc/stable/reference/arrays.scalars.html#numpy.str_" title="numpy.str_"><code>numpy.str_</code></a></li><li>‘object’ : <em>np.object</em>_ arrays</li><li>‘all’ : sets all types</li><li>‘int_kind’ : sets ‘int’</li><li>‘float_kind’ : sets ‘float’ and ‘longfloat’</li><li>‘complex_kind’ : sets ‘complexfloat’ and ‘longcomplexfloat’</li><li>‘str_kind’ : sets ‘numpystr</li></ul><p>好了，说了这么多，那回到上面的问题，到底该怎么控制整数位大于8的float数组不用科学计数法呢？有了formatter参数，就很简单了：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">36</span>]: a = np.random.rand(<span class="number">3</span>) * <span class="number">1e10</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">37</span>]: np.set_printoptions(formatter=&#123;<span class="string">&#x27;float&#x27;</span>: <span class="string">&#x27;&#123;:.8f&#125;&#x27;</span>.<span class="built_in">format</span>&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">38</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">7694883457.28612423</span> <span class="number">864845466.08411431</span> <span class="number">6505022487.23314571</span>]</span><br></pre></td></tr></table></figure><p>使用format格式语言轻松完成。</p><p>有些时候，数组中的元素长度各不相同，打印时要么对不齐不好查看，要么自动转换为科学计数法也不好分析，利用<code>formatter</code>能够显示对齐的数值，大大方便了数据查看：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: a = np.array(</span><br><span class="line">   ...: [[<span class="number">1</span>,  -<span class="number">1000</span>, <span class="number">2222222222.33333333</span>],</span><br><span class="line">   ...: [<span class="number">233</span>, <span class="number">240.03333333333333333333333333</span>, <span class="number">8.0</span>],</span><br><span class="line">   ...: [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]]</span><br><span class="line">   ...: )</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[[ <span class="number">1.00000000e+00</span> -<span class="number">1.00000000e+03</span>  <span class="number">2.22222222e+09</span>]</span><br><span class="line"> [ <span class="number">2.33000000e+02</span>  <span class="number">2.40033333e+02</span>  <span class="number">8.00000000e+00</span>]</span><br><span class="line"> [ <span class="number">1.00000000e+00</span>  <span class="number">2.00000000e+00</span>  <span class="number">3.00000000e+00</span>]]</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: np.set_printoptions(formatter=&#123;<span class="string">&#x27;float&#x27;</span>: <span class="string">&#x27;&#123;:&gt;20.8f&#125;&#x27;</span>.<span class="built_in">format</span>&#125;)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[[          <span class="number">1.00000000</span>       -<span class="number">1000.00000000</span>  <span class="number">2222222222.33333349</span>]</span><br><span class="line"> [        <span class="number">233.00000000</span>         <span class="number">240.03333333</span>           <span class="number">8.00000000</span>]</span><br><span class="line"> [          <span class="number">1.00000000</span>           <span class="number">2.00000000</span>           <span class="number">3.00000000</span>]]</span><br></pre></td></tr></table></figure><p>这里利用了<code>&gt;N</code>的format语法，向右对齐。</p><h3 id="threshold和edgeitems"><a href="#threshold和edgeitems" class="headerlink" title="threshold和edgeitems"></a>threshold和edgeitems</h3><p>假如我们有一个很大的数组(1024x4)，打印时默认只显示开始三行和最后三行：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: a = np.random.rand(<span class="number">1024</span>, <span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[[<span class="number">0.5159347</span>  <span class="number">0.06396333</span> <span class="number">0.18446106</span> <span class="number">0.06163127</span>]</span><br><span class="line"> [<span class="number">0.96894042</span> <span class="number">0.278889</span>   <span class="number">0.25117021</span> <span class="number">0.9757328</span> ]</span><br><span class="line"> [<span class="number">0.42980522</span> <span class="number">0.44724705</span> <span class="number">0.89322128</span> <span class="number">0.19697129</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">0.31956847</span> <span class="number">0.4790065</span>  <span class="number">0.45595315</span> <span class="number">0.98816687</span>]</span><br><span class="line"> [<span class="number">0.35240443</span> <span class="number">0.44400784</span> <span class="number">0.76815952</span> <span class="number">0.18499155</span>]</span><br><span class="line"> [<span class="number">0.33888548</span> <span class="number">0.50811964</span> <span class="number">0.32341108</span> <span class="number">0.98617324</span>]]</span><br></pre></td></tr></table></figure><p>这是因为Numpy默认设置，当数组的元素个数大于1000时，就会只显示开头和结尾部分。</p><p>如果想多显示一些数据，看更多内容，该怎么操作呢？<br><code>set_printoptions</code>提供了<code>threshold</code>参数，用于控制多少个元素后显示部分，另一个参数<code>edgeitems</code>,控制显示缩略部分的行数。</p><p>因此可以修改这两个参数，修改显示效果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">4</span>]: np.set_printoptions(edgeitems=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[[<span class="number">0.5159347</span>  <span class="number">0.06396333</span> <span class="number">0.18446106</span> <span class="number">0.06163127</span>]</span><br><span class="line"> [<span class="number">0.96894042</span> <span class="number">0.278889</span>   <span class="number">0.25117021</span> <span class="number">0.9757328</span> ]</span><br><span class="line"> [<span class="number">0.42980522</span> <span class="number">0.44724705</span> <span class="number">0.89322128</span> <span class="number">0.19697129</span>]</span><br><span class="line"> [<span class="number">0.41831831</span> <span class="number">0.32864348</span> <span class="number">0.9599147</span>  <span class="number">0.04244498</span>]</span><br><span class="line"> [<span class="number">0.17307071</span> <span class="number">0.70541496</span> <span class="number">0.12485861</span> <span class="number">0.68987846</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">0.36880553</span> <span class="number">0.66404444</span> <span class="number">0.12623872</span> <span class="number">0.32754608</span>]</span><br><span class="line"> [<span class="number">0.53076768</span> <span class="number">0.76770867</span> <span class="number">0.36680954</span> <span class="number">0.58596153</span>]</span><br><span class="line"> [<span class="number">0.31956847</span> <span class="number">0.4790065</span>  <span class="number">0.45595315</span> <span class="number">0.98816687</span>]</span><br><span class="line"> [<span class="number">0.35240443</span> <span class="number">0.44400784</span> <span class="number">0.76815952</span> <span class="number">0.18499155</span>]</span><br><span class="line"> [<span class="number">0.33888548</span> <span class="number">0.50811964</span> <span class="number">0.32341108</span> <span class="number">0.98617324</span>]]</span><br></pre></td></tr></table></figure><h3 id="linewidth"><a href="#linewidth" class="headerlink" title="linewidth"></a>linewidth</h3><p>linewidth参数用来控制一行显示多少个字符，默认是75，通过修改这个参数，能在一行显示更多数据：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">n [<span class="number">3</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: a = np.random.rand(<span class="number">1024</span>,<span class="number">6</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: np.set_printoptions(precision=<span class="number">16</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[[<span class="number">0.6151590922948798</span> <span class="number">0.8394381715187383</span> <span class="number">0.1287492144726177</span></span><br><span class="line">  <span class="number">0.432486748198503</span>  <span class="number">0.008210600687992</span>  <span class="number">0.5251777687645207</span>]</span><br><span class="line"> [<span class="number">0.8986836534319551</span> <span class="number">0.5275521098334796</span> <span class="number">0.1275787604074625</span></span><br><span class="line">  <span class="number">0.2088067024068581</span> <span class="number">0.9728215202746345</span> <span class="number">0.0222310180458779</span>]</span><br><span class="line"> [<span class="number">0.1919751621010076</span> <span class="number">0.7593251629630882</span> <span class="number">0.2216025287318845</span></span><br><span class="line">  <span class="number">0.1693395870716256</span> <span class="number">0.0447174013709218</span> <span class="number">0.2669167788671162</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">0.2056367250351134</span> <span class="number">0.1961953658298233</span> <span class="number">0.6844119224272207</span></span><br><span class="line">  <span class="number">0.396808314963211</span>  <span class="number">0.2270659358855954</span> <span class="number">0.1694468143457141</span>]</span><br><span class="line"> [<span class="number">0.0404784779577213</span> <span class="number">0.977932794679906</span>  <span class="number">0.319154876583544</span></span><br><span class="line">  <span class="number">0.6301954893143036</span> <span class="number">0.4533581710958777</span> <span class="number">0.4980767389069806</span>]</span><br><span class="line"> [<span class="number">0.5722796781670568</span> <span class="number">0.8683487818109435</span> <span class="number">0.819417328117305</span></span><br><span class="line">  <span class="number">0.5286251921005498</span> <span class="number">0.2252964609019765</span> <span class="number">0.7439441509500194</span>]]</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: np.set_printoptions(linewidth=<span class="number">150</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[[<span class="number">0.6151590922948798</span> <span class="number">0.8394381715187383</span> <span class="number">0.1287492144726177</span> <span class="number">0.432486748198503</span>  <span class="number">0.008210600687992</span>  <span class="number">0.5251777687645207</span>]</span><br><span class="line"> [<span class="number">0.8986836534319551</span> <span class="number">0.5275521098334796</span> <span class="number">0.1275787604074625</span> <span class="number">0.2088067024068581</span> <span class="number">0.9728215202746345</span> <span class="number">0.0222310180458779</span>]</span><br><span class="line"> [<span class="number">0.1919751621010076</span> <span class="number">0.7593251629630882</span> <span class="number">0.2216025287318845</span> <span class="number">0.1693395870716256</span> <span class="number">0.0447174013709218</span> <span class="number">0.2669167788671162</span>]</span><br><span class="line"> ...</span><br><span class="line"> [<span class="number">0.2056367250351134</span> <span class="number">0.1961953658298233</span> <span class="number">0.6844119224272207</span> <span class="number">0.396808314963211</span>  <span class="number">0.2270659358855954</span> <span class="number">0.1694468143457141</span>]</span><br><span class="line"> [<span class="number">0.0404784779577213</span> <span class="number">0.977932794679906</span>  <span class="number">0.319154876583544</span>  <span class="number">0.6301954893143036</span> <span class="number">0.4533581710958777</span> <span class="number">0.4980767389069806</span>]</span><br><span class="line"> [<span class="number">0.5722796781670568</span> <span class="number">0.8683487818109435</span> <span class="number">0.819417328117305</span>  <span class="number">0.5286251921005498</span> <span class="number">0.2252964609019765</span> <span class="number">0.7439441509500194</span>]]</span><br></pre></td></tr></table></figure><p>可以看到，增加linewidth到150后，以前一行显示不了的数据现在可以在一行上显示了。</p><h3 id="nanstr和infstr"><a href="#nanstr和infstr" class="headerlink" title="nanstr和infstr"></a>nanstr和infstr</h3><p>nanstr和infstr参数用来控制nan和inf数值的显示字符，默认是<code>nan</code>和<code>inf</code>，如果好奇想修改的话，可以设置对应的参数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">12</span>]: a = np.array([np.nan, np.inf])</span><br><span class="line"></span><br><span class="line">In [<span class="number">13</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[nan inf]</span><br><span class="line"></span><br><span class="line">In [<span class="number">14</span>]: np.set_printoptions(nanstr=<span class="string">&#x27;非数&#x27;</span>, infstr=<span class="string">&#x27;∞&#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">15</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[非数  ∞]</span><br></pre></td></tr></table></figure><p>有点好玩，但建议别修改，不然别人不知道你在do what 🤷</p><h3 id="sign"><a href="#sign" class="headerlink" title="sign"></a>sign</h3><p>sign参数用来控制每个数字前显示的符号，默认是<code>-</code>,也就是只有负数前面显示减号。如果是<code>+</code>，则在正数前面添加加号。如果是<code> </code>,则在正数前面添加空格：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">In [<span class="number">2</span>]: a = np.random.rand(<span class="number">3</span>) - <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">3</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[ <span class="number">0.44889495</span> -<span class="number">0.25608263</span> -<span class="number">0.23228835</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: np.set_printoptions(sign=<span class="string">&#x27;+&#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[+<span class="number">0.44889495</span> -<span class="number">0.25608263</span> -<span class="number">0.23228835</span>]</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: np.set_printoptions(sign=<span class="string">&#x27; &#x27;</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[ <span class="number">0.44889495</span> -<span class="number">0.25608263</span> -<span class="number">0.23228835</span>]</span><br></pre></td></tr></table></figure><h3 id="恢复默认配置"><a href="#恢复默认配置" class="headerlink" title="恢复默认配置"></a>恢复默认配置</h3><p>如何恢复默认配置呢？可以这么设置：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">np.set_printoptions(edgeitems=<span class="number">3</span>, infstr=<span class="string">&#x27;inf&#x27;</span>,linewidth=<span class="number">75</span>, nanstr=<span class="string">&#x27;nan&#x27;</span>, precision=<span class="number">8</span>,suppress=<span class="literal">False</span>, threshold=<span class="number">1000</span>, formatter=<span class="literal">None</span>)</span><br></pre></td></tr></table></figure><h3 id="使用with语句"><a href="#使用with语句" class="headerlink" title="使用with语句"></a>使用with语句</h3><p>通过使用with语句，可以临时修改打印配置项，在退出with语句的时候恢复默认配置，这样也减少侵入式地修改，避免造成不必要的后果。<br>在使用with语句时，需要将<code>np.set_printoptions</code> 替换为<code>np.printoptions</code>，也就是去掉函数中的<code>set_</code>前缀，函数的所有参数都一样。使用例子：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">6</span>]: a = np.random.rand(<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">In [<span class="number">7</span>]: <span class="keyword">with</span> np.printoptions(formatter=&#123;<span class="string">&#x27;float&#x27;</span>: <span class="keyword">lambda</span> x: <span class="built_in">str</span>(x)+<span class="string">&#x27;f&#x27;</span>&#125;):</span><br><span class="line">   ...:     <span class="built_in">print</span>(a)</span><br><span class="line">   ...:</span><br><span class="line">[<span class="number">0.24752544521208586</span>f <span class="number">0.01852100917834376</span>f <span class="number">0.9358432384604951</span>f]</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: <span class="built_in">print</span>(a)</span><br><span class="line">[<span class="number">0.24752544521208586</span>f <span class="number">0.01852100917834376</span>f <span class="number">0.9358432384604951</span>f]</span><br></pre></td></tr></table></figure><p>这就是<code>set_printoptions</code>几乎全部的参数了，更多用法，参考官方<a href="https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html">文档</a>。</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;Numpy是Python中常用的数值计算库，我们经常需要用到Numpy来打印数值，查看结果。为了能精确地控制Numpy打印的信息，Numpy提供了&lt;code&gt;set_printoptions&lt;/code&gt; &lt;a href=&quot;https://numpy.org/doc/stable/reference/generated/numpy.set_printoptions.html&quot;&gt;函数&lt;/a&gt;，包含数个参数，能满足数值打印的需要。&lt;/p&gt;
&lt;p&gt;这里以iPython中操作作为示例，从浅入深，一步步地探索set_printoptions提供的功能，如何来满足我们的打印需求。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="Numpy" scheme="http://vra.github.io/tags/Numpy/"/>
    
  </entry>
  
  <entry>
    <title>Code Llama 解读系列1-论文阅读</title>
    <link href="http://vra.github.io/2023/09/29/code-llama-1/"/>
    <id>http://vra.github.io/2023/09/29/code-llama-1/</id>
    <published>2023-09-29T02:17:04.000Z</published>
    <updated>2023-09-29T02:40:12.160Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Code Llama 是 Meta 基于 Llama 2 的代码生成AI模型， 在同类开源模型中取得比较好的结果。这里计划写3篇系列文章，从论文细节、代码使用、效果实测方面对 Code Llama 进行解读，欢迎关注我了解后续文章。</p></blockquote><h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h3><p>2023年8月24日，Meta 开源了基于 <a href="https://github.com/facebookresearch/llama">Llama 2</a>) 通用 LLM 的代码生成系列模型 <a href="https://github.com/facebookresearch/codellama">Code Llama</a>)，支持Python, C++, Java, PHP, TypeScript, C# 和 Bash 编程语言，而且支持学术研究和商业使用。</p><p>另外 Code Llama 官方代码只提供了一些简单的使用示例，没有提供生产环境可用的 VSCode 等 工具的插件，搜索了一下也没找到简单易用的第三方开发的插件。相信很快就会有人做出来的。如果你有看到基于 Code Llama 的 VSCode 或者 Vim 插件，欢迎评论指教。</p><span id="more"></span><p>一些链接：</p><ul><li>代码仓库: <a href="https://github.com/facebookresearch/codellama">https://github.com/facebookresearch/codellama</a></li><li>论文PDF: <a href="https://scontent-sjc3-1.xx.fbcdn.net/v/t39.2365-6/369856151_1754812304950972_1159666448927483931_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=wURKmnWKaloAX-JEHAz&_nc_ht=scontent-sjc3-1.xx&oh=00_AfBOeTPJWHrxyxjNs4TLPACB4M7xQIwQcM5SMRMzDo8uCg&oe=64EEAC4F">链接</a></li><li>Meta AI 博客文章：<a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/">链接</a></li></ul><h3 id="2-数据说明"><a href="#2-数据说明" class="headerlink" title="2. 数据说明"></a>2. 数据说明</h3><p>这篇论文中提到了几个不同的数据，有一些数据的构造还是挺巧妙的，这里列出来，希望能给大家一些启发。</p><h4 id="2-1-2T-token-dataset"><a href="#2-1-2T-token-dataset" class="headerlink" title="2.1 2T token dataset"></a>2.1 2T token dataset</h4><p>首先是 Llama 2 的训练数据，虽然不是这篇论文的贡献，但因为 Code Llama 模型都是从 Llama 2 初始化的，所以 这些代码生成的特化模型也都是见过这些数据的，包含它们中的知识。</p><p>Llama 2 是使用 2T token 数据训练的，其中代码相关的部分有80B token，占比只有4%。</p><h4 id="2-2-500B-token-dataset"><a href="#2-2-500B-token-dataset" class="headerlink" title="2.2 500B token dataset"></a>2.2 500B token dataset</h4><p>这篇论文先是提出了通用的 500B tokens 数据集, 85% 都是关于代码的，所以可以认为这个500B 就是一个代码数据集。</p><h4 id="2-3-100B-token-Python-dataset"><a href="#2-3-100B-token-Python-dataset" class="headerlink" title="2.3 100B token Python dataset"></a>2.3 100B token Python dataset</h4><p>除了通用的 500B token 代码数据集，为了提高对 Python 代码的生成能力，论文又提出了 100B token Python dataset。</p><h4 id="2-4-RLHF-V5-dataset"><a href="#2-4-RLHF-V5-dataset" class="headerlink" title="2.4 RLHF V5 dataset"></a>2.4 RLHF V5 dataset</h4><p>这是 Llama 2 论文中使用的人工纠正的数据集，是为了让代码可以更好的对应回答提问者的指令，也为了生成更安全的代码（可以理解成对生成的代码中某些危险的代码进行过滤？）</p><h4 id="2-5-self-instruct-5B-token-dataset"><a href="#2-5-self-instruct-5B-token-dataset" class="headerlink" title="2.5 self-instruct 5B token dataset"></a>2.5 self-instruct 5B token dataset</h4><p>self-instruct 是一个生成代码数据集，通过给llama2提问代码任务，得到它的结果，作为gt。 会不会存在错误答案？这里论文设计了一个很精巧的方案来构造生成代码数据：</p><ol><li>让 LLama 2 70B 大模型设计 Easy 和 Medium 难度的编程题，每次出50道题目，要求不重复，且可以用一个单独的Python函数来实现。总共得到52000个去重的问题。下面是提示词和一些回答的例子：<br><img data-src="/imgs/code_llama_1/20230826223518.png"></li><li>对上面得到的每个问题，用 Code Llama 7B 模型生成 单元测试代码和10个解题的代码，然后在解题代码上运行单元测试，将第一个通过单元测试的代码加入到 self-instruct 数据集中。</li></ol><p>这是一种很巧妙的设计，通过单元测试来判断代码的对错，能够做到完全自动化地构造数据。</p><p>当然如果单元测试代码本身错，那可能会将错误的解题代码加入到训练集中。而根据<a href="https://arxiv.org/abs/2308.02312">这篇论文</a>)的分析，作为最强的 LLM，ChatGPT 生成的代码错误率为52%。所以有理由认为  Code Llama 7B 生成的单元测试代码也会有错误，因此 self-instruct 不是一个完美的数据集。</p><h3 id="3-训练策略"><a href="#3-训练策略" class="headerlink" title="3. 训练策略"></a>3. 训练策略</h3><p>论文中针对不同的模型，尝试了不同的训练策略，整体来说是和数据集比较匹配的。</p><h4 id="3-1-从头训练-vs-Finetune"><a href="#3-1-从头训练-vs-Finetune" class="headerlink" title="3.1 从头训练 vs Finetune"></a>3.1 从头训练 vs Finetune</h4><p>论文中实验发现，采用通用llm (Llama 2)初始化，再在code数据集上finetune比在code数据集上从头训练效果要好。但同时也发现，只使用 Llama 2 模型来做代码生成，效果比 Llama 2 + Code 数据集训练要差，可见 2T token  pretrain + 500B token finetune才是做通用代码生成的最好选择。</p><h4 id="3-2-代码补全功能"><a href="#3-2-代码补全功能" class="headerlink" title="3.2 代码补全功能"></a>3.2 代码补全功能</h4><p>上面的基本训练策略中只会给定前面的代码，补全或预测后面的代码，但在有些常见，是已知前面和后面的代码，给出中间的代码，比如docstring的生成，就需要知道前面的内容(函数的名字和参数)和后面的内容（函数的具体实现），才能给出比较准确的函数说明docstring。这种任务模式论文中称之为补全 (Infilling)。</p><p>这种需求跟 LLM 预测下一个 token 的任务模式是不同的，因此需要对训练模式进行改造。总体来说，论文采用了 Casual Mask 的模式来训练网络，也就是将训练序列中间的一部分移动到最后，让网络来预测这部分内容。具体来说，将训练中的token分割为前缀、中间部分和后缀部分，分割位置利用均匀分布来确定。训练时以一半的概率喂前缀-后缀-中间（PSM）格式 token 序列，一半的概率喂后缀-前缀-中间（SPM）格式的 token 序列。</p><h4 id="3-3-长上下文输入微调"><a href="#3-3-长上下文输入微调" class="headerlink" title="3.3 长上下文输入微调"></a>3.3 长上下文输入微调</h4><p>Llama 2 模型的最长 token 数目为4096，对于代码生成任务来说，还是比较小，比如分析整个仓库中的代码，可能很容易超出限制。因此 Code Llama 在 finetune 阶段将 token 数从4096 提升到16384，提升了4倍。</p><p>位置embedding 采用旋转位置embedding, query 和 key vector都是 Rxn的一个线性组合，而R是一个块对角矩阵，也就是只有对角线和附近的4个值非零，每个位置i处的R公式如下：<br><img data-src="/imgs/code_llama_1/20230827002151.png"></p><p><img data-src="/imgs/code_llama_1/20230827002347.png"><br>d为总的token 维度。</p><h4 id="3-4-指令finetune"><a href="#3-4-指令finetune" class="headerlink" title="3.4 指令finetune"></a>3.4 指令finetune</h4><p>这部分也是为了生成更安全的代码，也更好地针对提问者的问题进行更人性化的回答。个人理解，这部分本身在策略上没有太多trick，核心是数据的构造和采集。</p><h3 id="4-模型说明"><a href="#4-模型说明" class="headerlink" title="4. 模型说明"></a>4. 模型说明</h3><p>基于几种数据和几个训练策略，就能得到不同的模型。<br>Code Llama 系列包含三大类模型，每类模型包含 7B, 13B 和 34B 三种参数大小，共9个模型。</p><p><img data-src="/imgs/code_llama_1/20230826222457.png"></p><p>第一类是Code Llama 通用代码生成模型，采用 Llama 2 的模型参数初始化，在 500B token 数据集上训练。其中 7B 和 13B 模型还进行了代码补全数据集上的训练，适用于 IDE 中实时的代码补全，而 34B 因为速度问题，并不适合实时补全，更适合作为编程助手。</p><p>第二类是 Code Llama-Python，这是针对 Python 专门优化的模型，在 500B 通用数据训练的基础上，又在额外的 100B Python 数据集上进行了finetune。</p><p>第三类是 Code. Llama-Instruct，在 Code Llama 通用模型基础上，增加了在 RLHF V5 和 self-instruct 数据集上的 finetune 过程，可以生成更符合指令需求的代码。</p><h3 id="5-结果对比"><a href="#5-结果对比" class="headerlink" title="5. 结果对比"></a>5. 结果对比</h3><p>论文中比较了非常多测试集上的指标，太多反而不知道模型的效果到底怎么样，所以这里也不列出来了。下面放的是博客文章中的和别的模型的对比表格，反而比较简洁，可以做一个大致的对比。</p><p><img data-src="/imgs/code_llama_1/20230826213734.png"></p><p>当然根据我之前的 LLMs 使用经验，实际使用时的智能感受貌似不能很好地和 Benchmark 上的结果对应起来，相差几个点对最终结果的提升有多大不太好说。 所以 Code Llama 具体使用体验如何，留待下一篇文章来分析。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Code Llama 是 Meta 基于 Llama 2 的代码生成AI模型， 在同类开源模型中取得比较好的结果。这里计划写3篇系列文章，从论文细节、代码使用、效果实测方面对 Code Llama 进行解读，欢迎关注我了解后续文章。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1. 背景&quot;&gt;&lt;/a&gt;1. 背景&lt;/h3&gt;&lt;p&gt;2023年8月24日，Meta 开源了基于 &lt;a href=&quot;https://github.com/facebookresearch/llama&quot;&gt;Llama 2&lt;/a&gt;) 通用 LLM 的代码生成系列模型 &lt;a href=&quot;https://github.com/facebookresearch/codellama&quot;&gt;Code Llama&lt;/a&gt;)，支持Python, C++, Java, PHP, TypeScript, C# 和 Bash 编程语言，而且支持学术研究和商业使用。&lt;/p&gt;
&lt;p&gt;另外 Code Llama 官方代码只提供了一些简单的使用示例，没有提供生产环境可用的 VSCode 等 工具的插件，搜索了一下也没找到简单易用的第三方开发的插件。相信很快就会有人做出来的。如果你有看到基于 Code Llama 的 VSCode 或者 Vim 插件，欢迎评论指教。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="Deep Learning" scheme="http://vra.github.io/tags/Deep-Learning/"/>
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="Code Llama" scheme="http://vra.github.io/tags/Code-Llama/"/>
    
  </entry>
  
  <entry>
    <title>FastViT 论文阅读</title>
    <link href="http://vra.github.io/2023/09/01/fastvit/"/>
    <id>http://vra.github.io/2023/09/01/fastvit/</id>
    <published>2023-09-01T07:56:52.000Z</published>
    <updated>2023-09-29T02:15:14.986Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p>论文地址：<a href="https://arxiv.org/abs/2303.14189">arxiv</a><br>代码地址：<a href="https://github.com/apple/ml-fastvit">ml-fastvit</a></p><p>FastViT 是苹果公司在 ICCV 2023上发表的网络结构设计的论文，在速度和精度上取得比较好的折衷，速度上既能和MobileOne这种轻量级网络匹敌，精度上也不输PoolFormer、ConvNeXt等比较新的大网络结构。</p><span id="more"></span><p><img data-src="/imgs/fastvit/20230901135142.png"></p><p>这是网络整体的结构图：<br><img data-src="/imgs/fastvit/20230901155048.png"><br>整体还是分成Stem和4个Stage，以及最后的输出Head。可以看到所有结构都在推理时进行了重参数化，保证只有一个网络分支。虽然叫ViT，但网络的核心还是由Conv层组成。</p><p>整个网络的的大部分模块是以MobileOne 的核心 MobileOneBlock 打底的，所以说是 MobileOne V2 也不为过。</p><p>比较有意思的是，FastVit 这篇论文的作者列表、作者顺序都和 MobileOne 一模一样！<br><img data-src="/imgs/fastvit/20230901154317.png"><br><img data-src="/imgs/fastvit/20230901154255.png"></p><p>所以可以说，FastViT 是 MobileOne 框架的延续，核心是在推理的时候保证只有一条网络分支，提升网络的推理速度。</p><p>具体来说，为了提升效果，网络设计上参考了比较新的 ConvMixer 结构。然后为了保证能够重参数化，将其中的非线性层省略掉，去掉残差模块。为了缓解 Self-Attention 模块计算量太大的问题，在浅层特征图比较大的情况下，采用 Large Kernel，也就是7x7 Kernel Size 的Conv网络。</p><p>下面依次对网络的几个核心模块进行说明。</p><h3 id="2-RepMixer"><a href="#2-RepMixer" class="headerlink" title="2. RepMixer"></a>2. RepMixer</h3><p>ConvMixer 提出了用Conv网络替代ViT网络的方法，在效果上超越了ViT方法。</p><p>已有的一些方法已经验证，Skip-Connection因为会有额外的内存访问开销，因此会显著增加网络延迟，如果能合并Skip-Connection，对于网络的加速会有很帮助。注意论文中的Skip-Connection其实指的是类似残差模块中的两个分支相加的操作（如下图），而不是更常见的Encoder和Decoder之间的跳层连接。<br><img data-src="/imgs/fastvit/20230901143032.png"></p><p>FastViT利用了 ConvMixer 网络结构优异的性能，同时为了能够在推理时进行重参数化，对 ConvMixer 进行了几个修改：</p><ol><li>去掉非线性层，否则没法进行重参数化</li><li>将BN放在DepthWiseConv之前</li><li>在推理时合并 Skip-Connection，用来加速推理。</li></ol><p>具体代码实现时，训练时采用了2个MobileOneBlock，分别表示mixer和normal，与原始输入x相加；推理的时候去掉残差相加，直接转换为一个MobileOne模块：<br><img data-src="/imgs/fastvit/20230901144427.png"></p><h3 id="3-训练时过参数化"><a href="#3-训练时过参数化" class="headerlink" title="3. 训练时过参数化"></a>3. 训练时过参数化</h3><p>过参数化是指训练的时候将结构相同的网络模块重复多遍，通过增加模型的复杂度来提点。在推理的时候，再通过重参数化trick将多个分支的结构合并到一个分支来提速。下面是过参数化的示意图（图片来自<a href="https://zhuanlan.zhihu.com/p/560894077">这里</a>):<br><img data-src="/imgs/fastvit/20230901150703.png"></p><p>MobileOne 论文中就采用了过参数模块，验证可以提高网络的学习能力。</p><p>在这篇论文中，为了提速，先是将普通的 KxK 的Conv修改为DepthWise KxK 的 Conv + 1x1 PointWise 的 Conv层，发现在提速后精度下降，例如论文中 Table 1 所示，这步修改后耗时从 1.58ms 下降到 1.26ms，但精度也从78.5下降到78.0:<br><img data-src="/imgs/fastvit/20230901151134.png"></p><p>为了弥补这一步造成的精度损失，作者叠加了上面提到的训练时重参数化的trick，保证速度不变的情况下，效果超过了之前的方法，从78.0上升到78.9。</p><p>当然这部分的结构优化其实比较”水”，是现有的两个工作的简单组合……</p><h3 id="4-Large-Kernel"><a href="#4-Large-Kernel" class="headerlink" title="4. Large Kernel"></a>4. Large Kernel</h3><p>由于Transformer结构的核心模块是Self-Attention模块，而且已经被无数实验验证具有强大的特征提取能力。<br>但Self-Attention的计算量很大，要做到手机上实时难度不小。</p><p>作者认为，Self-Attention 效果好跟它有很大的感受野有关系。而普通 Conv 层通过增加 Kernel. Size，也能达到提高感受野的效果。</p><p>因此最终网络结构设计上，在每个Stage开始的时候，采用 7x7 的 MobileOneBlock。7x7 的 Kernel Size 也是通过实验试出来的。</p><p>为了既能跟MobileOne这种轻量级网络对比，又能在 ImageNet 上和别的模型一较高下，论文中提出了7个 Fast-ViT的变种，各个变种的设置如下：<br><img data-src="/imgs/fastvit/20230901153635.png"></p><h3 id="5-实验"><a href="#5-实验" class="headerlink" title="5. 实验"></a>5. 实验</h3><p>对比实验在 ImageNet-1K 分类任务、COCO 物体检测，ADE20K 语义分割等标准任务上进行了对比</p><p><img data-src="/imgs/fastvit/20230901153843.png"><br><img data-src="/imgs/fastvit/20230901153923.png"><br><img data-src="/imgs/fastvit/20230901153936.png"><br>另外这篇论文还比较了FastVit在3D手重建这个下游任务上的效果，也是比MobRecon这些端侧实时的方法效果更好，当然还是刷不过MeshGraphormer等基于HRNet Backbone的模型。<br><img data-src="/imgs/fastvit/20230901135311.png"></p><h3 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h3><p>整个论文是比较实用的，没有太多自己的原创性的点子，更多的是将一些现有的网络结构设计思想融合进MobileOne的推理时单分支的网络结构中来。</p><p>另外一个值得注意的事情是，论文中给出的Mobile Latency都很低，像 FastVit-MA36 7.9G 的FLOPS，移动端延迟4.5毫秒。但要明白这是用iPhone 12 Pro Max上使用CoreML来测试的，本身iPhone 12 Pro Max 采用的A14芯片很强，而且CoreML针对苹果的硬件有专门的优化，所以在安卓机器或者低端一些的iPhone 上，采用别的推理引擎（如ONNX， MNN， TCNN）进行推理时，很有可能达不到这么高的速度，所以像 FastVit-MA36这种FLOPS 约为8G的模型在手机上用起来还是需要验证的。</p><p>总之对于想试用 FastViT 的小伙伴来说，用就完了，代码已经开源，也不存在复现的问题，直接用起来，好用就加入到自己的任务中，效果比较差或者速度有瓶颈抛弃即可。</p><p>另外 FastViT 的代码实现很简洁优雅，阅读起来很舒服，后面有空可以写一篇代码阅读的文章，欢迎感兴趣的小伙伴关注、点赞和评论区留言～</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h3&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/2303.14189&quot;&gt;arxiv&lt;/a&gt;&lt;br&gt;代码地址：&lt;a href=&quot;https://github.com/apple/ml-fastvit&quot;&gt;ml-fastvit&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;FastViT 是苹果公司在 ICCV 2023上发表的网络结构设计的论文，在速度和精度上取得比较好的折衷，速度上既能和MobileOne这种轻量级网络匹敌，精度上也不输PoolFormer、ConvNeXt等比较新的大网络结构。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Deep Learning" scheme="http://vra.github.io/tags/Deep-Learning/"/>
    
    <category term="Pytorch" scheme="http://vra.github.io/tags/Pytorch/"/>
    
    <category term="论文阅读" scheme="http://vra.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>一个简单好用的Python并行函数</title>
    <link href="http://vra.github.io/2023/08/12/python-parallel-function/"/>
    <id>http://vra.github.io/2023/08/12/python-parallel-function/</id>
    <published>2023-08-12T15:43:21.000Z</published>
    <updated>2023-08-12T15:44:23.350Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h3><p>用Python跑有大量数据的任务的时候，启用多进程加速效果明显。但因为我之前在使用Python的多进程库时总遇到卡住的问题，后来对这块避而远之，总是用别的方法来加速。最近发现OpenMMLab的一些库提供了多进程并行的函数功能，简单好用。比如一个简单的toy例子，OpenCV读图像，resize然后保存，在8个CPU核的 Mac 上，加速比能达到3.4倍(45ms vs 13ms)，也就是以前要跑3个多小时的任务，现在1个小时就能搞定，省了不少时间，更多实际例子也证明了这个函数的加速效果，还是挺实用的。这里写个教程，希望也能方便到别的有同样需要的人，当然同类型的库应该也有很多，这里只是取一瓢饮。</p><span id="more"></span><h3 id="2-函数实现"><a href="#2-函数实现" class="headerlink" title="2. 函数实现"></a>2. 函数实现</h3><p>具体实现是<a href="https://github.com/open-mmlab/mmengine">mmengine</a>中的<a href="https://github.com/open-mmlab/mmengine/blob/main/mmengine/utils/progressbar.py#L109">track_parallel_progress</a>函数，它底层也是调用了Python系统库的<a href="https://docs.python.org/3/library/multiprocessing.html">multiprocessing</a>，进行多进程加速脚本的运行。所以原理上来说我们也可以不用这个函数，自己写multiprocessing调用代码。但mmengine的这个封装，给我们省去了写multiprocessing比较复杂的调度代码的时间，拿来直接用还是能加速代码的开发节奏。</p><p>大致的调用框架:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="keyword">import</span> mmengine</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mmengine_track_func</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="comment"># wraps的作用是将装饰器的信息都传递给被装饰的函数，</span></span><br><span class="line">    <span class="comment"># 参考：https://stackoverflow.com/a/309000</span></span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapped_func</span>(<span class="params">args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> func(*args)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapped_func</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@mmengine_track_func</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">your_func</span>(<span class="params">arg1, arg2</span>):</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进程数</span></span><br><span class="line">NUM_PROC = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造调用参数</span></span><br><span class="line">params = [(arg1, arg2) <span class="keyword">for</span> arg1, arg2 <span class="keyword">in</span> <span class="built_in">zip</span>(arg1_list, arg2_list)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用mmengine封装好的多进程函数</span></span><br><span class="line">results = mmengine.track_parallel_progress(your_func, params, nproc=NUM_PROC)</span><br></pre></td></tr></table></figure><p>使用时需要先 <code>pip install mmengine</code>来安装依赖库 mmengine。</p><p>然后这里构造了一个装饰器<code>mmengine_track_func</code>，对实际调用的函数<code>your_func</code>进行封装。其中用到了functools中的wraps函数，它的作用是将装饰器的信息都传递给被装饰的函数，具体例子可以参考这个<a href="https://stackoverflow.com/a/309000">回答</a>。</p><p>实际使用时<code>mmengine_track_func</code> 不需要修改，直接采用这种形式。</p><p>然后是设置进程数，构造你自己函数的参数，再调用<code>mmengine.track_parallel_progress</code> 即可，它的必需的三个参数分别是:</p><ol><li>你的函数名</li><li>函数参数list</li><li>设置的进程数</li></ol><p>别的非必需参数可以参考<a href="https://github.com/open-mmlab/mmengine/blob/main/mmengine/utils/progressbar.py#L109">源码</a>。</p><h3 id="3-toy-例子"><a href="#3-toy-例子" class="headerlink" title="3. toy 例子"></a>3. toy 例子</h3><p>这里举一个简单的伪造例子，读取本地某个目录下的png图像，将它们都缩放到200x200，再保存到本地。完整代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> mmengine</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mmengine_track_func</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="comment"># wraps的作用是将装饰器的信息都传递给被装饰的函数，</span></span><br><span class="line">    <span class="comment"># 参考：https://stackoverflow.com/a/309000</span></span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapped_func</span>(<span class="params">args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> func(*args)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapped_func</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@mmengine_track_func</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">idx, img_path</span>):</span></span><br><span class="line">    img = cv2.imread(img_path)</span><br><span class="line">    img = cv2.resize(img, (<span class="number">200</span>, <span class="number">200</span>))</span><br><span class="line"></span><br><span class="line">    op = <span class="string">f&quot;<span class="subst">&#123;idx&#125;</span>.jpg&quot;</span></span><br><span class="line">    cv2.imwrite(op, img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 获取所有图片路径</span></span><br><span class="line">    img_paths = glob.glob(<span class="string">&quot;/path/to/folder/*.png&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试开多线程版本，耗时 13ms</span></span><br><span class="line">    params = [(idx, img_path) <span class="keyword">for</span> idx, img_path <span class="keyword">in</span> <span class="built_in">enumerate</span>(img_paths)]</span><br><span class="line">    mmengine.track_parallel_progress(run, params, nproc=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试不开多线程版本，耗时45ms</span></span><br><span class="line">    t0 = time.time()</span><br><span class="line">    <span class="keyword">for</span> idx, ip <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(img_paths)):</span><br><span class="line">        run.__wrapped__(idx, ip)</span><br><span class="line">    t1 = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;time:&quot;</span>, t1 - t0)</span><br></pre></td></tr></table></figure><p>这里有一个小的Python知识点：可以通过<code>func.__wrapped__</code> 属性来获取 <em>被装饰的函数</em> 对应的原始函数。</p><p>输出结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;] 4000/4000, 316.3 task/s, elapsed: 13s, ETA:     0s</span><br><span class="line">4000it [00:45, 88.84it/s]</span><br><span class="line">time: 45.0268120765686</span><br></pre></td></tr></table></figure><p>可以看到耗时从45ms下降到13ms，加速比3.4倍。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1. 背景&quot;&gt;&lt;/a&gt;1. 背景&lt;/h3&gt;&lt;p&gt;用Python跑有大量数据的任务的时候，启用多进程加速效果明显。但因为我之前在使用Python的多进程库时总遇到卡住的问题，后来对这块避而远之，总是用别的方法来加速。最近发现OpenMMLab的一些库提供了多进程并行的函数功能，简单好用。比如一个简单的toy例子，OpenCV读图像，resize然后保存，在8个CPU核的 Mac 上，加速比能达到3.4倍(45ms vs 13ms)，也就是以前要跑3个多小时的任务，现在1个小时就能搞定，省了不少时间，更多实际例子也证明了这个函数的加速效果，还是挺实用的。这里写个教程，希望也能方便到别的有同样需要的人，当然同类型的库应该也有很多，这里只是取一瓢饮。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="mmengine" scheme="http://vra.github.io/tags/mmengine/"/>
    
  </entry>
  
  <entry>
    <title>git clean 教程</title>
    <link href="http://vra.github.io/2023/07/30/git-clean-tutorial/"/>
    <id>http://vra.github.io/2023/07/30/git-clean-tutorial/</id>
    <published>2023-07-29T16:21:01.000Z</published>
    <updated>2023-08-09T07:08:19.177Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-引入"><a href="#1-引入" class="headerlink" title="1. 引入"></a>1. 引入</h3><p>git clean 是用来删除 git 仓库中没有被跟踪的文件的命令，在想要快速清理 git 仓库（比如，删除仓库中所有没有跟踪的文件，清除编译生成的临时文件）时很有用。是相比别的git子命令， git clean的配置选项比较少，使用起来简单一些，这里写一个简要教程。<br>友情提示：git clean真的会删除文件，而且没法用git命令来恢复（因为没有被 git 跟踪），所以使用git clean前务必慎重，建议每次删除文件之前先加<code>--dry-run</code> 选项来验证会删除哪些文件，确保没有误删。</p><span id="more"></span><h3 id="2-git-clean-选项的含义"><a href="#2-git-clean-选项的含义" class="headerlink" title="2. git clean 选项的含义"></a>2. git clean 选项的含义</h3><p>先创建一个简单的git 仓库环境来比较清晰地展示各个选项的效果:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">mkdir /tmp/git_clean_demo</span><br><span class="line"><span class="built_in">cd</span> /tmp/git_clean_demo</span><br><span class="line">git init</span><br><span class="line">touch a.py b.py</span><br><span class="line">git add a.py</span><br><span class="line">mkdir -p folder0/folder00</span><br><span class="line">mkdir -p folder0/folder01</span><br><span class="line">touch folder0/folder0.py</span><br><span class="line">touch folder0/folder00/folder00.py</span><br><span class="line">touch folder0/folder01/folder01.py</span><br><span class="line">git add folder0/folder0.py</span><br><span class="line">git add folder0/folder00/</span><br><span class="line">touch folder0/folder00/folder00_v2.py</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;*.pyc&quot;</span> &gt;&gt; .gitignore</span><br><span class="line">touch a.pyc</span><br><span class="line">git add .gitignore</span><br></pre></td></tr></table></figure><p>用<code>git status</code> 查看一下文件跟踪状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">On branch main</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use <span class="string">&quot;git rm --cached &lt;file&gt;...&quot;</span> to unstage)</span><br><span class="line">        new file:   a.py</span><br><span class="line">        new file:   folder0/folder0.py</span><br><span class="line">        new file:   folder0/folder00/folder00.py</span><br><span class="line"></span><br><span class="line">Untracked files:</span><br><span class="line">  (use <span class="string">&quot;git add &lt;file&gt;...&quot;</span> to include <span class="keyword">in</span> what will be committed)</span><br><span class="line">        b.py</span><br><span class="line">        folder0/folder00/folder00_v2.py</span><br><span class="line">        folder0/folder01/</span><br></pre></td></tr></table></figure><p>在 Git 的800多个配置选项中，只有一项是关于<code>git clean</code> 命令的：<code>clean.requireForce</code>。这个选项的意思是，使用<code>git clean</code> 时，必须加<code>-f</code>或者<code>--force</code> 参数才能删除文件，否则并不会删除文件，执行时会提示下面信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git clean</span><br><span class="line">fatal: clean.requireForce defaults to <span class="literal">true</span> and neither -i, -n, nor -f given; refusing to clean</span><br></pre></td></tr></table></figure><p>这是一个很好的保护文件不被轻易删除的选项，建议不要修改默认值。</p><p>所以 <code>-f/--force</code>的选项的含义就是强制删除，实际删除文件时必带此选项。</p><p><code>-n/--dry-run</code>表示不实际删除任何东西，只是空跑一下，用来看哪些文件会被删除掉。对于这种破坏性的命令，增加<code>--dry-run</code>选项真的是一个非常好的设定。</p><p>另一个很重要的选项是<code>-d</code>，表示进入<strong>未跟踪</strong>的目录来递归删除文件。注意对已经跟踪的目录，不加<code>-d</code> 命令也会清理其中的未跟踪文件，一定注意！<br>比如刚才创建的git仓库，不加<code>-d</code> 选项删除时结果如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f --dry-run</span><br><span class="line">Would remove b.py</span><br><span class="line">Would remove folder0/folder00/folder00_v2.py</span><br></pre></td></tr></table></figure><p>加了 <code>-d</code>选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d --dry-run </span><br><span class="line">Would remove b.py</span><br><span class="line">Would remove folder0/folder00/folder00_v2.py</span><br><span class="line">Would remove folder0/folder01/</span><br></pre></td></tr></table></figure><p>可以看到不管加不加<code>-d</code>，已经跟踪的目录下的未跟踪文件都会被删除；而只有加了<code>-d</code>，未跟踪的目录和下面的文件才会被删除。</p><p><code>-q/--quiet</code>表示静默操作，除了错误，别的信息不显示，实际效果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d -q --dry-run</span><br></pre></td></tr></table></figure><p>可以看到没有任何输出。</p><p><code>-i/--interactive</code> 表示交互式地删除文件，用于对文件删除进行精细操作。进入交互式界面后，又可以分按模式删除、按数字删除、每次删除前询问几种方式，具体看下面的交互式会话：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d -i --dry-run</span><br><span class="line">Would remove the following items:</span><br><span class="line">  b.py                             folder0/folder00/folder00_v2.py  folder0/folder01/</span><br><span class="line">*** Commands ***</span><br><span class="line">    1: clean                2: filter by pattern    3: select by numbers    4: ask each             5: quit                 6: <span class="built_in">help</span></span><br><span class="line">What now&gt; h</span><br><span class="line">clean               - start cleaning</span><br><span class="line">filter by pattern   - exclude items from deletion</span><br><span class="line">select by numbers   - select items to be deleted by numbers</span><br><span class="line">ask each            - confirm each deletion (like <span class="string">&quot;rm -i&quot;</span>)</span><br><span class="line">quit                - stop cleaning</span><br><span class="line"><span class="built_in">help</span>                - this screen</span><br><span class="line">?                   - <span class="built_in">help</span> <span class="keyword">for</span> prompt selection</span><br><span class="line">Would remove the following items:</span><br><span class="line">  b.py                             folder0/folder00/folder00_v2.py  folder0/folder01/</span><br><span class="line">*** Commands ***</span><br><span class="line">    1: clean                2: filter by pattern    3: select by numbers    4: ask each             5: quit                 6: <span class="built_in">help</span></span><br><span class="line">What now&gt; c</span><br><span class="line">Would remove b.py</span><br><span class="line">Would remove folder0/folder00/folder00_v2.py</span><br><span class="line">Would remove folder0/folder01/</span><br></pre></td></tr></table></figure><p>按规则忽略文件，也就是匹配到规则的图片不进行删除：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d -i --dry-run</span><br><span class="line">Would remove the following items:</span><br><span class="line">  b.py                             folder0/folder00/folder00_v2.py  folder0/folder01/</span><br><span class="line">*** Commands ***</span><br><span class="line">    1: clean                2: filter by pattern    3: select by numbers    4: ask each             5: quit                 6: <span class="built_in">help</span></span><br><span class="line">What now&gt; f</span><br><span class="line">  b.py                             folder0/folder00/folder00_v2.py  folder0/folder01/</span><br><span class="line">Input ignore patterns&gt;&gt; *folder*</span><br><span class="line">  b.py</span><br><span class="line">Input ignore patterns&gt;&gt;</span><br><span class="line">Would remove the following item:</span><br><span class="line">  b.py</span><br><span class="line">*** Commands ***</span><br><span class="line">    1: clean                2: filter by pattern    3: select by numbers    4: ask each             5: quit                 6: <span class="built_in">help</span></span><br><span class="line">What now&gt; c</span><br><span class="line">Would remove b.py</span><br></pre></td></tr></table></figure><p>按数字删除：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d -i --dry-run</span><br><span class="line">Would remove the following items:</span><br><span class="line">  b.py                             folder0/folder00/folder00_v2.py  folder0/folder01/</span><br><span class="line">*** Commands ***</span><br><span class="line">    1: clean                2: filter by pattern    3: select by numbers    4: ask each             5: quit                 6: <span class="built_in">help</span></span><br><span class="line">What now&gt; s</span><br><span class="line">    1: b.py                               2: folder0/folder00/folder00_v2.py    3: folder0/folder01/</span><br><span class="line">Select items to delete&gt;&gt; 1</span><br><span class="line">  * 1: b.py                               2: folder0/folder00/folder00_v2.py    3: folder0/folder01/</span><br><span class="line">Select items to delete&gt;&gt; 2</span><br><span class="line">  * 1: b.py                             * 2: folder0/folder00/folder00_v2.py    3: folder0/folder01/</span><br><span class="line">Select items to delete&gt;&gt; 3</span><br><span class="line">  * 1: b.py                             * 2: folder0/folder00/folder00_v2.py  * 3: folder0/folder01/</span><br><span class="line">Select items to delete&gt;&gt; 4</span><br><span class="line">Huh (4)?</span><br><span class="line">  * 1: b.py                             * 2: folder0/folder00/folder00_v2.py  * 3: folder0/folder01/</span><br><span class="line">Select items to delete&gt;&gt; 5</span><br><span class="line">Huh (5)?</span><br><span class="line">  * 1: b.py                             * 2: folder0/folder00/folder00_v2.py  * 3: folder0/folder01/</span><br><span class="line">Select items to delete&gt;&gt;</span><br><span class="line">Would remove the following items:</span><br><span class="line">  b.py                             folder0/folder00/folder00_v2.py  folder0/folder01/</span><br><span class="line">*** Commands ***</span><br><span class="line">    1: clean                2: filter by pattern    3: select by numbers    4: ask each             5: quit                 6: <span class="built_in">help</span></span><br><span class="line">What now&gt; c</span><br><span class="line">Would remove b.py</span><br><span class="line">Would remove folder0/folder00/folder00_v2.py</span><br><span class="line">Would remove folder0/folder01/</span><br></pre></td></tr></table></figure><p>注意看输入大于未跟踪文件数目的数字时的<code>Huh</code>，有点喜感。</p><p><code>-e/--exclude</code>表示删除时排除满足后面模式的文件，比如<code>-e &quot;*/&quot;</code> 表示排除所有文件夹，<code>-e &quot;*_v2.py&quot;</code>表示排除所有以<code>_v2.py</code>结尾的文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d -e <span class="string">&quot;*/&quot;</span> --dry-run</span><br><span class="line">Would remove b.py</span><br><span class="line"></span><br><span class="line">$ it clean -f -d -e <span class="string">&quot;*_v2.py&quot;</span> --dry-run</span><br><span class="line">Would remove b.py</span><br><span class="line">Would remove folder0/folder01</span><br></pre></td></tr></table></figure><p><code>-x</code>  表示不使用.gitignore中的规则。如果不加这个选项，默认会跳过.gitignore 规则中的文件，启用这个选项后会将<code>.gitignore</code> 中的文件也删除，比如创建示例仓库时我们忽略了<code>*.pyc</code>，前面的结果中都没跳过了这一类文件，加了<code>-x</code>选项后输出如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d -x --dry-run</span><br><span class="line">Would remove a.pyc</span><br><span class="line">Would remove b.py</span><br><span class="line">Would remove folder0/folder00/folder00_v2.py</span><br><span class="line">Would remove folder0/folder01/</span><br></pre></td></tr></table></figure><p><code>a.pyc</code> 也被删除掉了。</p><p><code>-X</code>选项（大写的X）与<code>-x</code> 相反，只删除满足<code>.gitignore</code> 中规则的文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d -X --dry-run</span><br><span class="line">Would remove a.pyc</span><br></pre></td></tr></table></figure><h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><p>实际实践中，我对<code>git clean</code> 用的还不多(严格来说正经使用只用过一次)，本文中如果错误，欢迎批评指正。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-引入&quot;&gt;&lt;a href=&quot;#1-引入&quot; class=&quot;headerlink&quot; title=&quot;1. 引入&quot;&gt;&lt;/a&gt;1. 引入&lt;/h3&gt;&lt;p&gt;git clean 是用来删除 git 仓库中没有被跟踪的文件的命令，在想要快速清理 git 仓库（比如，删除仓库中所有没有跟踪的文件，清除编译生成的临时文件）时很有用。是相比别的git子命令， git clean的配置选项比较少，使用起来简单一些，这里写一个简要教程。&lt;br&gt;友情提示：git clean真的会删除文件，而且没法用git命令来恢复（因为没有被 git 跟踪），所以使用git clean前务必慎重，建议每次删除文件之前先加&lt;code&gt;--dry-run&lt;/code&gt; 选项来验证会删除哪些文件，确保没有误删。&lt;/p&gt;</summary>
    
    
    
    
    <category term="总结" scheme="http://vra.github.io/tags/%E6%80%BB%E7%BB%93/"/>
    
    <category term="Git" scheme="http://vra.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>dinov2_retrieval:一个基于DINOv2 的图片检索应用</title>
    <link href="http://vra.github.io/2023/07/14/dinov2-retrieval/"/>
    <id>http://vra.github.io/2023/07/14/dinov2-retrieval/</id>
    <published>2023-07-13T16:05:35.000Z</published>
    <updated>2023-07-13T16:16:35.363Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p>前些天 Meta 公司发布了 <a href="https://github.com/facebookresearch/dinov2">DINOv2</a> 视觉预训练模型。DINOv2 能够高效地提出图像中的特征，提取的特征可以直接用于像分类等任务，而且只需要一个简单的线性层就能取得比较好的结果。</p><p>为了展示 DINOv2 强大的特征提取能力， Meta 提供了一个在线 <a href="https://dinov2.metademolab.com/">Demo</a>，上传一张图片，就能从一些艺术画作中检索出最相似的作品。</p><p>拿随手拍的照片体验后，DINOv2 特征提取能力确实强大，能够准确地理解图片中的语义信息。</p><p><img data-src="/imgs/dinov2_retrieval/dinov2_demo_result.jpeg"></p><p>由于 DINOv2 预训练模型是开源的，因为基于它来测试实际的效果是可行的。比如，我想找到相册中跟某张照片最相似的图片，就可以用 DINOv2 来测试照片和相册中所有照片的特征，然后计算相册中照片特征与测试照片最相近的那一张，就是我想要的。</p><p>整体思路是很简单直接的，经过一天的开发，终于完成了一个相对完善的Python工具 <a href="https://github.com/vra/dinov2-retrieval">dinov2_retrieval</a>，能够检索若干张图片在测试数据集中最相似的图。</p><p>写完后拿最近拍的一些随机照片跑了一下，检索结果还是挺不错的。最左边是测试图片，右边的5张图是在[Caltech 256](<a href="https://data.caltech.edu/records/nyy15-4j048">Caltech 256</a>)数据集中检索得到的top5相似的图像：<br><img data-src="/imgs/dinov2_retrieval/1688175364717_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688175364731_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688175364741_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688175364753_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688175364766_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688175364775_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688175364786_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688175364801_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688219476149_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688219476156_output.jpg"></p><p>通过和ResNet50预训练模型提取的特征做检索对比，发现 DINOv2 提取的特征还是更准确一些，检索结果也更好。</p><p>后面部分详细说说这个工具 dinov2_retrieval 的使用。</p><span id="more"></span><h3 id="2-安装和使用"><a href="#2-安装和使用" class="headerlink" title="2. 安装和使用"></a>2. 安装和使用</h3><p>dinov2_retrieval 已经发布到 PyPI，因此可以使用pip来直接安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install dinov2_retrieval</span><br></pre></td></tr></table></figure><p>安装后在命令行执行<code>dinov2_retrieval -h</code> 来检查安装是否成功：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">dinov2_retrieval -h</span><br><span class="line">usage: dinov2_retrieval [-h] [-s &#123;small,base,large,largest&#125;] [-p MODEL_PATH] [-o OUTPUT_ROOT] -q QUERY -d DATABASE [-n NUM] [--size SIZE]</span><br><span class="line">                        [-m &#123;0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100&#125;] [--disable-cache] [-v]</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --<span class="built_in">help</span>            show this <span class="built_in">help</span> message and <span class="built_in">exit</span></span><br><span class="line">  -s &#123;small,base,large,largest&#125;, --model-size &#123;small,base,large,largest&#125;</span><br><span class="line">                        DinoV2 model <span class="built_in">type</span></span><br><span class="line">  -p MODEL_PATH, --model-path MODEL_PATH</span><br><span class="line">                        path to dinov2 model, useful when github is unavailable</span><br><span class="line">  -o OUTPUT_ROOT, --output-root OUTPUT_ROOT</span><br><span class="line">                        root folder to save output results</span><br><span class="line">  -q QUERY, --query QUERY</span><br><span class="line">                        path to a query image file or image folder</span><br><span class="line">  -d DATABASE, --database DATABASE</span><br><span class="line">                        path to the database image file or image folder</span><br><span class="line">  -n NUM, --num NUM     How many images to show <span class="keyword">in</span> retrieval results</span><br><span class="line">  --size SIZE           image output size</span><br><span class="line">  -m &#123;0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100&#125;, --margin &#123;0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100&#125;</span><br><span class="line">                        margin size (<span class="keyword">in</span> pixel) between concatenated images</span><br><span class="line">  --disable-cache       don<span class="string">&#x27;t cache database features, will extract features each time, quite time-consuming for large database</span></span><br><span class="line"><span class="string">  -v, --verbose         show detailed logs</span></span><br></pre></td></tr></table></figure><p>如果有上面的输出说明就安装成功了，否则就有问题，解决不了的情况下可以在<a href="https://github.com/vra/dinov2-retrieval/issues">这里</a>提交issue。</p><p>运行时一般来说只需要设置一下<code>--query</code> 和<code>--database</code> 参数，分别代表测试图像和数据集的地址。两者都可以是单张图片或者目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dinov2_retrieval -q /path/to/query/image -d /path/to/database/images</span><br></pre></td></tr></table></figure><p>检索得到的结果会保存在<code>output</code>目录下。</p><p>另外的选项含义如下：</p><ul><li>-s/–model-size: 模型大小，可以设置small，base，large或者largest</li><li>-p/–model_path: 模型缓存路径，一般是<code>$HOME/.cache/torch/hub/facebookresearch_dinov2_main</code>，对于GitHub连接不太稳定的情况使用此选项可以从本地读取模型</li><li>-o/–output-root: 输出结果的保存目录，默认是<code>output</code></li><li>-n/–num: 显示多少张最相似的图片，默认是1张</li><li>–size: 图像缩放到多大来显示，默认是224</li><li>-m/–margin: 不同图像拼接时的间距，默认10像素</li><li>–disable-cache: 禁用database特征的cache，开启后每次运行都会对database所有图像提取一遍特征，耗时大大增加</li><li>-v/–verbose: 开启debug log，会显示更多有用信息，比如图像的相似度等</li></ul><h2 id="3-思考"><a href="#3-思考" class="headerlink" title="3. 思考"></a>3. 思考</h2><p>写完这个工具后，有一点体会，检索这个任务要做出有意思的东西，还是要有足够丰富有趣的数据库。这也是一个通用的问题，现在的AI有强大的能力，但对于普通开发者来说，AI的能力用到哪里，怎么产生出有意思有意义的实际应用场景，是个值得思考的问题。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h3&gt;&lt;p&gt;前些天 Meta 公司发布了 &lt;a href=&quot;https://github.com/facebookresearch/dinov2&quot;&gt;DINOv2&lt;/a&gt; 视觉预训练模型。DINOv2 能够高效地提出图像中的特征，提取的特征可以直接用于像分类等任务，而且只需要一个简单的线性层就能取得比较好的结果。&lt;/p&gt;
&lt;p&gt;为了展示 DINOv2 强大的特征提取能力， Meta 提供了一个在线 &lt;a href=&quot;https://dinov2.metademolab.com/&quot;&gt;Demo&lt;/a&gt;，上传一张图片，就能从一些艺术画作中检索出最相似的作品。&lt;/p&gt;
&lt;p&gt;拿随手拍的照片体验后，DINOv2 特征提取能力确实强大，能够准确地理解图片中的语义信息。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/dinov2_demo_result.jpeg&quot;&gt;&lt;/p&gt;
&lt;p&gt;由于 DINOv2 预训练模型是开源的，因为基于它来测试实际的效果是可行的。比如，我想找到相册中跟某张照片最相似的图片，就可以用 DINOv2 来测试照片和相册中所有照片的特征，然后计算相册中照片特征与测试照片最相近的那一张，就是我想要的。&lt;/p&gt;
&lt;p&gt;整体思路是很简单直接的，经过一天的开发，终于完成了一个相对完善的Python工具 &lt;a href=&quot;https://github.com/vra/dinov2-retrieval&quot;&gt;dinov2_retrieval&lt;/a&gt;，能够检索若干张图片在测试数据集中最相似的图。&lt;/p&gt;
&lt;p&gt;写完后拿最近拍的一些随机照片跑了一下，检索结果还是挺不错的。最左边是测试图片，右边的5张图是在[Caltech 256](&lt;a href=&quot;https://data.caltech.edu/records/nyy15-4j048&quot;&gt;Caltech 256&lt;/a&gt;)数据集中检索得到的top5相似的图像：&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364717_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364731_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364741_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364753_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364766_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364775_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364786_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364801_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688219476149_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688219476156_output.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;通过和ResNet50预训练模型提取的特征做检索对比，发现 DINOv2 提取的特征还是更准确一些，检索结果也更好。&lt;/p&gt;
&lt;p&gt;后面部分详细说说这个工具 dinov2_retrieval 的使用。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="pip" scheme="http://vra.github.io/tags/pip/"/>
    
    <category term="Pytorch" scheme="http://vra.github.io/tags/Pytorch/"/>
    
  </entry>
  
  <entry>
    <title>git diff 的一个妙用</title>
    <link href="http://vra.github.io/2023/06/30/git-diff-a-special-use-case/"/>
    <id>http://vra.github.io/2023/06/30/git-diff-a-special-use-case/</id>
    <published>2023-06-30T14:41:49.000Z</published>
    <updated>2023-06-30T14:42:42.742Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-git-diff-常规用法"><a href="#1-git-diff-常规用法" class="headerlink" title="1. git diff 常规用法"></a>1. git diff 常规用法</h3><p>git diff 可以用来比较在git仓库中的两次提交或两个文件的diff，常见用法如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示当前代码与最新commit的代码之间的差别</span></span><br><span class="line">git diff</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示暂存（也就是已经git add 但还没有git commit）的代码提交</span></span><br><span class="line">git diff --staged</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示当前代码与&lt;commit-id&gt;时代码的区别</span></span><br><span class="line">git diff &lt;commit-id&gt; </span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示暂存代码与&lt;commit-id&gt;时代码的区别</span></span><br><span class="line">git diff --staged &lt;commit-id&gt; </span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示两次commit-id之间的代码区别</span></span><br><span class="line">git diff &lt;commit-id1&gt; &lt;commit-id2&gt;  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示当前分支与 branch1 分支上的代码区别</span></span><br><span class="line">git diff &lt;branch1&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示两个分支上的代码之间的区别</span></span><br><span class="line">git diff &lt;branch1&gt; &lt;branch2&gt;</span><br></pre></td></tr></table></figure><p>所有上述命令后面都可以加一个目录或文件路径来只显示这个目录或文件中的区别：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git diff /path/to/folder</span><br><span class="line"></span><br><span class="line">git diff /path/to/file.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可用git的参数终止符号--，避免文件名和参数重名时将文件名解析为参数</span></span><br><span class="line">git diff --  /path/to/file.py</span><br></pre></td></tr></table></figure><span id="more"></span><h3 id="2-git-diff-妙用"><a href="#2-git-diff-妙用" class="headerlink" title="2. git diff 妙用"></a>2. git diff 妙用</h3><p>git diff 有一个选项<code>--no-index</code> ，可以用来不在git仓库中的两个文件或目录。<br><code>--no-index</code>的git帮助文档中说明如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git diff [&lt;options&gt;] --no-index [--] &lt;path&gt; &lt;path&gt;</span><br><span class="line">This form is to compare the given two paths on the filesystem. You can omit the --no-index option when running the command in a working tree controlled by Git and at least one of the paths points outside the working tree, or when running the command outside a working tree controlled by Git. This form implies --exit-code.</span><br></pre></td></tr></table></figure><p>说明它可以用来比较两个给定的路径。</p><p>那为什么要用<code>git diff</code> 来比较非git仓库里面的两个路径呢，直接用Linux和Mac上自带的<code>diff</code> 命令不好吗？</p><p><code>git diff</code> 相比<code>diff</code> 的优势是它能生成以<code>+</code> 和<code>-</code> 开头的diff结果，红色表示删去，绿色表示添加，因此能很直观地看出增加和删除了哪些地方，而diff给出来的是黑色的代码差别，展示很不直观。</p><p>另外<code>git diff</code>的结果可以写入文件，粘贴到Markdown文件中，大部分 Markdown 渲染器都能够识别diff块，比较好地渲染出diff结果。</p><p>实际操作中，需要在一个git仓库目录中来执行<code>git diff --no-index</code>,例如比较两个文件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff --no-index ~/a.py ~/b.py</span><br></pre></td></tr></table></figure><p>比较两个目录:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff --no-index ~/folder-a ~/folder-b</span><br></pre></td></tr></table></figure><h3 id="One-More-Thing"><a href="#One-More-Thing" class="headerlink" title="One More Thing"></a>One More Thing</h3><p>其实我之前写过一个比较两个目录的Python工具<a href="https://github.com/vra/dompare">dompare</a>(名字含义是directory compare)，通过执行一条命令得到得到两个目录中文件的diff，并且保存到HTML网页中打开浏览器进行展示。感兴趣的小伙伴可以玩一玩。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-git-diff-常规用法&quot;&gt;&lt;a href=&quot;#1-git-diff-常规用法&quot; class=&quot;headerlink&quot; title=&quot;1. git diff 常规用法&quot;&gt;&lt;/a&gt;1. git diff 常规用法&lt;/h3&gt;&lt;p&gt;git diff 可以用来比较在git仓库中的两次提交或两个文件的diff，常见用法如下：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 显示当前代码与最新commit的代码之间的差别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 显示暂存（也就是已经git add 但还没有git commit）的代码提交&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff --staged&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 显示当前代码与&amp;lt;commit-id&amp;gt;时代码的区别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff &amp;lt;commit-id&amp;gt; &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 显示暂存代码与&amp;lt;commit-id&amp;gt;时代码的区别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff --staged &amp;lt;commit-id&amp;gt; &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 显示两次commit-id之间的代码区别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff &amp;lt;commit-id1&amp;gt; &amp;lt;commit-id2&amp;gt;  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 显示当前分支与 branch1 分支上的代码区别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff &amp;lt;branch1&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 显示两个分支上的代码之间的区别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff &amp;lt;branch1&amp;gt; &amp;lt;branch2&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;所有上述命令后面都可以加一个目录或文件路径来只显示这个目录或文件中的区别：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;git diff /path/to/folder&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff /path/to/file.py&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 也可用git的参数终止符号--，避免文件名和参数重名时将文件名解析为参数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff --  /path/to/file.py&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="Git" scheme="http://vra.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>C++ std::optional 使用教程</title>
    <link href="http://vra.github.io/2023/06/30/cpp-optional-tutorial/"/>
    <id>http://vra.github.io/2023/06/30/cpp-optional-tutorial/</id>
    <published>2023-06-30T14:39:41.000Z</published>
    <updated>2023-06-30T14:40:50.994Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-std-optional-是什么"><a href="#1-std-optional-是什么" class="headerlink" title="1. std::optional 是什么"></a>1. std::optional 是什么</h3><p>C++ 17 引入了std::optional，表示一个可能有值的对象（没有值时就是默认的<code>std::nullopt</code>)，例如这个例子中，std::optional 对象 even_value，如果<code>is_even</code> 为真的话就是128，否则就是默认值<code>std::nullopt</code>: </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;optiona&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">bool</span> is_even = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 没有值的情况下 std::optional 对象的值为 std::nullopt</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; even_value = is_even ? std::optional&lt;<span class="keyword">int</span>&gt;(<span class="number">128</span>) : std::nullopt;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以用 std::optional 对象是否等于 std::nullopt 来判断 std::optional 对象是否有值</span></span><br><span class="line"><span class="keyword">if</span> (even_value != std::nullopt) &#123;</span><br><span class="line">    <span class="comment">// 采用.value 获取 std::optional 对象的值</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;has value, which is &quot;</span> &lt;&lt; even_value.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;no value&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其实std::optional的作用和Python里面的<code>None</code>比较像，例如上面的例子用Python来写就是这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">is_even = <span class="literal">True</span></span><br><span class="line">even_value = <span class="number">128</span> <span class="keyword">if</span> is_even <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> even_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;has value, which is&quot;</span>, even_value)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;no value&quot;</span>)</span><br></pre></td></tr></table></figure><span id="more"></span><h3 id="2-为什么要引入-std-optional"><a href="#2-为什么要引入-std-optional" class="headerlink" title="2. 为什么要引入 std::optional"></a>2. 为什么要引入 std::optional</h3><p>我觉得提出std::optional就是因为C++底层缺少<code>None</code> 这个表示，所以将std::nullopt和某种特定类型的变量合并在一起构造成一个<code>std::optional</code>对象，用以解决因为缺少之前<code>None</code>因而存在的一些不怎么直接的用法。</p><p>这里举个例子来说明前面提到的”不直接”的用法。这是一个寻找数组中的第一个非0元素的函数：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findFirstNonZero</span><span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (arr[i] != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> arr[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>; <span class="comment">// 如果数组中没有非0元素，则返回-1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，没找到元素时返回-1，所以当拿到-1时，没法判断是第一个非0元素为-1还是没找到非0元素。<br>改进方案是返回一个pair，第一个位置表示是否包含非0元素，第二个位置表示非0元素的值：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">std::pair&lt;<span class="keyword">bool</span>, <span class="keyword">int</span>&gt; <span class="title">findFirstNonZero</span><span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (arr[i] != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> std::<span class="built_in">make_pair</span>(<span class="literal">true</span>, arr[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> std::<span class="built_in">make_pair</span>(<span class="literal">false</span>, <span class="number">-1</span>); <span class="comment">// 如果数组中没有非0元素，则返回false和-1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>但这样其实比较繁琐且不直观，两个变量的解析和使用成本还是有些高，如果能用一个变量来完成的话就更简洁了。</p><p>采用std::optional可以简化上面的代码：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;optional&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">std::optional&lt;<span class="keyword">int</span>&gt; <span class="title">findFirstNonZero</span><span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (arr[i] != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> arr[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> std::nullopt; <span class="comment">// 如果数组中没有非0元素，则返回std::nullopt</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意这里int类型的返回值可以隐式地转换为 std::optional 对象。</p><p>使用这个函数时也只需要判断一下返回值是否为<code>std::nullopt</code> 就可以。</p><p>总之可以将std::optional对象当作支持判断是否为NULL的对象的封装，在不确定对象是否存在的情况下，建议使用。</p><h3 id="3-std-optional-的构造"><a href="#3-std-optional-的构造" class="headerlink" title="3. std::optional 的构造"></a>3. std::optional 的构造</h3><p>空的 std::optional 对象可以用<code>std::nullopt</code> 或者<code>&#123;&#125;</code> 来构造，然后用<code>emplace</code> 函数来插入数值：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.0 采用 std::nullopt 初始化再调用 emplace 插入值</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val0 = std::nullopt;</span><br><span class="line">val0.<span class="built_in">emplace</span>(<span class="number">128</span>);</span><br><span class="line">std::cout &lt;&lt; val0.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.1 采用 &#123;&#125; 初始化再调用 emplace 插入值</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val1 = &#123;&#125;;</span><br><span class="line">val1.<span class="built_in">emplace</span>(<span class="number">128</span>);</span><br><span class="line">std::cout &lt;&lt; val1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure><p>每次调用<code>emplace</code> 时，会清除掉之前的值，因此可以多次调用，且能保证每次都是最新的数值。</p><p>也可以用 <code>std::make_optional</code> 函数来构造：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.7 采用 std::make_optional&lt;T&gt;(val) 初始化</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val7 = std::make_optional&lt;<span class="keyword">int</span>&gt;(<span class="number">128</span>);</span><br><span class="line">std::cout &lt;&lt; val7.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.8 采用 std::make_optional(val) 初始化，自动推导变量类型</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val8 = std::<span class="built_in">make_optional</span>(<span class="number">128</span>);</span><br><span class="line">std::cout &lt;&lt; val8.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>除此之外还有很多种初始化 std::optional 对象的方法，都写在这个示例代码里面了，记得看注释：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.2 采用 std::optional&lt;T&gt;(val) 初始化</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val2 = std::optional&lt;<span class="keyword">int</span>&gt;(<span class="number">128</span>);</span><br><span class="line">std::cout &lt;&lt; val2.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.3 采用 std::optional(val) 初始化，自动推导变量类型</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val3 = std::<span class="built_in">optional</span>(<span class="number">128</span>);</span><br><span class="line">std::cout &lt;&lt; val3.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.4 采用 std::optional&lt;T&gt;&#123;val&#125; 初始化</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val4 = std::optional&lt;<span class="keyword">int</span>&gt;&#123;<span class="number">128</span>&#125;;</span><br><span class="line">std::cout &lt;&lt; val4.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.5 采用 std::optional&#123;val&#125; 初始化</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val5 = std::optional&#123;<span class="number">128</span>&#125;;</span><br><span class="line">std::cout &lt;&lt; val5.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.6 采用 &#123;val&#125; 初始化</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val6 = &#123;<span class="number">128</span>&#125;;</span><br><span class="line">std::cout &lt;&lt; val6.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="4-std-optional-判断是否有值"><a href="#4-std-optional-判断是否有值" class="headerlink" title="4. std::optional 判断是否有值"></a>4. std::optional 判断是否有值</h3><p>判断 std::optional 对象是否有值可以用 <code>has_value</code>函数，或者判断是否不等于<code>std::nullopt</code>，或者直接用if语句对对象进行判断：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; result1 = <span class="built_in">find_the_first_postive_value</span>(pos_values);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (result1.<span class="built_in">has_value</span>()) &#123;</span><br><span class="line">    std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (result1 != std::nullopt) &#123;</span><br><span class="line">    std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (result1) &#123;</span><br><span class="line">    std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5-std-optional-获取值"><a href="#5-std-optional-获取值" class="headerlink" title="5. std::optional 获取值"></a>5. std::optional 获取值</h3><p>获取值的话可以用<code>.value()</code> 函数，或者<code>*</code> 运算符：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (result1) &#123;</span><br><span class="line">     std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">if</span> (result1) &#123;</span><br><span class="line">     std::cout &lt;&lt; *result1 &lt;&lt; std::endl;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>如果想在std::optional对象为<code>std::nullopt</code>的情况下设置默认值的话，可以用<code>value_or</code> 函数：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val9 = std::nullopt;</span><br><span class="line">std::cout &lt;&lt; val9.<span class="built_in">value_or</span>(<span class="number">-1</span>) &lt;&lt; std::endl; <span class="comment">// 输出 -1</span></span><br><span class="line">val9.<span class="built_in">emplace</span>(<span class="number">128</span>);</span><br><span class="line">std::cout &lt;&lt; val9.<span class="built_in">value_or</span>(<span class="number">-1</span>) &lt;&lt; std::endl; <span class="comment">// 输出 128</span></span><br></pre></td></tr></table></figure><p>很明显，<code>value_or</code>函数中的默认值需要和optional对象的类型一致，否则会编译报错。</p><h3 id="6-没有值时的异常处理"><a href="#6-没有值时的异常处理" class="headerlink" title="6. 没有值时的异常处理"></a>6. 没有值时的异常处理</h3><p>如果在没有值的情况下调用<code>.value</code> 函数，会在运行时报错<code>std::bad_optional_access</code>:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val10 = std::nullopt;</span><br><span class="line">std::cout &lt;&lt; val10.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">libc++abi: terminating due to uncaught exception of type std::bad_optional_access: bad_optional_access</span><br></pre></td></tr></table></figure><p>所以建议使用<code>.value_or</code>来处理，如果要强行使用<code>.value</code>的话，需要使用 try-catch 语句：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val11 = std::nullopt;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    std::cout &lt;&lt; val11.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125; <span class="built_in"><span class="keyword">catch</span></span> (<span class="keyword">const</span> std::bad_optional_access&amp; e) &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;==&gt; error: &quot;</span> &lt;&lt; e.<span class="built_in">what</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="7-示例代码"><a href="#7-示例代码" class="headerlink" title="7. 示例代码"></a>7. 示例代码</h3><p>上面的所有示例代码汇总：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;optional&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">std::optional&lt;<span class="keyword">int</span>&gt; <span class="title">find_the_first_postive_value</span><span class="params">(<span class="keyword">const</span> std::vector&lt;<span class="keyword">int</span>&gt;&amp; values)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; val : values) &#123;</span><br><span class="line">        <span class="keyword">if</span> (val &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> std::optional&lt;<span class="keyword">int</span>&gt;(val);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> std::nullopt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">std::optional&lt;<span class="keyword">int</span>&gt; <span class="title">find_the_first_postive_value_v2</span><span class="params">(<span class="keyword">const</span> std::vector&lt;<span class="keyword">int</span>&gt;&amp; values)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> it = std::<span class="built_in">find_if</span>(values.<span class="built_in">begin</span>(), values.<span class="built_in">end</span>(), [](<span class="keyword">int</span> val) &#123; <span class="keyword">return</span> val &gt; <span class="number">0</span>; &#125;);</span><br><span class="line">    <span class="keyword">return</span> it != values.<span class="built_in">end</span>() ? std::<span class="built_in">make_optional</span>(*it) : std::nullopt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">show_backend</span><span class="params">(std::optional&lt;std::string&gt; backend)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (backend) &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;==&gt; use set backend: &quot;</span> &lt;&lt; backend.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;==&gt; use default backend: CPU&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// std::optional 简单例子</span></span><br><span class="line">    <span class="keyword">bool</span> is_even = <span class="literal">true</span>;</span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; even_value = is_even ? std::optional&lt;<span class="keyword">int</span>&gt;(<span class="number">128</span>) : std::nullopt;</span><br><span class="line">    <span class="keyword">if</span> (even_value != std::nullopt) &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;has value, which is &quot;</span> &lt;&lt; even_value.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;no value&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. std::optional 对象的构造</span></span><br><span class="line">    <span class="comment">// 1.0 采用 std::nullopt 初始化再调用 emplace 插入值</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val0 = std::nullopt;</span><br><span class="line">    val0.<span class="built_in">emplace</span>(<span class="number">128</span>);</span><br><span class="line">    val0.<span class="built_in">emplace</span>(<span class="number">129</span>);</span><br><span class="line">    std::cout &lt;&lt; val0.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.1 采用 &#123;&#125; 初始化再调用 emplace 插入值</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val1 = &#123;&#125;;</span><br><span class="line">    val1.<span class="built_in">emplace</span>(<span class="number">128</span>);</span><br><span class="line">    std::cout &lt;&lt; val1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.2 采用 std::optional&lt;T&gt;(val) 初始化</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val2 = std::optional&lt;<span class="keyword">int</span>&gt;(<span class="number">128</span>);</span><br><span class="line">    std::cout &lt;&lt; val2.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.3 采用 std::optional(val) 初始化，自动推导变量类型</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val3 = std::<span class="built_in">optional</span>(<span class="number">128</span>);</span><br><span class="line">    std::cout &lt;&lt; val3.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.4 采用 std::optional&lt;T&gt;&#123;val&#125; 初始化</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val4 = std::optional&lt;<span class="keyword">int</span>&gt;&#123;<span class="number">128</span>&#125;;</span><br><span class="line">    std::cout &lt;&lt; val4.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.5 采用 std::optional&#123;val&#125; 初始化</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val5 = std::optional&#123;<span class="number">128</span>&#125;;</span><br><span class="line">    std::cout &lt;&lt; val5.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.6 采用 &#123;val&#125; 初始化</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val6 = &#123;<span class="number">128</span>&#125;;</span><br><span class="line">    std::cout &lt;&lt; val6.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.7 采用 std::make_optional&lt;T&gt;(val) 初始化</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val7 = std::make_optional&lt;<span class="keyword">int</span>&gt;(<span class="number">128</span>);</span><br><span class="line">    std::cout &lt;&lt; val7.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.8 采用 std::make_optional(val) 初始化，自动推导变量类型</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val8 = std::<span class="built_in">make_optional</span>(<span class="number">128</span>);</span><br><span class="line">    std::cout &lt;&lt; val8.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val9 = std::nullopt;</span><br><span class="line">    std::cout &lt;&lt; val9.<span class="built_in">value_or</span>(<span class="number">-1</span>) &lt;&lt; std::endl;</span><br><span class="line">    val9.<span class="built_in">emplace</span>(<span class="number">128</span>);</span><br><span class="line">    std::cout &lt;&lt; val9.<span class="built_in">value_or</span>(<span class="number">-1</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    std::optional&lt;int&gt; val10 = std::nullopt;</span></span><br><span class="line">    <span class="comment">//    std::cout &lt;&lt; val10.value() &lt;&lt; std::endl;</span></span><br><span class="line"></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val11 = std::nullopt;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        std::cout &lt;&lt; val11.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125; <span class="built_in"><span class="keyword">catch</span></span> (<span class="keyword">const</span> std::bad_optional_access&amp; e) &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;==&gt; error: &quot;</span> &lt;&lt; e.<span class="built_in">what</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 函数调用例子</span></span><br><span class="line">    std::vector&lt;<span class="keyword">int</span>&gt; neg_values = &#123;<span class="number">-1</span>, <span class="number">-3</span>, <span class="number">-5</span>&#125;;</span><br><span class="line">    std::vector&lt;<span class="keyword">int</span>&gt; pos_values = &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> result1 = <span class="built_in">find_the_first_postive_value_v2</span>(pos_values);</span><br><span class="line">    <span class="keyword">if</span> (result1.<span class="built_in">has_value</span>()) &#123;</span><br><span class="line">        std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (result1 != std::nullopt) &#123;</span><br><span class="line">        std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (result1) &#123;</span><br><span class="line">        std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (result1) &#123;</span><br><span class="line">        std::cout &lt;&lt; *result1 &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// try-catch 示例</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125; <span class="built_in"><span class="keyword">catch</span></span> (<span class="keyword">const</span> std::bad_optional_access&amp; e) &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;==&gt; error: &quot;</span> &lt;&lt; e.<span class="built_in">what</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">show_backend</span>(std::nullopt);</span><br><span class="line">    <span class="built_in">show_backend</span>(std::<span class="built_in">make_optional</span>(<span class="string">&quot;CUDA&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> my_backend = std::optional&lt;std::string&gt;&#123;<span class="string">&quot;MPS&quot;</span>&#125;;</span><br><span class="line">    <span class="built_in">show_backend</span>(my_backend);</span><br><span class="line">    my_backend.<span class="built_in">emplace</span>(<span class="string">&quot;DSP&quot;</span>);</span><br><span class="line">    <span class="built_in">show_backend</span>(my_backend);</span><br><span class="line"></span><br><span class="line">    std::optional&lt;std::vector&lt;<span class="keyword">int</span>&gt;&gt; res = std::optional&lt;std::vector&lt;<span class="keyword">int</span>&gt;&gt;(&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;);</span><br><span class="line">    std::cout &lt;&lt; res.<span class="built_in">value</span>()[<span class="number">0</span>] &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以通过<code>g++ -std=c++17 main.cpp  &amp;&amp; ./a.out</code> 来编译运行。</p><h3 id="8-参考"><a href="#8-参考" class="headerlink" title="8. 参考"></a>8. 参考</h3><ol><li><a href="https://en.cppreference.com/w/cpp/utility/optional">https://en.cppreference.com/w/cpp/utility/optional</a></li><li><a href="https://devblogs.microsoft.com/cppblog/stdoptional-how-when-and-why">https://devblogs.microsoft.com/cppblog/stdoptional-how-when-and-why</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-std-optional-是什么&quot;&gt;&lt;a href=&quot;#1-std-optional-是什么&quot; class=&quot;headerlink&quot; title=&quot;1. std::optional 是什么&quot;&gt;&lt;/a&gt;1. std::optional 是什么&lt;/h3&gt;&lt;p&gt;C++ 17 引入了std::optional，表示一个可能有值的对象（没有值时就是默认的&lt;code&gt;std::nullopt&lt;/code&gt;)，例如这个例子中，std::optional 对象 even_value，如果&lt;code&gt;is_even&lt;/code&gt; 为真的话就是128，否则就是默认值&lt;code&gt;std::nullopt&lt;/code&gt;: &lt;/p&gt;
&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;include&lt;/span&gt; &lt;span class=&quot;meta-string&quot;&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;include&lt;/span&gt; &lt;span class=&quot;meta-string&quot;&gt;&amp;lt;optiona&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt; is_even = &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// 在 没有值的情况下 std::optional 对象的值为 std::nullopt&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;std::optional&amp;lt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;&amp;gt; even_value = is_even ? std::optional&amp;lt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;&amp;gt;(&lt;span class=&quot;number&quot;&gt;128&lt;/span&gt;) : std::nullopt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// 可以用 std::optional 对象是否等于 std::nullopt 来判断 std::optional 对象是否有值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (even_value != std::nullopt) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;// 采用.value 获取 std::optional 对象的值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    std::cout &amp;lt;&amp;lt; &lt;span class=&quot;string&quot;&gt;&amp;quot;has value, which is &amp;quot;&lt;/span&gt; &amp;lt;&amp;lt; even_value.&lt;span class=&quot;built_in&quot;&gt;value&lt;/span&gt;() &amp;lt;&amp;lt; std::endl;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125; &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    std::cout &amp;lt;&amp;lt; &lt;span class=&quot;string&quot;&gt;&amp;quot;no value&amp;quot;&lt;/span&gt; &amp;lt;&amp;lt; std::endl;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;其实std::optional的作用和Python里面的&lt;code&gt;None&lt;/code&gt;比较像，例如上面的例子用Python来写就是这样：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;is_even = &lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;even_value = &lt;span class=&quot;number&quot;&gt;128&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; is_even &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; even_value &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&amp;quot;has value, which is&amp;quot;&lt;/span&gt;, even_value)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&amp;quot;no value&amp;quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="C++" scheme="http://vra.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>libtorch系列教程3：优雅地训练MNIST分类模型</title>
    <link href="http://vra.github.io/2023/06/30/libtorch-tutorial3/"/>
    <id>http://vra.github.io/2023/06/30/libtorch-tutorial3/</id>
    <published>2023-06-30T14:38:16.000Z</published>
    <updated>2023-06-30T14:39:28.217Z</updated>
    
    <content type="html"><![CDATA[<p>在这篇文章中，我们对如何使用Libtorch进行MNIST分类模型的训练和测试进行详细描述。首先会浏览官方MNIST示例，然后对其进行模块化重构，为后续别的模型的训练提供 codebase。</p><p>由于Libtorch中包含很多和Pytorch中没有的类型，所以看Libtorch代码的时候时常会遇到不了解的函数或者类，这时候可以在<a href="https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/include/torch">这里</a>查找对应的类的实现，了解其作用。Libtorch C++ 代码中的注释虽然不多但基本够用了。</p><p>这里列举一些常见的类的代码路径，方便查询：</p><ul><li>Datasets: <a href="https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/base.h">https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/base.h</a></li><li>DataLoader:<a href="https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/include/torch/data/dataloader/base.h">https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/include/torch/data/dataloader/base.h</a></li><li>MNIST: <a href="https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/mnist.h">https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/mnist.h</a></li><li>Stack: <a href="https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/transforms/stack.h">https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/transforms/stack.h</a></li><li>RandomSampler:  <a href="https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/random.cpp">https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/random.cpp</a></li><li>SequentialSampler: <a href="https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/sequential.cpp">https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/sequential.cpp</a></li></ul><span id="more"></span><ul><li><h3 id="1-官方MNIST示例"><a href="#1-官方MNIST示例" class="headerlink" title="1. 官方MNIST示例"></a>1. 官方MNIST示例</h3>Libtorch官方的训练代码仓库在<a href="https://github.com/pytorch/examples/tree/main/cpp">这里</a>，拿里面的训练MNIST为例，代码如下：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstddef&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Where to find the MNIST dataset.</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* kDataRoot = <span class="string">&quot;./data&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// The batch size for training.</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int64_t</span> kTrainBatchSize = <span class="number">64</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// The batch size for testing.</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int64_t</span> kTestBatchSize = <span class="number">1000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// The number of epochs to train.</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int64_t</span> kNumberOfEpochs = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// After how many batches to log a new update with the loss value.</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int64_t</span> kLogInterval = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Net</span> :</span> torch::nn::Module &#123;</span><br><span class="line">  <span class="built_in">Net</span>()</span><br><span class="line">      : <span class="built_in">conv1</span>(torch::nn::<span class="built_in">Conv2dOptions</span>(<span class="number">1</span>, <span class="number">10</span>, <span class="comment">/*kernel_size=*/</span><span class="number">5</span>)),</span><br><span class="line">        <span class="built_in">conv2</span>(torch::nn::<span class="built_in">Conv2dOptions</span>(<span class="number">10</span>, <span class="number">20</span>, <span class="comment">/*kernel_size=*/</span><span class="number">5</span>)),</span><br><span class="line">        <span class="built_in">fc1</span>(<span class="number">320</span>, <span class="number">50</span>),</span><br><span class="line">        <span class="built_in">fc2</span>(<span class="number">50</span>, <span class="number">10</span>) &#123;</span><br><span class="line">    <span class="built_in">register_module</span>(<span class="string">&quot;conv1&quot;</span>, conv1);</span><br><span class="line">    <span class="built_in">register_module</span>(<span class="string">&quot;conv2&quot;</span>, conv2);</span><br><span class="line">    <span class="built_in">register_module</span>(<span class="string">&quot;conv2_drop&quot;</span>, conv2_drop);</span><br><span class="line">    <span class="built_in">register_module</span>(<span class="string">&quot;fc1&quot;</span>, fc1);</span><br><span class="line">    <span class="built_in">register_module</span>(<span class="string">&quot;fc2&quot;</span>, fc2);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">torch::Tensor <span class="title">forward</span><span class="params">(torch::Tensor x)</span> </span>&#123;</span><br><span class="line">    x = torch::<span class="built_in">relu</span>(torch::<span class="built_in">max_pool2d</span>(conv1-&gt;forward(x), <span class="number">2</span>));</span><br><span class="line">    x = torch::<span class="built_in">relu</span>(</span><br><span class="line">        torch::<span class="built_in">max_pool2d</span>(conv2_drop-&gt;forward(conv2-&gt;forward(x)), <span class="number">2</span>));</span><br><span class="line">    x = x.<span class="built_in">view</span>(&#123;<span class="number">-1</span>, <span class="number">320</span>&#125;);</span><br><span class="line">    x = torch::<span class="built_in">relu</span>(fc1-&gt;forward(x));</span><br><span class="line">    x = torch::<span class="built_in">dropout</span>(x, <span class="comment">/*p=*/</span><span class="number">0.5</span>, <span class="comment">/*training=*/</span><span class="built_in">is_training</span>());</span><br><span class="line">    x = fc2-&gt;forward(x);</span><br><span class="line">    <span class="keyword">return</span> torch::<span class="built_in">log_softmax</span>(x, <span class="comment">/*dim=*/</span><span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  torch::nn::Conv2d conv1;</span><br><span class="line">  torch::nn::Conv2d conv2;</span><br><span class="line">  torch::nn::Dropout2d conv2_drop;</span><br><span class="line">  torch::nn::Linear fc1;</span><br><span class="line">  torch::nn::Linear fc2;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> DataLoader&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">train</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">size_t</span> epoch,</span></span></span><br><span class="line"><span class="params"><span class="function">    Net&amp; model,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Device device,</span></span></span><br><span class="line"><span class="params"><span class="function">    DataLoader&amp; data_loader,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::optim::Optimizer&amp; optimizer,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">size_t</span> dataset_size)</span> </span>&#123;</span><br><span class="line">  model.<span class="built_in">train</span>();</span><br><span class="line">  <span class="keyword">size_t</span> batch_idx = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; batch : data_loader) &#123;</span><br><span class="line">    <span class="keyword">auto</span> data = batch.data.<span class="built_in">to</span>(device), targets = batch.target.<span class="built_in">to</span>(device);</span><br><span class="line">    optimizer.<span class="built_in">zero_grad</span>();</span><br><span class="line">    <span class="keyword">auto</span> output = model.forward(data);</span><br><span class="line">    <span class="keyword">auto</span> loss = torch::<span class="built_in">nll_loss</span>(output, targets);</span><br><span class="line">    <span class="built_in">AT_ASSERT</span>(!std::<span class="built_in">isnan</span>(loss.<span class="keyword">template</span> item&lt;<span class="keyword">float</span>&gt;()));</span><br><span class="line">    loss.<span class="built_in">backward</span>();</span><br><span class="line">    optimizer.<span class="built_in">step</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (batch_idx++ % kLogInterval == <span class="number">0</span>) &#123;</span><br><span class="line">      std::<span class="built_in">printf</span>(</span><br><span class="line">          <span class="string">&quot;\rTrain Epoch: %ld [%5ld/%5ld] Loss: %.4f&quot;</span>,</span><br><span class="line">          epoch,</span><br><span class="line">          batch_idx * batch.data.<span class="built_in">size</span>(<span class="number">0</span>),</span><br><span class="line">          dataset_size,</span><br><span class="line">          loss.<span class="keyword">template</span> item&lt;<span class="keyword">float</span>&gt;());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> DataLoader&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">test</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    Net&amp; model,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Device device,</span></span></span><br><span class="line"><span class="params"><span class="function">    DataLoader&amp; data_loader,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">size_t</span> dataset_size)</span> </span>&#123;</span><br><span class="line">  torch::NoGradGuard no_grad;</span><br><span class="line">  model.<span class="built_in">eval</span>();</span><br><span class="line">  <span class="keyword">double</span> test_loss = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int32_t</span> correct = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; batch : data_loader) &#123;</span><br><span class="line">    <span class="keyword">auto</span> data = batch.data.<span class="built_in">to</span>(device), targets = batch.target.<span class="built_in">to</span>(device);</span><br><span class="line">    <span class="keyword">auto</span> output = model.forward(data);</span><br><span class="line">    test_loss += torch::<span class="built_in">nll_loss</span>(</span><br><span class="line">                     output,</span><br><span class="line">                     targets,</span><br><span class="line">                     <span class="comment">/*weight=*/</span>&#123;&#125;,</span><br><span class="line">                     torch::Reduction::Sum)</span><br><span class="line">                     .<span class="keyword">template</span> item&lt;<span class="keyword">float</span>&gt;();</span><br><span class="line">    <span class="keyword">auto</span> pred = output.<span class="built_in">argmax</span>(<span class="number">1</span>);</span><br><span class="line">    correct += pred.<span class="built_in">eq</span>(targets).<span class="built_in">sum</span>().<span class="keyword">template</span> item&lt;<span class="keyword">int64_t</span>&gt;();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  test_loss /= dataset_size;</span><br><span class="line">  std::<span class="built_in">printf</span>(</span><br><span class="line">      <span class="string">&quot;\nTest set: Average loss: %.4f | Accuracy: %.3f\n&quot;</span>,</span><br><span class="line">      test_loss,</span><br><span class="line">      <span class="keyword">static_cast</span>&lt;<span class="keyword">double</span>&gt;(correct) / dataset_size);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">auto</span> <span class="title">main</span><span class="params">()</span> -&gt; <span class="keyword">int</span> </span>&#123;</span><br><span class="line">  torch::<span class="built_in">manual_seed</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  torch::DeviceType device_type;</span><br><span class="line">  <span class="keyword">if</span> (torch::cuda::<span class="built_in">is_available</span>()) &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;CUDA available! Training on GPU.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    device_type = torch::kCUDA;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Training on CPU.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    device_type = torch::kCPU;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">torch::Device <span class="title">device</span><span class="params">(device_type)</span></span>;</span><br><span class="line"></span><br><span class="line">  Net model;</span><br><span class="line">  model.<span class="built_in">to</span>(device);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> train_dataset = torch::data::datasets::<span class="built_in">MNIST</span>(kDataRoot)</span><br><span class="line">                           .<span class="built_in">map</span>(torch::data::transforms::Normalize&lt;&gt;(<span class="number">0.1307</span>, <span class="number">0.3081</span>))</span><br><span class="line">                           .<span class="built_in">map</span>(torch::data::transforms::Stack&lt;&gt;());</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">size_t</span> train_dataset_size = train_dataset.<span class="built_in">size</span>().<span class="built_in">value</span>();</span><br><span class="line">  <span class="keyword">auto</span> train_loader =</span><br><span class="line">      torch::data::make_data_loader&lt;torch::data::samplers::SequentialSampler&gt;(</span><br><span class="line">          std::<span class="built_in">move</span>(train_dataset), kTrainBatchSize);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> test_dataset = torch::data::datasets::<span class="built_in">MNIST</span>(</span><br><span class="line">                          kDataRoot, torch::data::datasets::MNIST::Mode::kTest)</span><br><span class="line">                          .<span class="built_in">map</span>(torch::data::transforms::Normalize&lt;&gt;(<span class="number">0.1307</span>, <span class="number">0.3081</span>))</span><br><span class="line">                          .<span class="built_in">map</span>(torch::data::transforms::Stack&lt;&gt;());</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">size_t</span> test_dataset_size = test_dataset.<span class="built_in">size</span>().<span class="built_in">value</span>();</span><br><span class="line">  <span class="keyword">auto</span> test_loader =</span><br><span class="line">      torch::data::<span class="built_in">make_data_loader</span>(std::<span class="built_in">move</span>(test_dataset), kTestBatchSize);</span><br><span class="line"></span><br><span class="line">  torch::<span class="function">optim::SGD <span class="title">optimizer</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      model.parameters(), torch::optim::SGDOptions(<span class="number">0.01</span>).momentum(<span class="number">0.5</span>))</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> epoch = <span class="number">1</span>; epoch &lt;= kNumberOfEpochs; ++epoch) &#123;</span><br><span class="line">    <span class="built_in">train</span>(epoch, model, device, *train_loader, optimizer, train_dataset_size);</span><br><span class="line">    <span class="built_in">test</span>(model, device, *test_loader, test_dataset_size);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>代码具体细节可以先不用理解，后文有一些说明。可以看到所有的模型搭建、数据读取、网络训练和测试代码都混在一个文件里面，别的几个例子里面也是类似的写法。</p><p>这样写当然是可以的，但对于习惯了Pytorch训练的我们来说，这样所有的代码在一个文件中的写法很不易读，<br>修改数据和网络都相互有影响，且不利用真正严肃地模型训练迭代。</p><h3 id="2-重构-MNIST-示例代码"><a href="#2-重构-MNIST-示例代码" class="headerlink" title="2. 重构 MNIST 示例代码"></a>2. 重构 MNIST 示例代码</h3><p>所以一个简单的想法是改进写法，将DataLoader, Model 和训练逻辑拆分出来，分别进行模块化，放到单独的文件中处理。</p><h4 id="2-1-简单拆分的问题"><a href="#2-1-简单拆分的问题" class="headerlink" title="2.1 简单拆分的问题"></a>2.1 简单拆分的问题</h4><p>第一次尝试是将Dataset和DataLoader放到一个模块中，网络定义放到一个模块中，训练和测试代码放到一个模块中。<br>但这样拆分遇到很大问题，核心原因是 Libtorch 的DataLoader类别太复杂了，对于我这种C++了解不深入的人来说改造难度太大。</p><p>举个例子，我们对MNIST Dataset类进行Normalize后Stack，然后构造一个DataLoader对象<code>train_loader</code>，代码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> train_dataset = torch::data::datasets::<span class="built_in">MNIST</span>(data_root)</span><br><span class="line">                             .<span class="built_in">map</span>(torch::data::transforms::Normalize&lt;&gt;(<span class="number">0.1307</span>, <span class="number">0.3081</span>))</span><br><span class="line">                             .<span class="built_in">map</span>(torch::data::transforms::Stack&lt;&gt;());</span><br><span class="line"><span class="keyword">auto</span> train_loader =</span><br><span class="line">        torch::data::make_data_loader&lt;torch::data::samplers::SequentialSampler&gt;(std::<span class="built_in">move</span>(train_dataset), <span class="number">64</span>);</span><br></pre></td></tr></table></figure><p>生成的<code>train_loader</code>对象的类型是：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch::<span class="keyword">disable_if_t</span>&lt;MapDataset&lt;MapDataset&lt;MNIST, Normalize&lt;&gt;&gt;, Stack&lt;&gt;&gt;::is_stateful || !std::is_constructible&lt;SequentialSampler, <span class="keyword">size_t</span>&gt;::value, std::unique_ptr&lt;StatelessDataLoader&lt;MapDataset&lt;MapDataset&lt;MNIST, Normalize&lt;&gt;&gt;, Stack&lt;&gt;&gt;, SequentialSampler&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>这个类型太复杂了……</p><p>因为官方示例是所有代码在一个文件，因此可以通过<code>auto</code> 来让编译器自动判定类型，省去了写着一长串类型的问题。</p><p>但如果我们要拆分DataLoader到单独的类里面的话，就没法使用<code>auto</code>，需要显式的指出DataLoader的类型，然而即使是这样一长串的类型写上了，还是会有不知道是哪里的问题，导致编译报错。</p><p>当然也有可能有简单的方法来解决这个问题，欢迎C++高手讨论指导。</p><p>这次体验让我真正体会到了动态类型语言的简洁性，以及Python的所有类型转C++会存在哪些坑。</p><h4 id="2-2-一种比较简单的重构方案"><a href="#2-2-一种比较简单的重构方案" class="headerlink" title="2.2 一种比较简单的重构方案"></a>2.2 一种比较简单的重构方案</h4><p>最后给出了一个妥协的方案：DataSet在单独的类中定义里面，而DataLoader在训练逻辑中构造，避免繁琐的类型问题。</p><p>整体代码结构如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">├── CMakeLists.txt <span class="comment"># CMake配置文件</span></span><br><span class="line">├── main.cpp <span class="comment"># 主入口</span></span><br><span class="line">├── my_dataset.cpp <span class="comment"># 数据集实现</span></span><br><span class="line">├── my_dataset.h </span><br><span class="line">├── my_model.cpp <span class="comment"># 模型定义</span></span><br><span class="line">├── my_model.h</span><br><span class="line">├── my_trainer.cpp <span class="comment"># 训练和测试脚手架代码</span></span><br><span class="line">└── my_trainer.h</span><br></pre></td></tr></table></figure><h5 id="2-2-1-CMake-配置文件"><a href="#2-2-1-CMake-配置文件" class="headerlink" title="2.2.1 CMake 配置文件"></a>2.2.1 CMake 配置文件</h5><p>CMake 配置文件<code>CMakeLists.txt</code>中将几个实现文件加入到编译依赖即可，别的部分与前两篇文章中的类似。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.0</span> FATAL_ERROR)</span><br><span class="line"><span class="keyword">project</span>(mnist_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要找到Libtorch</span></span><br><span class="line"><span class="keyword">find_package</span>(Torch REQUIRED)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; $&#123;TORCH_CXX_FLAGS&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> main.cpp my_model.cpp my_dataset.cpp my_trainer.cpp)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> <span class="string">&quot;$&#123;TORCH_LIBRARIES&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Libtorch是基于C++14来实现的</span></span><br><span class="line"><span class="keyword">set_property</span>(<span class="keyword">TARGET</span> <span class="variable">$&#123;PROJECT_NAME&#125;</span> PROPERTY CXX_STANDARD <span class="number">14</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="2-2-2-主入口文件定义"><a href="#2-2-2-主入口文件定义" class="headerlink" title="2.2.2 主入口文件定义"></a>2.2.2 主入口文件定义</h5><p>主入口文件实现了超参数设置，网络和数据集初始化，以及调用Trainer进行训练和测试：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_dataset.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_model.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_trainer.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 超参数设置</span></span><br><span class="line">  std::string data_root = <span class="string">&quot;./data&quot;</span>;</span><br><span class="line">  <span class="keyword">int</span> train_batch_size = <span class="number">128</span>;</span><br><span class="line">  <span class="keyword">int</span> test_batch_size = <span class="number">1000</span>;</span><br><span class="line">  <span class="keyword">int</span> total_epoch_num = <span class="number">30</span>;</span><br><span class="line">  <span class="keyword">int</span> log_interval = <span class="number">10</span>;</span><br><span class="line">  <span class="keyword">int</span> num_workers = <span class="number">32</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 设置随机数种子</span></span><br><span class="line">  torch::<span class="built_in">manual_seed</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获取设备类型</span></span><br><span class="line">  torch::DeviceType device_type = torch::kCPU;</span><br><span class="line">  <span class="keyword">if</span> (torch::cuda::<span class="built_in">is_available</span>()) &#123;</span><br><span class="line">    device_type = torch::kCUDA;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">torch::Device <span class="title">device</span><span class="params">(device_type)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 构造网络</span></span><br><span class="line">  MyModel model;</span><br><span class="line">  model.<span class="built_in">to</span>(device);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 设置优化器</span></span><br><span class="line">  torch::<span class="function">optim::SGD <span class="title">optimizer</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      model.parameters(), torch::optim::SGDOptions(<span class="number">0.01</span>).momentum(<span class="number">0.5</span>))</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 构造训练和测试dataset</span></span><br><span class="line">  <span class="keyword">auto</span> train_dataset =</span><br><span class="line">      <span class="built_in">MyDataset</span>(data_root, torch::data::datasets::MNIST::Mode::kTrain);</span><br><span class="line">  <span class="keyword">auto</span> test_dataset =</span><br><span class="line">      <span class="built_in">MyDataset</span>(data_root, torch::data::datasets::MNIST::Mode::kTest);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Trainer初始化</span></span><br><span class="line">  <span class="keyword">auto</span> trainer = <span class="built_in">MyTrainer</span>(log_interval);</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> epoch = <span class="number">1</span>; epoch &lt; total_epoch_num; ++epoch) &#123;</span><br><span class="line">   <span class="comment">// 运行训练</span></span><br><span class="line">    trainer.<span class="built_in">train</span>(</span><br><span class="line">        epoch,</span><br><span class="line">        model,</span><br><span class="line">        optimizer,</span><br><span class="line">        device,</span><br><span class="line">        train_dataset,</span><br><span class="line">        train_batch_size,</span><br><span class="line">        num_workers);</span><br><span class="line">        </span><br><span class="line">    <span class="comment">// 运行测试</span></span><br><span class="line">    trainer.<span class="built_in">test</span>(model, device, test_dataset, test_batch_size, num_workers);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="2-2-3-网络定义"><a href="#2-2-3-网络定义" class="headerlink" title="2.2.3 网络定义"></a>2.2.3 网络定义</h5><p>网络结构采用简单的LeNet，两个conv层和2个fc层。<br>头文件 my_model.h 内容:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> once</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span> :</span> <span class="keyword">public</span> torch::nn::Module &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">MyModel</span>();</span><br><span class="line">  <span class="function">torch::Tensor <span class="title">forward</span><span class="params">(torch::Tensor x)</span></span>;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  torch::nn::Conv2d conv1 = <span class="literal">nullptr</span>;</span><br><span class="line">  torch::nn::Conv2d conv2 = <span class="literal">nullptr</span>;</span><br><span class="line">  torch::nn::Dropout2d conv2_drop;</span><br><span class="line">  torch::nn::Linear fc1 = <span class="literal">nullptr</span>;</span><br><span class="line">  torch::nn::Linear fc2 = <span class="literal">nullptr</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>实现文件 my_model.cpp:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_model.h&quot;</span></span></span><br><span class="line"></span><br><span class="line">MyModel::<span class="built_in">MyModel</span>() &#123;</span><br><span class="line">  conv1 = torch::nn::<span class="built_in">Conv2d</span>(torch::nn::<span class="built_in">Conv2dOptions</span>(<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>));</span><br><span class="line">  conv2 = torch::nn::<span class="built_in">Conv2d</span>(torch::nn::<span class="built_in">Conv2dOptions</span>(<span class="number">10</span>, <span class="number">20</span>, <span class="number">5</span>));</span><br><span class="line">  fc1 = torch::nn::<span class="built_in">Linear</span>(<span class="number">320</span>, <span class="number">50</span>);</span><br><span class="line">  fc2 = torch::nn::<span class="built_in">Linear</span>(<span class="number">50</span>, <span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">register_module</span>(<span class="string">&quot;conv1&quot;</span>, conv1);</span><br><span class="line">  <span class="built_in">register_module</span>(<span class="string">&quot;conv2&quot;</span>, conv2);</span><br><span class="line">  <span class="built_in">register_module</span>(<span class="string">&quot;conv2_drop&quot;</span>, conv2_drop);</span><br><span class="line">  <span class="built_in">register_module</span>(<span class="string">&quot;fc1&quot;</span>, fc1);</span><br><span class="line">  <span class="built_in">register_module</span>(<span class="string">&quot;fc2&quot;</span>, fc2);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">MyModel::forward</span><span class="params">(torch::Tensor x)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// conv1</span></span><br><span class="line">  x = conv1-&gt;forward(x);</span><br><span class="line">  x = torch::<span class="built_in">max_pool2d</span>(x, <span class="number">2</span>);</span><br><span class="line">  x = torch::<span class="built_in">relu</span>(x);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// conv2</span></span><br><span class="line">  x = conv2-&gt;forward(x);</span><br><span class="line">  x = conv2_drop-&gt;forward(x);</span><br><span class="line">  x = torch::<span class="built_in">max_pool2d</span>(x, <span class="number">2</span>);</span><br><span class="line">  x = torch::<span class="built_in">relu</span>(x);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// fc1</span></span><br><span class="line">  x = x.<span class="built_in">view</span>(&#123;<span class="number">-1</span>, <span class="number">320</span>&#125;);</span><br><span class="line">  x = fc1-&gt;forward(x);</span><br><span class="line">  x = torch::<span class="built_in">relu</span>(x);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// dropout</span></span><br><span class="line">  x = torch::<span class="built_in">dropout</span>(x, <span class="number">0.5</span>, <span class="built_in">is_training</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// fc2</span></span><br><span class="line">  x = fc2-&gt;forward(x);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// log softmax</span></span><br><span class="line">  x = torch::<span class="built_in">log_softmax</span>(x, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到网络的定义还是比较简单直接，可以直接从Python 网络定义迁移过去，几个核心点：</p><ul><li>网络类的定义需要继承<code>torch::nn::Module</code> 类</li><li>实现<code>forward</code> 函数来进行网络前项运算，其中每个层需要显式地调用<code>forward</code> 函数</li></ul><h5 id="2-2-4-数据集定义"><a href="#2-2-4-数据集定义" class="headerlink" title="2.2.4 数据集定义"></a>2.2.4 数据集定义</h5><p>由于 Libtorch 自带 MNIST的实现，我们这里只是做了一个简单的封装，作为模块化的例子。<br>头文件<code>my_dataset.h</code> 内容：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> once</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span> &#123;</span></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">MyDataset</span>(</span><br><span class="line">      <span class="keyword">const</span> std::string&amp; data_root,</span><br><span class="line">      torch::data::datasets::MNIST::Mode phase);</span><br><span class="line"></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  torch::data::datasets::MNIST mnist_dataset;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>实现文件<code>my_dataset.cpp</code> 内容：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_dataset.h&quot;</span></span></span><br><span class="line"></span><br><span class="line">MyDataset::<span class="built_in">MyDataset</span>(</span><br><span class="line">    <span class="keyword">const</span> std::string&amp; data_root,</span><br><span class="line">    torch::data::datasets::MNIST::Mode phase)</span><br><span class="line">    : <span class="built_in">mnist_dataset</span>(torch::data::datasets::<span class="built_in">MNIST</span>(data_root, phase)) &#123;&#125;</span><br></pre></td></tr></table></figure><p>这里有一个需要注意的点，由于MNIST类本身没有默认构造函数，所以在<code>MyDataset</code> 类的初始化列表中就必须给成员变量<code>mnist_dataset</code>赋值，否则会报下面的错:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">constructor for &#x27;MyDataset&#x27; must explicitly initialize the member &#x27;mnist_dataset&#x27; which does not have a default constructor</span><br></pre></td></tr></table></figure><h5 id="2-2-5-Trainer定义"><a href="#2-2-5-Trainer定义" class="headerlink" title="2.2.5 Trainer定义"></a>2.2.5 Trainer定义</h5><p>Trainer 包含训练和测试的两个函数，对数据和网络，优化器等输入进行计算，得到输出，计算loss和准确率。<br>头文件<code>my_trainer.h</code>内容：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> once</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_dataset.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_model.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyTrainer</span> &#123;</span></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">MyTrainer</span>(<span class="keyword">int</span> log_interval) : <span class="built_in">log_interval_</span>(log_interval)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">train</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="keyword">size_t</span> epoch,</span></span></span><br><span class="line"><span class="params"><span class="function">      MyModel&amp; model,</span></span></span><br><span class="line"><span class="params"><span class="function">      torch::optim::Optimizer&amp; optimizer,</span></span></span><br><span class="line"><span class="params"><span class="function">      torch::Device device,</span></span></span><br><span class="line"><span class="params"><span class="function">      MyDataset&amp; train_dataset,</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="keyword">int</span> num_workers)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">test</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      MyModel&amp; model,</span></span></span><br><span class="line"><span class="params"><span class="function">      torch::Device device,</span></span></span><br><span class="line"><span class="params"><span class="function">      MyDataset&amp; test_dataset,</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="keyword">int</span> num_workers)</span></span>;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="keyword">int</span> log_interval_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>实现文件<code>my_trainer.cpp</code> 内容：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_trainer.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MyTrainer::train</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">size_t</span> epoch,</span></span></span><br><span class="line"><span class="params"><span class="function">    MyModel&amp; model,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::optim::Optimizer&amp; optimizer,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Device device,</span></span></span><br><span class="line"><span class="params"><span class="function">    MyDataset&amp; train_dataset,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">int</span> num_workers)</span> </span>&#123;</span><br><span class="line">  model.<span class="built_in">train</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 对MNIST数据进行Normalize和Stack（将多个Tensor stack成一个Tensor)</span></span><br><span class="line">  <span class="keyword">auto</span> dataset = train_dataset.mnist_dataset</span><br><span class="line">                     .<span class="built_in">map</span>(torch::data::transforms::Normalize&lt;&gt;(<span class="number">0.1307</span>, <span class="number">0.3081</span>))</span><br><span class="line">                     .<span class="built_in">map</span>(torch::data::transforms::Stack&lt;&gt;());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 构造 DataLoader, 设置 batch size 和 worker 数目</span></span><br><span class="line">  <span class="keyword">auto</span> data_loader = torch::data::<span class="built_in">make_data_loader</span>(</span><br><span class="line">      dataset,</span><br><span class="line">      torch::data::<span class="built_in">DataLoaderOptions</span>()</span><br><span class="line">          .<span class="built_in">batch_size</span>(batch_size)</span><br><span class="line">          .<span class="built_in">workers</span>(num_workers));</span><br><span class="line">  <span class="keyword">auto</span> dataset_size = dataset.<span class="built_in">size</span>().<span class="built_in">value</span>();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">size_t</span> batch_idx = <span class="number">0</span>;</span><br><span class="line">  <span class="comment">// 网络训练</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; batch : *data_loader) &#123;</span><br><span class="line">    <span class="comment">// 获取数据和label</span></span><br><span class="line">    <span class="keyword">auto</span> data = batch.data.<span class="built_in">to</span>(device);</span><br><span class="line">    <span class="keyword">auto</span> targets = batch.target.<span class="built_in">to</span>(device);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 优化器 梯度清零</span></span><br><span class="line">    optimizer.<span class="built_in">zero_grad</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 模型前向操作，得到预测输出</span></span><br><span class="line">    <span class="keyword">auto</span> output = model.forward(data);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算loss</span></span><br><span class="line">    <span class="keyword">auto</span> loss = torch::<span class="built_in">nll_loss</span>(output, targets);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// loss 反传</span></span><br><span class="line">    loss.<span class="built_in">backward</span>();</span><br><span class="line">    optimizer.<span class="built_in">step</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印log信息</span></span><br><span class="line">    <span class="keyword">if</span> (batch_idx++ % log_interval_ == <span class="number">0</span>) &#123;</span><br><span class="line">      std::<span class="built_in">printf</span>(</span><br><span class="line">          <span class="string">&quot;\rTrain Epoch: %ld [%5llu/%5ld] Loss: %.4f&quot;</span>,</span><br><span class="line">          epoch,</span><br><span class="line">          batch_idx * batch.data.<span class="built_in">size</span>(<span class="number">0</span>),</span><br><span class="line">          dataset_size,</span><br><span class="line">          loss.<span class="keyword">template</span> item&lt;<span class="keyword">float</span>&gt;());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MyTrainer::test</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    MyModel&amp; model,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Device device,</span></span></span><br><span class="line"><span class="params"><span class="function">    MyDataset&amp; test_dataset,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">int</span> num_workers)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 测试时要将模型置为eval模式</span></span><br><span class="line">  model.<span class="built_in">eval</span>();</span><br><span class="line">  <span class="keyword">double</span> test_loss = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int32_t</span> correct = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 对MNIST数据进行Normalize和Stack（将多个Tensor stack成一个Tensor)</span></span><br><span class="line">  <span class="keyword">auto</span> dataset = test_dataset.mnist_dataset</span><br><span class="line">                     .<span class="built_in">map</span>(torch::data::transforms::Normalize&lt;&gt;(<span class="number">0.1307</span>, <span class="number">0.3081</span>))</span><br><span class="line">                     .<span class="built_in">map</span>(torch::data::transforms::Stack&lt;&gt;());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 构造 DataLoader, 设置 batch size 和 worker 数目</span></span><br><span class="line">  <span class="keyword">auto</span> data_loader = torch::data::<span class="built_in">make_data_loader</span>(</span><br><span class="line">      dataset,</span><br><span class="line">      torch::data::<span class="built_in">DataLoaderOptions</span>()</span><br><span class="line">          .<span class="built_in">batch_size</span>(batch_size)</span><br><span class="line">          .<span class="built_in">workers</span>(num_workers));</span><br><span class="line">  <span class="keyword">auto</span> dataset_size = dataset.<span class="built_in">size</span>().<span class="built_in">value</span>();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; batch : *data_loader) &#123;</span><br><span class="line">    <span class="comment">// 获取数据和label</span></span><br><span class="line">    <span class="keyword">auto</span> data = batch.data.<span class="built_in">to</span>(device);</span><br><span class="line">    <span class="keyword">auto</span> targets = batch.target.<span class="built_in">to</span>(device);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 模型前向操作，得到预测输出</span></span><br><span class="line">    <span class="keyword">auto</span> output = model.forward(data);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算测试时的 loss</span></span><br><span class="line">    test_loss += torch::<span class="built_in">nll_loss</span>(</span><br><span class="line">                     output,</span><br><span class="line">                     targets,</span><br><span class="line">                     <span class="comment">/*weight=*/</span>&#123;&#125;,</span><br><span class="line">                     torch::Reduction::Sum)</span><br><span class="line">                     .item&lt;<span class="keyword">float</span>&gt;();</span><br><span class="line">    <span class="keyword">auto</span> pred = output.<span class="built_in">argmax</span>(<span class="number">1</span>);</span><br><span class="line">    correct += pred.<span class="built_in">eq</span>(targets).<span class="built_in">sum</span>().<span class="keyword">template</span> item&lt;<span class="keyword">int64_t</span>&gt;();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  test_loss /= dataset_size;</span><br><span class="line">  std::<span class="built_in">printf</span>(</span><br><span class="line">      <span class="string">&quot;\nTest set: Average loss: %.4f | Accuracy: %.3f\n&quot;</span>,</span><br><span class="line">      test_loss,</span><br><span class="line">      <span class="keyword">static_cast</span>&lt;<span class="keyword">double</span>&gt;(correct) / dataset_size);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="2-2-6-编译和运行方式"><a href="#2-2-6-编译和运行方式" class="headerlink" title="2.2.6 编译和运行方式"></a>2.2.6 编译和运行方式</h5><p>我们基于CMake 编译上面的代码，同时下载MNIST数据集，完整的执行命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..  -DCMAKE_PREFIX_PATH=`python -c <span class="string">&#x27;import torch;print(torch.utils.cmake_prefix_path)&#x27;</span>`</span><br><span class="line">make -j8</span><br><span class="line"><span class="comment"># 下载MNIST数据</span></span><br><span class="line">mkdir data &amp;&amp; <span class="built_in">cd</span> data</span><br><span class="line">wget <span class="string">&quot;http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz&quot;</span> &amp;&amp; gunzip train-images-idx3-ubyte.gz</span><br><span class="line">wget <span class="string">&quot;http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz&quot;</span> &amp;&amp; gunzip train-labels-idx1-ubyte.gz</span><br><span class="line">wget <span class="string">&quot;http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz&quot;</span> &amp;&amp; gunzip t10k-images-idx3-ubyte.gz</span><br><span class="line">wget <span class="string">&quot;http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz&quot;</span> &amp;&amp; gunzip t10k-labels-idx1-ubyte.gz</span><br><span class="line"><span class="built_in">cd</span> ../</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行可执行文件</span></span><br><span class="line">./mnist_train</span><br></pre></td></tr></table></figure><p>训练和测试输出如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Train Epoch: 1 [59008/60000] Loss: 0.6824</span><br><span class="line">Test set: Average loss: 0.3265 | Accuracy: 0.910</span><br><span class="line">Train Epoch: 2 [59008/60000] Loss: 0.5521</span><br><span class="line">Test set: Average loss: 0.2018 | Accuracy: 0.941</span><br><span class="line">Train Epoch: 3 [59008/60000] Loss: 0.3403</span><br><span class="line">Test set: Average loss: 0.1523 | Accuracy: 0.954</span><br><span class="line">Train Epoch: 4 [59008/60000] Loss: 0.3885</span><br><span class="line">Test set: Average loss: 0.1236 | Accuracy: 0.965</span><br><span class="line">Train Epoch: 5 [59008/60000] Loss: 0.3502</span><br><span class="line">Test set: Average loss: 0.1083 | Accuracy: 0.967</span><br><span class="line">Train Epoch: 6 [59008/60000] Loss: 0.1389</span><br><span class="line">Test set: Average loss: 0.0961 | Accuracy: 0.970</span><br><span class="line">Train Epoch: 7 [59008/60000] Loss: 0.3550</span><br><span class="line">Test set: Average loss: 0.0899 | Accuracy: 0.972</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>可以看到准确率在逐渐提升。</p><p>这篇文章的内容主要就是这些，后面会根据训练一个实际一些的例子，比如nanoGPT，将在本文的codebase基础上，主要覆盖下面的内容：</p><ul><li>自定义数据集的Dataset类的搭建</li><li>复杂网络的定义(如ResNet, Transformer)</li><li>模型checkpoint的保存和读取</li></ul><p>欢迎点赞和关注！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在这篇文章中，我们对如何使用Libtorch进行MNIST分类模型的训练和测试进行详细描述。首先会浏览官方MNIST示例，然后对其进行模块化重构，为后续别的模型的训练提供 codebase。&lt;/p&gt;
&lt;p&gt;由于Libtorch中包含很多和Pytorch中没有的类型，所以看Libtorch代码的时候时常会遇到不了解的函数或者类，这时候可以在&lt;a href=&quot;https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/include/torch&quot;&gt;这里&lt;/a&gt;查找对应的类的实现，了解其作用。Libtorch C++ 代码中的注释虽然不多但基本够用了。&lt;/p&gt;
&lt;p&gt;这里列举一些常见的类的代码路径，方便查询：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Datasets: &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/base.h&quot;&gt;https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/base.h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DataLoader:&lt;a href=&quot;https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/include/torch/data/dataloader/base.h&quot;&gt;https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/include/torch/data/dataloader/base.h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MNIST: &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/mnist.h&quot;&gt;https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/mnist.h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stack: &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/transforms/stack.h&quot;&gt;https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/transforms/stack.h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RandomSampler:  &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/random.cpp&quot;&gt;https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/random.cpp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SequentialSampler: &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/sequential.cpp&quot;&gt;https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/sequential.cpp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="C++" scheme="http://vra.github.io/tags/C/"/>
    
    <category term="Pytorch" scheme="http://vra.github.io/tags/Pytorch/"/>
    
    <category term="Libtorch" scheme="http://vra.github.io/tags/Libtorch/"/>
    
  </entry>
  
  <entry>
    <title>NeoVim 代码格式化教程</title>
    <link href="http://vra.github.io/2023/06/17/neoformat-python-cpp/"/>
    <id>http://vra.github.io/2023/06/17/neoformat-python-cpp/</id>
    <published>2023-06-17T01:54:45.000Z</published>
    <updated>2023-06-17T03:20:33.226Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p><a href="https://github.com/sbdchd/neoformat">neoformat</a> 是 (Neo)Vim 的代码格式化插件，支持多种语言的格式化。这篇文章覆盖 Neoformat 对 Python 和 C++ 进行格式化的配置，以及如何在保存代码时自动进行格式化，可以直接应用的配置代码段在文章最后。</p><span id="more"></span><h3 id="2-neoformat安装"><a href="#2-neoformat安装" class="headerlink" title="2. neoformat安装"></a>2. neoformat安装</h3><p>采用 <a href="https://github.com/junegunn/vim-plug">Vim-Plug</a> 进行插件管理，在<code>~/.config/nvim/init.vim</code> 中添加下面的插件:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Plug &#x27;sbdchd/neoformat&#x27;</span><br></pre></td></tr></table></figure><p>然后用<code>:PlugInstall</code> 命令来安装插件。由于插件源码在 GitHub 上，国内访问时断时续，一次执行可能安装不成功，可以多执行几次这个命令，直到输出窗口显示安装成功。</p><h3 id="3-neoformat-格式化-Python-代码"><a href="#3-neoformat-格式化-Python-代码" class="headerlink" title="3. neoformat 格式化 Python 代码"></a>3. neoformat 格式化 Python 代码</h3><h4 id="3-1-安装格式化工具"><a href="#3-1-安装格式化工具" class="headerlink" title="3.1 安装格式化工具"></a>3.1 安装格式化工具</h4><p>neoformat本 身不会安装格式化工具，它只会调用系统已经安装好的格式化工具来进行代码格式化，所以你还需要自己手动在系统上安装格式化工具。</p><p>以 Python 格式化为例，我们采用 black 来格式化代码，那么需要先用<code>pip</code> 命令来安装black:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install black</span><br></pre></td></tr></table></figure><p>然后需要确保在命令行执行<code>black --version</code> 命令能正常输出，neoformat 才能找到black:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ black --version</span><br><span class="line">black, 23.3.0 (compiled: yes)</span><br><span class="line">Python (CPython) 3.8.16</span><br></pre></td></tr></table></figure><h4 id="3-2-格式化配置"><a href="#3-2-格式化配置" class="headerlink" title="3.2 格式化配置"></a>3.2 格式化配置</h4><p>安装好以后，我们就可以在<code>~/.config/nvim/init.vim</code> 文件中进行 neoformat 配置:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">let g:neoformat_python_black = &#123;</span><br><span class="line">            \ &#x27;exe&#x27;: &#x27;black&#x27;,</span><br><span class="line">            \ &#x27;args&#x27;: [&#x27;-q&#x27;, &#x27;-&#x27;],</span><br><span class="line">            \ &#x27;stdin&#x27;: 1,</span><br><span class="line">            \ &#125;</span><br><span class="line"></span><br><span class="line">let g:neoformat_enabled_python = [&#x27;black&#x27;]</span><br></pre></td></tr></table></figure><p>这是 VimScript 的语法，<code>let g:neoformat_python_black</code> 是创建一个全局变量<code>neoformat_python_black</code>, 全局变量的特点是所有打开的窗口和缓冲区都可以访问该变量。</p><p>注意这个变量的命名方式，<code>neoformat_&lt;Language&gt;_&lt;formatter&gt;</code>，表示针对某个语言的某一个格式化工具，这个格式化工具的名字会被注册，在下面的enable语句中使用到。</p><p>全局变量的值的含义如下：</p><ul><li><code>exe</code> 表示格式化运行需要执行的程序名，就跟我们在命令行访问某个程序一样的机制，需要知道它叫什么才能来执行。</li><li><code>args</code> 表示程序执行时需要的参数。这里<code>-q</code>是black命令的参数项，表示静默执行，不打印输出；<code>-</code> 表示从标准输入读取内容来格式化</li><li><code>stdin</code>: 这个参数表示是否从标准输入来读取内容来格式化。标准输入对应的是文件的内容，除了标准输入外还有缓存区</li></ul><p>所有的可配置参数参考 <a href="https://github.com/sbdchd/neoformat#config-optional">neoformat 文档</a>。这里我们配置这几个参数项就可以了。</p><p>下面还有一条语句，创建全局变量<code>neoformat_enabled_python</code>，表示针对 Python 启用的格式化工具，这里我们使用上面创建变量后注册的<code>black</code>。</p><h4 id="3-3-执行格式化"><a href="#3-3-执行格式化" class="headerlink" title="3.3 执行格式化"></a>3.3 执行格式化</h4><p>加了上面的 VimScript 配置后，我们在编辑文件时，就可以使用 <code>:Neoformat</code> 命令来格式化代码了。</p><p>如果想要使用特定的格式化工具，可以使用<code>:Neoformat &lt;formater&gt;</code> 来操作。</p><h4 id="3-4-保存文件时自动格式化"><a href="#3-4-保存文件时自动格式化" class="headerlink" title="3.4 保存文件时自动格式化"></a>3.4 保存文件时自动格式化</h4><p>前面的配置我们还需要手动执行<code>:Neoformat</code> 命令来格式化，下面我们添加一些配置到<code>~/.config/nvim/init.vim</code>，在保存文件时自动地进行格式化。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">augroup fmt</span><br><span class="line">  autocmd!</span><br><span class="line">  autocmd BufWritePre * Neoformat</span><br><span class="line">augroup END</span><br></pre></td></tr></table></figure><p>这段代码创建了一个自动化组并命名为<code>fmt</code>，用于将一组命令放在一起，方便管理。</p><p>我们首先使用<code>autocmd!</code>清空这个自动化组中的所有自动化命令，避免影响后面的命令设置。</p><p>然后用<code>autocmd BufWritePre * Neoformat</code>来完成在写buffer之前，对所有类型的文件都执行<code>Neoformat</code>命令。<code>autocmd</code>表示这是一条自动化命令。<code>BufWritePre</code>表示是在Write Buffer之前执行的操作,<code>*</code>表示匹配任意的文件，如果是<code>*.py</code>则只匹配后缀为<code>.py</code>的文件。<code>Neoformat</code> 表示要执行的命令。</p><p>这样，在保存文件时，就可以自动执行代码格式化了。</p><h4 id="3-5-调试命令"><a href="#3-5-调试命令" class="headerlink" title="3.5 调试命令"></a>3.5 调试命令</h4><p>如果出现格式化错误，或者格式化不生效，可以设置 <code>:set verbose=1</code> 来打开 NeoVim 的 log 显示，查看报错信息。实际测试发现这个命令真的很有用，很多信息打印出来后，对于定位问题帮助很大。</p><h3 id="4-neoformat-格式化-C-C-代码"><a href="#4-neoformat-格式化-C-C-代码" class="headerlink" title="4. neoformat 格式化 C/C++ 代码"></a>4. neoformat 格式化 C/C++ 代码</h3><p>对 C/C++代码的格式化与 Python 是类似的，只不过使用的格式化工具不同而已。这里以 <a href="https://clang.llvm.org/docs/ClangFormat.html">clang-format</a> 为例，记录需要执行的步骤。</p><h4 id="4-1-安装格式化工具"><a href="#4-1-安装格式化工具" class="headerlink" title="4.1 安装格式化工具"></a>4.1 安装格式化工具</h4><p>Ubuntu:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install clang-format</span><br></pre></td></tr></table></figure><p>Mac:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install clang-format</span><br></pre></td></tr></table></figure><h4 id="4-2-格式化配置"><a href="#4-2-格式化配置" class="headerlink" title="4.2 格式化配置"></a>4.2 格式化配置</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">let g:neoformat_c_clangformat = &#123;</span><br><span class="line">            \ &#x27;exe&#x27;: &#x27;clang-format&#x27;,</span><br><span class="line">\ &#x27;args&#x27;: [&#x27;-assume-filename=%:p&#x27;],</span><br><span class="line">            \ &#x27;stdin&#x27;: 1,</span><br><span class="line">            \ &#125;</span><br><span class="line"></span><br><span class="line">let g:neoformat_enabled_c = [&#x27;clangformat&#x27;]</span><br></pre></td></tr></table></figure><p>与 Python black 的配置类似，语言修改为<code>c</code>, formatter 修改为 <code>clangformat</code>，参数有所不同，<code>-assume-filename=%:p</code> 表示将当前编辑的文件名传递给 clang-format，以便它可以正确地处理预编译指令等特殊情况。</p><h4 id="4-3-自定义格式化文件"><a href="#4-3-自定义格式化文件" class="headerlink" title="4.3 自定义格式化文件"></a>4.3 自定义格式化文件</h4><p>如果不想用默认的 clang-format 格式化配置，可以通过下面的方式来生成格式化文件，并通过<code>args</code> 参数传递给Neoformat来使用。</p><p>首先生成一个默认的配置文件，例如选择以google的风格来生成:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clang-format -style=google -dump-config &gt; /Users/name/.clang-format</span><br></pre></td></tr></table></figure><p>然后编辑生成的文件，修改为你想要的格式。例如我想修改默认的2空格缩进为4空格，那么去掉默认文件中的<code># BasedOnStyle:  Google</code>的注释，继承google风格的默认配置，删除后面所有的内容，只修改<code>IndentWidth</code> 项：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">Language:        Cpp</span><br><span class="line">BasedOnStyle:  Google</span><br><span class="line">IndentWidth:     4</span><br></pre></td></tr></table></figure><p>然后用<code>--style=/path/to/.clang-format</code>来代码规范文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">let g:neoformat_c_clangformat = &#123;</span><br><span class="line">            \ &#x27;exe&#x27;: &#x27;clang-format&#x27;,</span><br><span class="line">\ &#x27;args&#x27;: [&#x27;-assume-filename=%:p&#x27;, &#x27;--styel=/Users/name/.clang-format&#x27;],</span><br><span class="line">            \ &#x27;stdin&#x27;: 1,</span><br><span class="line">            \ &#125;</span><br><span class="line"></span><br><span class="line">let g:neoformat_enabled_c = [&#x27;clangformat&#x27;]</span><br></pre></td></tr></table></figure><h4 id="4-4-保存文件时自动格式化"><a href="#4-4-保存文件时自动格式化" class="headerlink" title="4.4 保存文件时自动格式化"></a>4.4 保存文件时自动格式化</h4><p>上面 3.4 部分的代码已经开启了保存时自动格式化代码，这里不需要额外增加配置了。</p><h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><p>总结下来，涉及到的需要增加在<code>~/.config/nvim/init.vim</code>中的代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">call plug#begin(&quot;~/.nvim/bundle&quot;)</span><br><span class="line">...</span><br><span class="line">&quot; 增加neoformat</span><br><span class="line">Plug &#x27;sbdchd/neoformat&#x27;</span><br><span class="line">...</span><br><span class="line">call plug#end()</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">&quot; code format</span><br><span class="line">augroup fmt</span><br><span class="line">  autocmd!</span><br><span class="line">&quot;  autocmd BufWritePre * undojoin | Neoformat</span><br><span class="line">  autocmd BufWritePre * Neoformat</span><br><span class="line">augroup END</span><br><span class="line"></span><br><span class="line">&quot; format python</span><br><span class="line">let g:neoformat_python_black = &#123;</span><br><span class="line">            \ &#x27;exe&#x27;: &#x27;black&#x27;,</span><br><span class="line">            \ &#x27;args&#x27;: [&#x27;-q&#x27;, &#x27;-&#x27;],</span><br><span class="line">            \ &#x27;stdin&#x27;: 1,</span><br><span class="line">            \ &#125;</span><br><span class="line"></span><br><span class="line">let g:neoformat_enabled_python = [&#x27;black&#x27;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot; format c/c++</span><br><span class="line">let g:neoformat_c_clangformat = &#123;</span><br><span class="line">            \ &#x27;exe&#x27;: &#x27;clang-format&#x27;,</span><br><span class="line">\ &#x27;args&#x27;: [&#x27;-assume-filename=%:p&#x27;],</span><br><span class="line">            \ &#x27;stdin&#x27;: 1,</span><br><span class="line">            \ &#125;</span><br><span class="line"></span><br><span class="line">let g:neoformat_enabled_c = [&#x27;clangformat&#x27;]</span><br></pre></td></tr></table></figure><p>格式化工具需要单独通过命令行来安装:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install black</span><br><span class="line">brew install clang-format</span><br></pre></td></tr></table></figure><p>通过 <code>:set verbose=1</code> 来打开 log 信息，对于定位问题很有帮助。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/sbdchd/neoformat&quot;&gt;neoformat&lt;/a&gt; 是 (Neo)Vim 的代码格式化插件，支持多种语言的格式化。这篇文章覆盖 Neoformat 对 Python 和 C++ 进行格式化的配置，以及如何在保存代码时自动进行格式化，可以直接应用的配置代码段在文章最后。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="C++" scheme="http://vra.github.io/tags/C/"/>
    
    <category term="Vim" scheme="http://vra.github.io/tags/Vim/"/>
    
    <category term="NeoVim" scheme="http://vra.github.io/tags/NeoVim/"/>
    
    <category term="black" scheme="http://vra.github.io/tags/black/"/>
    
    <category term="clang-format" scheme="http://vra.github.io/tags/clang-format/"/>
    
  </entry>
  
  <entry>
    <title>homebrew禁止执行install命令时自动更新</title>
    <link href="http://vra.github.io/2023/06/08/homebrew-disable-auto-update/"/>
    <id>http://vra.github.io/2023/06/08/homebrew-disable-auto-update/</id>
    <published>2023-06-08T14:02:49.000Z</published>
    <updated>2023-06-08T14:13:49.420Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://brew.sh/">Homebrew</a> 是 macOS 下的默认的包管理器，不需要sudo权限就可以安装包，比较好用。</p><p>不过用<code>brew install</code>安装包时有个问题，它默认会先执行<code>brew update</code>来更新brew的版本。但由于brew 的源国内访问比较慢，常常<code>brew update</code>执行耗时比较久，影响每次安装包的体验。</p><p>解决办法是设置<code>HOMEBREW_NO_AUTO_UPDATE</code>环境变量为1，这样每次<code>brew install</code>时跳过更新brew的步骤，实际体验安装包速度提升明显。</p><p>可以添加下面的语句到你的.bashrc或.zshrc中，重启shell即生效:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HOMEBREW_NO_AUTO_UPDATE=1</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://brew.sh/&quot;&gt;Homebrew&lt;/a&gt; 是 macOS 下的默认的包管理器，不需要sudo权限就可以安装包，比较好用。&lt;/p&gt;
&lt;p&gt;不过用&lt;code&gt;brew install&lt;/code&gt;安装包时有个问题，它默认会先执行&lt;code&gt;</summary>
      
    
    
    
    
    <category term="Homebrew" scheme="http://vra.github.io/tags/Homebrew/"/>
    
    <category term="macOS" scheme="http://vra.github.io/tags/macOS/"/>
    
  </entry>
  
  <entry>
    <title>Python 命令补全神器 argcomplete</title>
    <link href="http://vra.github.io/2023/05/28/python-autocomplete-with-argcomplete/"/>
    <id>http://vra.github.io/2023/05/28/python-autocomplete-with-argcomplete/</id>
    <published>2023-05-28T02:33:12.000Z</published>
    <updated>2023-06-09T15:44:59.896Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>在使用Python 命令或者 Python的命令行工具的时候，一个痛点是没有补全。比如<code>python -m</code>后面输入包名字，就没有提示，每次想运行一个http server的时候，都需要搜索一下http服务的包名。另外，像pip，pipx等命令也没有提示，使用不太方便。</p><p>偶然看到<a href="https://github.com/kislyuk/argcomplete">argcomplete</a>这个库，按tab键就可以给Python的命令行添加自动补全，简直是使用Python的一个神器。</p><p>具体来说，argcomplete有下面的特点</p><ul><li>官方支持支持bash和zsh两种shell，对tcsh和fish有第三方贡献者提供的支持（不好意思Windows用户这里又被当做二等公民了😂）</li><li>可以对python命令和pip命令进行补全</li><li>其他任何以argparse解析的第三方包的命令都可以用自动补全，添加argcomplete的几行代码就行</li></ul><p>下面具体展开怎么对已有的工具启用自动补全，以及如何让自己的Python包支持argcomplete。</p><span id="more"></span><h2 id="2-对Python和pip启用自动补全"><a href="#2-对Python和pip启用自动补全" class="headerlink" title="2. 对Python和pip启用自动补全"></a>2. 对Python和pip启用自动补全</h2><p>首先通过<code>pip</code>命令来安装argcomplete:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install argcomplete</span><br></pre></td></tr></table></figure><p>然后执行下面的语句来启用对Python和pip的自动补全:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activate-global-python-argcomplete</span><br></pre></td></tr></table></figure><p>重启Shell，试试输入<code>pip</code>然后按tab，发现就会列出所有的命令选项。</p><h2 id="3-如何对别的第三方库启用自动补全"><a href="#3-如何对别的第三方库启用自动补全" class="headerlink" title="3. 如何对别的第三方库启用自动补全"></a>3. 如何对别的第三方库启用自动补全</h2><p>有些库的命令行程序是已经支持argcomplete补全，只需要用下面的命令来激活：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">eval</span> <span class="string">&quot;<span class="subst">$(register-python-argcomplete &lt;python-app-name&gt;)</span>&quot;</span></span><br></pre></td></tr></table></figure><p>例如 pipx 包安装后会在系统安装一个命令行程序pipx，且pipx已经支持argcomplete，我们就可以用下面的命令来激活自动补全:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">eval</span> <span class="string">&quot;<span class="subst">$(register-python-argcomplete pipx)</span>&quot;</span></span><br></pre></td></tr></table></figure><p>激活后输入<code>pipx in</code> 按tab键，就可以列出pipx所有以<code>in</code>开头的命令，再按tab键会在各个候选命令之间切换。</p><p>⚠️注意：这个激活命令是只对哪些代码中已经支持了argcomplete语句的程序才生效，如果代码中无这些语句，那是不生效的。</p><h2 id="4-如何让自己的Python库支持自动补全"><a href="#4-如何让自己的Python库支持自动补全" class="headerlink" title="4. 如何让自己的Python库支持自动补全"></a>4. 如何让自己的Python库支持自动补全</h2><p>只需要增加下面几行代码，就能让你的库的命令行支持自动补全:</p><pre><code class="python"># 在ArgumentParser对象初始化前增加这两行# PYTHON_ARGCOMPLETE_OKimport argcomplete, argparse# 原有代码parser = argparse.ArgumentParser()...# 在调用parse_args()函数前增加这一行argcomplete.autocomplete(parser)# 原有代码args = parser.parse_args()...</code></pre><p>然后你的包安装后，对应的命令行程序就可以用<code>eval &quot;$(register-python-argcomplete &lt;app-name&gt;)&quot;</code>来补全了。</p><p>⚠️注意：如果程序执行到<code>argcomplete.autocomplete()</code> 被调用的地方耗时很久的话，用户按tab就会有明显的延迟感。所以尽量将一些比较耗时的操作放在<code>argcomplete.autocomplete()</code> 语句后面，比如一些<code>import</code>语句，常常比较耗时，可以往后放。</p><p>希望这个程序能让你的Python开发变得舒服一些。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h2&gt;&lt;p&gt;在使用Python 命令或者 Python的命令行工具的时候，一个痛点是没有补全。比如&lt;code&gt;python -m&lt;/code&gt;后面输入包名字，就没有提示，每次想运行一个http server的时候，都需要搜索一下http服务的包名。另外，像pip，pipx等命令也没有提示，使用不太方便。&lt;/p&gt;
&lt;p&gt;偶然看到&lt;a href=&quot;https://github.com/kislyuk/argcomplete&quot;&gt;argcomplete&lt;/a&gt;这个库，按tab键就可以给Python的命令行添加自动补全，简直是使用Python的一个神器。&lt;/p&gt;
&lt;p&gt;具体来说，argcomplete有下面的特点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;官方支持支持bash和zsh两种shell，对tcsh和fish有第三方贡献者提供的支持（不好意思Windows用户这里又被当做二等公民了😂）&lt;/li&gt;
&lt;li&gt;可以对python命令和pip命令进行补全&lt;/li&gt;
&lt;li&gt;其他任何以argparse解析的第三方包的命令都可以用自动补全，添加argcomplete的几行代码就行&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面具体展开怎么对已有的工具启用自动补全，以及如何让自己的Python包支持argcomplete。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>talkGPT4All 2.0</title>
    <link href="http://vra.github.io/2023/05/27/talkgpt4all-2-0/"/>
    <id>http://vra.github.io/2023/05/27/talkgpt4all-2-0/</id>
    <published>2023-05-27T06:49:45.000Z</published>
    <updated>2023-05-27T06:58:57.916Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p><a href="https://github.com/vra/talkGPT4All">talkGPT4All</a>是基于<a href="https://gpt4all.io/index.html">GPT4All</a>的一个语音聊天程序，运行在本地CPU上，支持Linux，Mac和Windows。它利用OpenAI的Whisper模型将用户输入的语音转换为文本，再调用GPT4All的语言模型得到回答文本，最后利用文本转语音(TTS)的程序将回答文本朗读出来。</p><p>关于 talkGPT4All 1.0的介绍在<a href="https://juejin.cn/post/7217112585802498107">这篇文章</a>。</p><p>talkGPT4All 1.0的<a href="https://www.zhihu.com/zvideo/1625779747656515584">视频效果</a>。</p><p>由于GPT4All一直在迭代，相比上一篇文章发布时(2023-04-10)已经有较大的更新，今天将GPT4All的一些更新同步到talkGPT4All，由于支持的模型和运行模式都有较大的变化，因此发布 talkGPT4All 2.0。</p><p>具体来说，2.0版本相比1.0有下面的更新。</p><p>首先是GPT4All框架支持的语言模型从1个增加到8个，并且可以一键切换模型。具体的模型是</p><ul><li>  Vicuna-7B-1.1-q4_2</li><li>  Vicuna-7B-1.2-q4_2</li><li>  wizardLM-7B.q4_2</li><li>  GPT4All</li><li>  GPT4All-J</li><li>  GPT4All-J-v1.1</li><li>  GPT4All-J-v1.2</li><li>  GPT4All-J-v1.3</li></ul><p>可以看到除了GPT4All系列的模型，这个框架也支持Vicuna和Wizard的模型了。更多模型因为证书和格式的问题，还在集成中。</p><p>根据GPT4All的<a href="https://gpt4all.io/index.html">文档</a>，不同模型在benchmark上的结果：</p><p><img data-src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/988ae9ef513049d68d790e742f9e2139~tplv-k3u1fbpfcp-zoom-1.image"></p><p>可以看到GPT4All系列的模型的指标还是比较高的。</p><p>另一个重要更新是GPT4All发布了更成熟的Python包，可以直接通过pip 来安装，因此1.0中集成的不同平台不同的GPT4All二进制包也不需要了。集成PyPI包的好处多多，既可以查看源码学习内部的实现，又更方便定位问题（之前的二进制包没法调试内部代码），且保证了不同平台安装命令一致（之前不同平台二进制包不同）。</p><p>还有一个变化是GPT4All会自动按需下载模型，因此用户不需要手动下载和维护模型路径。同时将模型统一放置到<a href="https://gpt4all.io/models/">https://gpt4all.io/models/</a> 目录下，测试国内模型下载速度也很快，大家玩起来也会更舒服。</p><p>核心的更新内容就这些，下面对talkGPT4All的安装和使用进行说明，后面有空会添加一些多个语言模型效果的对比视频。</p><span id="more"></span><h3 id="2-安装与使用"><a href="#2-安装与使用" class="headerlink" title="2. 安装与使用"></a>2. 安装与使用</h3><h3 id="2-1-安装"><a href="#2-1-安装" class="headerlink" title="2.1 安装"></a>2.1 安装</h3><p>由于GPT4All, OpenAI Whisper 和TTS工具都是PyPI的包，因此所有的依赖都可以用pip 命令来安装。</p><p>流程大致上就是clone代码，创建Python虚拟环境，安装依赖，开始聊天：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/vra/talkGPT4All.git</span><br><span class="line"><span class="built_in">cd</span> talkGPT4All</span><br><span class="line">python -m venv talkgpt4all-env</span><br><span class="line"><span class="built_in">source</span> talkgpt4all-env/bin/activate</span><br><span class="line">pip install -U pip</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>如果在Linux环境下使用，还需要安装 TTS 工具 pyttsx3 的一些前置依赖，例如在Ubuntu下，可以这么安装（别的发行版切换apt 为对应的包管理命令应该就可以）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update &amp;&amp; sudo apt install -y espeak ffmpeg libespeak1</span><br></pre></td></tr></table></figure><p>依赖安装完后即刻开始聊天：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python chat.py</span><br></pre></td></tr></table></figure><p>语音输入问题，Whisper会识别语音到文字，第一次需要下载模型Whisper的模型，可能耗时会比较久。Whisper 模型默认存储地址是<code>~/.cache/whisper/</code>。</p><p>文字识别后，输入到语言模型部分后会下载语言模型文件，文件默认存储到<code>~/.cache/gpt4all</code> 目录。</p><h3 id="2-2-切换不同的LLM"><a href="#2-2-切换不同的LLM" class="headerlink" title="2.2 切换不同的LLM"></a>2.2 切换不同的LLM</h3><p>默认的语言模型是GPT4All-J-v1.3,，可以通过命令行选项–gpt-model-name来切换模型，所有的选项是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;ggml-gpt4all-j-v1.3-groovy&quot;</span></span><br><span class="line"><span class="string">&quot;ggml-gpt4all-j-v1.2-jazzy&quot;</span></span><br><span class="line"><span class="string">&quot;ggml-gpt4all-j-v1.1-breezy&quot;</span></span><br><span class="line"><span class="string">&quot;ggml-gpt4all-j&quot;</span></span><br><span class="line"><span class="string">&quot;ggml-gpt4all-l13b-snoozy&quot;</span></span><br><span class="line"><span class="string">&quot;ggml-vicuna-7b-1.1-q4_2&quot;</span></span><br><span class="line"><span class="string">&quot;ggml-vicuna-13b-1.1-q4_2&quot;</span></span><br><span class="line"><span class="string">&quot;ggml-wizardLM-7B.q4_2&quot;</span></span><br></pre></td></tr></table></figure><p>例如可以这样使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python chat.py --gpt-model-name ggml-wizardLM-7B.q4_2</span><br></pre></td></tr></table></figure><p>如果模型未下载过，会进行下载。</p><p>这里有个小问题，GPT4All工具貌似没有对模型的完整性进行校验，所以如果之前模型下载没完成就退出，再次进入后会加载不完整的文件，造成报错。所以需要手动删除不完整的文件再次完整下载后使用。</p><h3 id="2-3-切换不同大小的Whisper模型"><a href="#2-3-切换不同大小的Whisper模型" class="headerlink" title="2.3 切换不同大小的Whisper模型"></a>2.3 切换不同大小的Whisper模型</h3><p>OpenAI Whisper 也有一系列的模型，模型越大识别结果应该是越准。talkGPT4All默认使用的是base模型，也提供了命令行参数–whisper-model-type 来修改，所有的可选项是:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;tiny.en&quot;</span></span><br><span class="line"><span class="string">&quot;tiny&quot;</span></span><br><span class="line"><span class="string">&quot;base.en&quot;</span></span><br><span class="line"><span class="string">&quot;base&quot;</span></span><br><span class="line"><span class="string">&quot;small.en&quot;</span></span><br><span class="line"><span class="string">&quot;small&quot;</span></span><br><span class="line"><span class="string">&quot;medium.en&quot;</span></span><br><span class="line"><span class="string">&quot;medium&quot;</span></span><br><span class="line"><span class="string">&quot;large-v1&quot;</span></span><br><span class="line"><span class="string">&quot;large-v2&quot;</span></span><br><span class="line"><span class="string">&quot;large&quot;</span></span><br></pre></td></tr></table></figure><p>例如可以这样使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python chat.py --whisper-model-type large</span><br></pre></td></tr></table></figure><h3 id="2-4-调整声音语速"><a href="#2-4-调整声音语速" class="headerlink" title="2.4 调整声音语速"></a>2.4 调整声音语速</h3><p>talkGPT4All也提供了一个参数–voice rate 来调整 TTS发音的速度，默认是165，设置越大速度越快:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python chat.py --voice-rate 200</span><br></pre></td></tr></table></figure><h3 id="3-缺陷和改进思考"><a href="#3-缺陷和改进思考" class="headerlink" title="3. 缺陷和改进思考"></a>3. 缺陷和改进思考</h3><p>其实talkGPT4All一直以来的缺陷是比较明显的：</p><ol><li> 大模型在CPU上出词太慢</li><li> 离线的文本转语音的程序太生硬</li></ol><p>针对第一个问题，我的思考是这样，要在非Nvidia GPU设备上流畅运行基于Transformer结构的大语言模型，除了4比特量化、fp16这种 low-hang fruit外，必须要做很多底层的AI工程的优化，这个我觉得我自己是没有能力来完成的，甚至我猜测，可能GPT4All背后的Nomic AI团队也没有这方面的积累来解决这个问题。</p><p>可喜的是最近看到了<a href="https://github.com/mlc-ai/mlc-llm">MLC LLM</a>这个工作，是TVM 团队利用TVM Unity来优化语言模型，成功地将Vicuna-7B运行到了<a href="https://github.com/mlc-ai/mlc-llm/blob/main/android/README.md">Android</a>和<a href="https://github.com/mlc-ai/mlc-llm/blob/main/ios/README.md">iOS</a>手机上，我自己用小米12 Pro测试每秒能输出3～4个token，体验算是比较好的。这也是我第一次在自己手机上运行大语言模型，也意识到真正要提高大语言模型的覆盖设备，一个极致优化的底层AI工具是必不可少的。</p><p>所以对talkGPT4All这个项目感兴趣的朋友也可以了解一下MLC LLM这个工作，我认为在未来这个项目会促进LLM的真正落地。</p><p>针对第二个问题，说实话还没有找到比较自然的离线 TTS Python工具，如果看到这篇文章的你有这方面的经验，欢迎评论交流～</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/vra/talkGPT4All&quot;&gt;talkGPT4All&lt;/a&gt;是基于&lt;a href=&quot;https://gpt4all.io/index.html&quot;&gt;GPT4All&lt;/a&gt;的一个语音聊天程序，运行在本地CPU上，支持Linux，Mac和Windows。它利用OpenAI的Whisper模型将用户输入的语音转换为文本，再调用GPT4All的语言模型得到回答文本，最后利用文本转语音(TTS)的程序将回答文本朗读出来。&lt;/p&gt;
&lt;p&gt;关于 talkGPT4All 1.0的介绍在&lt;a href=&quot;https://juejin.cn/post/7217112585802498107&quot;&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;talkGPT4All 1.0的&lt;a href=&quot;https://www.zhihu.com/zvideo/1625779747656515584&quot;&gt;视频效果&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;由于GPT4All一直在迭代，相比上一篇文章发布时(2023-04-10)已经有较大的更新，今天将GPT4All的一些更新同步到talkGPT4All，由于支持的模型和运行模式都有较大的变化，因此发布 talkGPT4All 2.0。&lt;/p&gt;
&lt;p&gt;具体来说，2.0版本相比1.0有下面的更新。&lt;/p&gt;
&lt;p&gt;首先是GPT4All框架支持的语言模型从1个增加到8个，并且可以一键切换模型。具体的模型是&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;  Vicuna-7B-1.1-q4_2&lt;/li&gt;
&lt;li&gt;  Vicuna-7B-1.2-q4_2&lt;/li&gt;
&lt;li&gt;  wizardLM-7B.q4_2&lt;/li&gt;
&lt;li&gt;  GPT4All&lt;/li&gt;
&lt;li&gt;  GPT4All-J&lt;/li&gt;
&lt;li&gt;  GPT4All-J-v1.1&lt;/li&gt;
&lt;li&gt;  GPT4All-J-v1.2&lt;/li&gt;
&lt;li&gt;  GPT4All-J-v1.3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以看到除了GPT4All系列的模型，这个框架也支持Vicuna和Wizard的模型了。更多模型因为证书和格式的问题，还在集成中。&lt;/p&gt;
&lt;p&gt;根据GPT4All的&lt;a href=&quot;https://gpt4all.io/index.html&quot;&gt;文档&lt;/a&gt;，不同模型在benchmark上的结果：&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/988ae9ef513049d68d790e742f9e2139~tplv-k3u1fbpfcp-zoom-1.image&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以看到GPT4All系列的模型的指标还是比较高的。&lt;/p&gt;
&lt;p&gt;另一个重要更新是GPT4All发布了更成熟的Python包，可以直接通过pip 来安装，因此1.0中集成的不同平台不同的GPT4All二进制包也不需要了。集成PyPI包的好处多多，既可以查看源码学习内部的实现，又更方便定位问题（之前的二进制包没法调试内部代码），且保证了不同平台安装命令一致（之前不同平台二进制包不同）。&lt;/p&gt;
&lt;p&gt;还有一个变化是GPT4All会自动按需下载模型，因此用户不需要手动下载和维护模型路径。同时将模型统一放置到&lt;a href=&quot;https://gpt4all.io/models/&quot;&gt;https://gpt4all.io/models/&lt;/a&gt; 目录下，测试国内模型下载速度也很快，大家玩起来也会更舒服。&lt;/p&gt;
&lt;p&gt;核心的更新内容就这些，下面对talkGPT4All的安装和使用进行说明，后面有空会添加一些多个语言模型效果的对比视频。&lt;/p&gt;</summary>
    
    
    
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="GPT" scheme="http://vra.github.io/tags/GPT/"/>
    
    <category term="GPT4All" scheme="http://vra.github.io/tags/GPT4All/"/>
    
  </entry>
  
</feed>
