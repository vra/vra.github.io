<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Yunfeng&#39;s Simple Blog</title>
  
  <subtitle>Love, Life, Linux</subtitle>
  <link href="http://vra.github.io/atom.xml" rel="self"/>
  
  <link href="http://vra.github.io/"/>
  <updated>2023-09-29T02:40:12.160Z</updated>
  <id>http://vra.github.io/</id>
  
  <author>
    <name>Yunfeng Wang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Code Llama 解读系列1-论文阅读</title>
    <link href="http://vra.github.io/2023/09/29/code-llama-1/"/>
    <id>http://vra.github.io/2023/09/29/code-llama-1/</id>
    <published>2023-09-29T02:17:04.000Z</published>
    <updated>2023-09-29T02:40:12.160Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Code Llama 是 Meta 基于 Llama 2 的代码生成AI模型， 在同类开源模型中取得比较好的结果。这里计划写3篇系列文章，从论文细节、代码使用、效果实测方面对 Code Llama 进行解读，欢迎关注我了解后续文章。</p></blockquote><h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h3><p>2023年8月24日，Meta 开源了基于 <a href="https://github.com/facebookresearch/llama">Llama 2</a>) 通用 LLM 的代码生成系列模型 <a href="https://github.com/facebookresearch/codellama">Code Llama</a>)，支持Python, C++, Java, PHP, TypeScript, C# 和 Bash 编程语言，而且支持学术研究和商业使用。</p><p>另外 Code Llama 官方代码只提供了一些简单的使用示例，没有提供生产环境可用的 VSCode 等 工具的插件，搜索了一下也没找到简单易用的第三方开发的插件。相信很快就会有人做出来的。如果你有看到基于 Code Llama 的 VSCode 或者 Vim 插件，欢迎评论指教。</p><span id="more"></span><p>一些链接：</p><ul><li>代码仓库: <a href="https://github.com/facebookresearch/codellama">https://github.com/facebookresearch/codellama</a></li><li>论文PDF: <a href="https://scontent-sjc3-1.xx.fbcdn.net/v/t39.2365-6/369856151_1754812304950972_1159666448927483931_n.pdf?_nc_cat=107&ccb=1-7&_nc_sid=3c67a6&_nc_ohc=wURKmnWKaloAX-JEHAz&_nc_ht=scontent-sjc3-1.xx&oh=00_AfBOeTPJWHrxyxjNs4TLPACB4M7xQIwQcM5SMRMzDo8uCg&oe=64EEAC4F">链接</a></li><li>Meta AI 博客文章：<a href="https://ai.meta.com/blog/code-llama-large-language-model-coding/">链接</a></li></ul><h3 id="2-数据说明"><a href="#2-数据说明" class="headerlink" title="2. 数据说明"></a>2. 数据说明</h3><p>这篇论文中提到了几个不同的数据，有一些数据的构造还是挺巧妙的，这里列出来，希望能给大家一些启发。</p><h4 id="2-1-2T-token-dataset"><a href="#2-1-2T-token-dataset" class="headerlink" title="2.1 2T token dataset"></a>2.1 2T token dataset</h4><p>首先是 Llama 2 的训练数据，虽然不是这篇论文的贡献，但因为 Code Llama 模型都是从 Llama 2 初始化的，所以 这些代码生成的特化模型也都是见过这些数据的，包含它们中的知识。</p><p>Llama 2 是使用 2T token 数据训练的，其中代码相关的部分有80B token，占比只有4%。</p><h4 id="2-2-500B-token-dataset"><a href="#2-2-500B-token-dataset" class="headerlink" title="2.2 500B token dataset"></a>2.2 500B token dataset</h4><p>这篇论文先是提出了通用的 500B tokens 数据集, 85% 都是关于代码的，所以可以认为这个500B 就是一个代码数据集。</p><h4 id="2-3-100B-token-Python-dataset"><a href="#2-3-100B-token-Python-dataset" class="headerlink" title="2.3 100B token Python dataset"></a>2.3 100B token Python dataset</h4><p>除了通用的 500B token 代码数据集，为了提高对 Python 代码的生成能力，论文又提出了 100B token Python dataset。</p><h4 id="2-4-RLHF-V5-dataset"><a href="#2-4-RLHF-V5-dataset" class="headerlink" title="2.4 RLHF V5 dataset"></a>2.4 RLHF V5 dataset</h4><p>这是 Llama 2 论文中使用的人工纠正的数据集，是为了让代码可以更好的对应回答提问者的指令，也为了生成更安全的代码（可以理解成对生成的代码中某些危险的代码进行过滤？）</p><h4 id="2-5-self-instruct-5B-token-dataset"><a href="#2-5-self-instruct-5B-token-dataset" class="headerlink" title="2.5 self-instruct 5B token dataset"></a>2.5 self-instruct 5B token dataset</h4><p>self-instruct 是一个生成代码数据集，通过给llama2提问代码任务，得到它的结果，作为gt。 会不会存在错误答案？这里论文设计了一个很精巧的方案来构造生成代码数据：</p><ol><li>让 LLama 2 70B 大模型设计 Easy 和 Medium 难度的编程题，每次出50道题目，要求不重复，且可以用一个单独的Python函数来实现。总共得到52000个去重的问题。下面是提示词和一些回答的例子：<br><img data-src="/imgs/code_llama_1/20230826223518.png"></li><li>对上面得到的每个问题，用 Code Llama 7B 模型生成 单元测试代码和10个解题的代码，然后在解题代码上运行单元测试，将第一个通过单元测试的代码加入到 self-instruct 数据集中。</li></ol><p>这是一种很巧妙的设计，通过单元测试来判断代码的对错，能够做到完全自动化地构造数据。</p><p>当然如果单元测试代码本身错，那可能会将错误的解题代码加入到训练集中。而根据<a href="https://arxiv.org/abs/2308.02312">这篇论文</a>)的分析，作为最强的 LLM，ChatGPT 生成的代码错误率为52%。所以有理由认为  Code Llama 7B 生成的单元测试代码也会有错误，因此 self-instruct 不是一个完美的数据集。</p><h3 id="3-训练策略"><a href="#3-训练策略" class="headerlink" title="3. 训练策略"></a>3. 训练策略</h3><p>论文中针对不同的模型，尝试了不同的训练策略，整体来说是和数据集比较匹配的。</p><h4 id="3-1-从头训练-vs-Finetune"><a href="#3-1-从头训练-vs-Finetune" class="headerlink" title="3.1 从头训练 vs Finetune"></a>3.1 从头训练 vs Finetune</h4><p>论文中实验发现，采用通用llm (Llama 2)初始化，再在code数据集上finetune比在code数据集上从头训练效果要好。但同时也发现，只使用 Llama 2 模型来做代码生成，效果比 Llama 2 + Code 数据集训练要差，可见 2T token  pretrain + 500B token finetune才是做通用代码生成的最好选择。</p><h4 id="3-2-代码补全功能"><a href="#3-2-代码补全功能" class="headerlink" title="3.2 代码补全功能"></a>3.2 代码补全功能</h4><p>上面的基本训练策略中只会给定前面的代码，补全或预测后面的代码，但在有些常见，是已知前面和后面的代码，给出中间的代码，比如docstring的生成，就需要知道前面的内容(函数的名字和参数)和后面的内容（函数的具体实现），才能给出比较准确的函数说明docstring。这种任务模式论文中称之为补全 (Infilling)。</p><p>这种需求跟 LLM 预测下一个 token 的任务模式是不同的，因此需要对训练模式进行改造。总体来说，论文采用了 Casual Mask 的模式来训练网络，也就是将训练序列中间的一部分移动到最后，让网络来预测这部分内容。具体来说，将训练中的token分割为前缀、中间部分和后缀部分，分割位置利用均匀分布来确定。训练时以一半的概率喂前缀-后缀-中间（PSM）格式 token 序列，一半的概率喂后缀-前缀-中间（SPM）格式的 token 序列。</p><h4 id="3-3-长上下文输入微调"><a href="#3-3-长上下文输入微调" class="headerlink" title="3.3 长上下文输入微调"></a>3.3 长上下文输入微调</h4><p>Llama 2 模型的最长 token 数目为4096，对于代码生成任务来说，还是比较小，比如分析整个仓库中的代码，可能很容易超出限制。因此 Code Llama 在 finetune 阶段将 token 数从4096 提升到16384，提升了4倍。</p><p>位置embedding 采用旋转位置embedding, query 和 key vector都是 Rxn的一个线性组合，而R是一个块对角矩阵，也就是只有对角线和附近的4个值非零，每个位置i处的R公式如下：<br><img data-src="/imgs/code_llama_1/20230827002151.png"></p><p><img data-src="/imgs/code_llama_1/20230827002347.png"><br>d为总的token 维度。</p><h4 id="3-4-指令finetune"><a href="#3-4-指令finetune" class="headerlink" title="3.4 指令finetune"></a>3.4 指令finetune</h4><p>这部分也是为了生成更安全的代码，也更好地针对提问者的问题进行更人性化的回答。个人理解，这部分本身在策略上没有太多trick，核心是数据的构造和采集。</p><h3 id="4-模型说明"><a href="#4-模型说明" class="headerlink" title="4. 模型说明"></a>4. 模型说明</h3><p>基于几种数据和几个训练策略，就能得到不同的模型。<br>Code Llama 系列包含三大类模型，每类模型包含 7B, 13B 和 34B 三种参数大小，共9个模型。</p><p><img data-src="/imgs/code_llama_1/20230826222457.png"></p><p>第一类是Code Llama 通用代码生成模型，采用 Llama 2 的模型参数初始化，在 500B token 数据集上训练。其中 7B 和 13B 模型还进行了代码补全数据集上的训练，适用于 IDE 中实时的代码补全，而 34B 因为速度问题，并不适合实时补全，更适合作为编程助手。</p><p>第二类是 Code Llama-Python，这是针对 Python 专门优化的模型，在 500B 通用数据训练的基础上，又在额外的 100B Python 数据集上进行了finetune。</p><p>第三类是 Code. Llama-Instruct，在 Code Llama 通用模型基础上，增加了在 RLHF V5 和 self-instruct 数据集上的 finetune 过程，可以生成更符合指令需求的代码。</p><h3 id="5-结果对比"><a href="#5-结果对比" class="headerlink" title="5. 结果对比"></a>5. 结果对比</h3><p>论文中比较了非常多测试集上的指标，太多反而不知道模型的效果到底怎么样，所以这里也不列出来了。下面放的是博客文章中的和别的模型的对比表格，反而比较简洁，可以做一个大致的对比。</p><p><img data-src="/imgs/code_llama_1/20230826213734.png"></p><p>当然根据我之前的 LLMs 使用经验，实际使用时的智能感受貌似不能很好地和 Benchmark 上的结果对应起来，相差几个点对最终结果的提升有多大不太好说。 所以 Code Llama 具体使用体验如何，留待下一篇文章来分析。</p>]]></content>
    
    
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;Code Llama 是 Meta 基于 Llama 2 的代码生成AI模型， 在同类开源模型中取得比较好的结果。这里计划写3篇系列文章，从论文细节、代码使用、效果实测方面对 Code Llama 进行解读，欢迎关注我了解后续文章。&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h3 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1. 背景&quot;&gt;&lt;/a&gt;1. 背景&lt;/h3&gt;&lt;p&gt;2023年8月24日，Meta 开源了基于 &lt;a href=&quot;https://github.com/facebookresearch/llama&quot;&gt;Llama 2&lt;/a&gt;) 通用 LLM 的代码生成系列模型 &lt;a href=&quot;https://github.com/facebookresearch/codellama&quot;&gt;Code Llama&lt;/a&gt;)，支持Python, C++, Java, PHP, TypeScript, C# 和 Bash 编程语言，而且支持学术研究和商业使用。&lt;/p&gt;
&lt;p&gt;另外 Code Llama 官方代码只提供了一些简单的使用示例，没有提供生产环境可用的 VSCode 等 工具的插件，搜索了一下也没找到简单易用的第三方开发的插件。相信很快就会有人做出来的。如果你有看到基于 Code Llama 的 VSCode 或者 Vim 插件，欢迎评论指教。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="Deep Learning" scheme="http://vra.github.io/tags/Deep-Learning/"/>
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="Code Llama" scheme="http://vra.github.io/tags/Code-Llama/"/>
    
  </entry>
  
  <entry>
    <title>FastViT 论文阅读</title>
    <link href="http://vra.github.io/2023/09/01/fastvit/"/>
    <id>http://vra.github.io/2023/09/01/fastvit/</id>
    <published>2023-09-01T07:56:52.000Z</published>
    <updated>2023-09-29T02:15:14.986Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p>论文地址：<a href="https://arxiv.org/abs/2303.14189">arxiv</a><br>代码地址：<a href="https://github.com/apple/ml-fastvit">ml-fastvit</a></p><p>FastViT 是苹果公司在 ICCV 2023上发表的网络结构设计的论文，在速度和精度上取得比较好的折衷，速度上既能和MobileOne这种轻量级网络匹敌，精度上也不输PoolFormer、ConvNeXt等比较新的大网络结构。</p><span id="more"></span><p><img data-src="/imgs/fastvit/20230901135142.png"></p><p>这是网络整体的结构图：<br><img data-src="/imgs/fastvit/20230901155048.png"><br>整体还是分成Stem和4个Stage，以及最后的输出Head。可以看到所有结构都在推理时进行了重参数化，保证只有一个网络分支。虽然叫ViT，但网络的核心还是由Conv层组成。</p><p>整个网络的的大部分模块是以MobileOne 的核心 MobileOneBlock 打底的，所以说是 MobileOne V2 也不为过。</p><p>比较有意思的是，FastVit 这篇论文的作者列表、作者顺序都和 MobileOne 一模一样！<br><img data-src="/imgs/fastvit/20230901154317.png"><br><img data-src="/imgs/fastvit/20230901154255.png"></p><p>所以可以说，FastViT 是 MobileOne 框架的延续，核心是在推理的时候保证只有一条网络分支，提升网络的推理速度。</p><p>具体来说，为了提升效果，网络设计上参考了比较新的 ConvMixer 结构。然后为了保证能够重参数化，将其中的非线性层省略掉，去掉残差模块。为了缓解 Self-Attention 模块计算量太大的问题，在浅层特征图比较大的情况下，采用 Large Kernel，也就是7x7 Kernel Size 的Conv网络。</p><p>下面依次对网络的几个核心模块进行说明。</p><h3 id="2-RepMixer"><a href="#2-RepMixer" class="headerlink" title="2. RepMixer"></a>2. RepMixer</h3><p>ConvMixer 提出了用Conv网络替代ViT网络的方法，在效果上超越了ViT方法。</p><p>已有的一些方法已经验证，Skip-Connection因为会有额外的内存访问开销，因此会显著增加网络延迟，如果能合并Skip-Connection，对于网络的加速会有很帮助。注意论文中的Skip-Connection其实指的是类似残差模块中的两个分支相加的操作（如下图），而不是更常见的Encoder和Decoder之间的跳层连接。<br><img data-src="/imgs/fastvit/20230901143032.png"></p><p>FastViT利用了 ConvMixer 网络结构优异的性能，同时为了能够在推理时进行重参数化，对 ConvMixer 进行了几个修改：</p><ol><li>去掉非线性层，否则没法进行重参数化</li><li>将BN放在DepthWiseConv之前</li><li>在推理时合并 Skip-Connection，用来加速推理。</li></ol><p>具体代码实现时，训练时采用了2个MobileOneBlock，分别表示mixer和normal，与原始输入x相加；推理的时候去掉残差相加，直接转换为一个MobileOne模块：<br><img data-src="/imgs/fastvit/20230901144427.png"></p><h3 id="3-训练时过参数化"><a href="#3-训练时过参数化" class="headerlink" title="3. 训练时过参数化"></a>3. 训练时过参数化</h3><p>过参数化是指训练的时候将结构相同的网络模块重复多遍，通过增加模型的复杂度来提点。在推理的时候，再通过重参数化trick将多个分支的结构合并到一个分支来提速。下面是过参数化的示意图（图片来自<a href="https://zhuanlan.zhihu.com/p/560894077">这里</a>):<br><img data-src="/imgs/fastvit/20230901150703.png"></p><p>MobileOne 论文中就采用了过参数模块，验证可以提高网络的学习能力。</p><p>在这篇论文中，为了提速，先是将普通的 KxK 的Conv修改为DepthWise KxK 的 Conv + 1x1 PointWise 的 Conv层，发现在提速后精度下降，例如论文中 Table 1 所示，这步修改后耗时从 1.58ms 下降到 1.26ms，但精度也从78.5下降到78.0:<br><img data-src="/imgs/fastvit/20230901151134.png"></p><p>为了弥补这一步造成的精度损失，作者叠加了上面提到的训练时重参数化的trick，保证速度不变的情况下，效果超过了之前的方法，从78.0上升到78.9。</p><p>当然这部分的结构优化其实比较”水”，是现有的两个工作的简单组合……</p><h3 id="4-Large-Kernel"><a href="#4-Large-Kernel" class="headerlink" title="4. Large Kernel"></a>4. Large Kernel</h3><p>由于Transformer结构的核心模块是Self-Attention模块，而且已经被无数实验验证具有强大的特征提取能力。<br>但Self-Attention的计算量很大，要做到手机上实时难度不小。</p><p>作者认为，Self-Attention 效果好跟它有很大的感受野有关系。而普通 Conv 层通过增加 Kernel. Size，也能达到提高感受野的效果。</p><p>因此最终网络结构设计上，在每个Stage开始的时候，采用 7x7 的 MobileOneBlock。7x7 的 Kernel Size 也是通过实验试出来的。</p><p>为了既能跟MobileOne这种轻量级网络对比，又能在 ImageNet 上和别的模型一较高下，论文中提出了7个 Fast-ViT的变种，各个变种的设置如下：<br><img data-src="/imgs/fastvit/20230901153635.png"></p><h3 id="5-实验"><a href="#5-实验" class="headerlink" title="5. 实验"></a>5. 实验</h3><p>对比实验在 ImageNet-1K 分类任务、COCO 物体检测，ADE20K 语义分割等标准任务上进行了对比</p><p><img data-src="/imgs/fastvit/20230901153843.png"><br><img data-src="/imgs/fastvit/20230901153923.png"><br><img data-src="/imgs/fastvit/20230901153936.png"><br>另外这篇论文还比较了FastVit在3D手重建这个下游任务上的效果，也是比MobRecon这些端侧实时的方法效果更好，当然还是刷不过MeshGraphormer等基于HRNet Backbone的模型。<br><img data-src="/imgs/fastvit/20230901135311.png"></p><h3 id="6-总结"><a href="#6-总结" class="headerlink" title="6. 总结"></a>6. 总结</h3><p>整个论文是比较实用的，没有太多自己的原创性的点子，更多的是将一些现有的网络结构设计思想融合进MobileOne的推理时单分支的网络结构中来。</p><p>另外一个值得注意的事情是，论文中给出的Mobile Latency都很低，像 FastVit-MA36 7.9G 的FLOPS，移动端延迟4.5毫秒。但要明白这是用iPhone 12 Pro Max上使用CoreML来测试的，本身iPhone 12 Pro Max 采用的A14芯片很强，而且CoreML针对苹果的硬件有专门的优化，所以在安卓机器或者低端一些的iPhone 上，采用别的推理引擎（如ONNX， MNN， TCNN）进行推理时，很有可能达不到这么高的速度，所以像 FastVit-MA36这种FLOPS 约为8G的模型在手机上用起来还是需要验证的。</p><p>总之对于想试用 FastViT 的小伙伴来说，用就完了，代码已经开源，也不存在复现的问题，直接用起来，好用就加入到自己的任务中，效果比较差或者速度有瓶颈抛弃即可。</p><p>另外 FastViT 的代码实现很简洁优雅，阅读起来很舒服，后面有空可以写一篇代码阅读的文章，欢迎感兴趣的小伙伴关注、点赞和评论区留言～</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h3&gt;&lt;p&gt;论文地址：&lt;a href=&quot;https://arxiv.org/abs/2303.14189&quot;&gt;arxiv&lt;/a&gt;&lt;br&gt;代码地址：&lt;a href=&quot;https://github.com/apple/ml-fastvit&quot;&gt;ml-fastvit&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;FastViT 是苹果公司在 ICCV 2023上发表的网络结构设计的论文，在速度和精度上取得比较好的折衷，速度上既能和MobileOne这种轻量级网络匹敌，精度上也不输PoolFormer、ConvNeXt等比较新的大网络结构。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Deep Learning" scheme="http://vra.github.io/tags/Deep-Learning/"/>
    
    <category term="Pytorch" scheme="http://vra.github.io/tags/Pytorch/"/>
    
    <category term="论文阅读" scheme="http://vra.github.io/tags/%E8%AE%BA%E6%96%87%E9%98%85%E8%AF%BB/"/>
    
  </entry>
  
  <entry>
    <title>一个简单好用的Python并行函数</title>
    <link href="http://vra.github.io/2023/08/12/python-parallel-function/"/>
    <id>http://vra.github.io/2023/08/12/python-parallel-function/</id>
    <published>2023-08-12T15:43:21.000Z</published>
    <updated>2023-08-12T15:44:23.350Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-背景"><a href="#1-背景" class="headerlink" title="1. 背景"></a>1. 背景</h3><p>用Python跑有大量数据的任务的时候，启用多进程加速效果明显。但因为我之前在使用Python的多进程库时总遇到卡住的问题，后来对这块避而远之，总是用别的方法来加速。最近发现OpenMMLab的一些库提供了多进程并行的函数功能，简单好用。比如一个简单的toy例子，OpenCV读图像，resize然后保存，在8个CPU核的 Mac 上，加速比能达到3.4倍(45ms vs 13ms)，也就是以前要跑3个多小时的任务，现在1个小时就能搞定，省了不少时间，更多实际例子也证明了这个函数的加速效果，还是挺实用的。这里写个教程，希望也能方便到别的有同样需要的人，当然同类型的库应该也有很多，这里只是取一瓢饮。</p><span id="more"></span><h3 id="2-函数实现"><a href="#2-函数实现" class="headerlink" title="2. 函数实现"></a>2. 函数实现</h3><p>具体实现是<a href="https://github.com/open-mmlab/mmengine">mmengine</a>中的<a href="https://github.com/open-mmlab/mmengine/blob/main/mmengine/utils/progressbar.py#L109">track_parallel_progress</a>函数，它底层也是调用了Python系统库的<a href="https://docs.python.org/3/library/multiprocessing.html">multiprocessing</a>，进行多进程加速脚本的运行。所以原理上来说我们也可以不用这个函数，自己写multiprocessing调用代码。但mmengine的这个封装，给我们省去了写multiprocessing比较复杂的调度代码的时间，拿来直接用还是能加速代码的开发节奏。</p><p>大致的调用框架:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="keyword">import</span> mmengine</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mmengine_track_func</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="comment"># wraps的作用是将装饰器的信息都传递给被装饰的函数，</span></span><br><span class="line">    <span class="comment"># 参考：https://stackoverflow.com/a/309000</span></span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapped_func</span>(<span class="params">args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> func(*args)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapped_func</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@mmengine_track_func</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">your_func</span>(<span class="params">arg1, arg2</span>):</span></span><br><span class="line">    <span class="comment"># your code here</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line"><span class="comment"># 进程数</span></span><br><span class="line">NUM_PROC = <span class="number">8</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构造调用参数</span></span><br><span class="line">params = [(arg1, arg2) <span class="keyword">for</span> arg1, arg2 <span class="keyword">in</span> <span class="built_in">zip</span>(arg1_list, arg2_list)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用mmengine封装好的多进程函数</span></span><br><span class="line">results = mmengine.track_parallel_progress(your_func, params, nproc=NUM_PROC)</span><br></pre></td></tr></table></figure><p>使用时需要先 <code>pip install mmengine</code>来安装依赖库 mmengine。</p><p>然后这里构造了一个装饰器<code>mmengine_track_func</code>，对实际调用的函数<code>your_func</code>进行封装。其中用到了functools中的wraps函数，它的作用是将装饰器的信息都传递给被装饰的函数，具体例子可以参考这个<a href="https://stackoverflow.com/a/309000">回答</a>。</p><p>实际使用时<code>mmengine_track_func</code> 不需要修改，直接采用这种形式。</p><p>然后是设置进程数，构造你自己函数的参数，再调用<code>mmengine.track_parallel_progress</code> 即可，它的必需的三个参数分别是:</p><ol><li>你的函数名</li><li>函数参数list</li><li>设置的进程数</li></ol><p>别的非必需参数可以参考<a href="https://github.com/open-mmlab/mmengine/blob/main/mmengine/utils/progressbar.py#L109">源码</a>。</p><h3 id="3-toy-例子"><a href="#3-toy-例子" class="headerlink" title="3. toy 例子"></a>3. toy 例子</h3><p>这里举一个简单的伪造例子，读取本地某个目录下的png图像，将它们都缩放到200x200，再保存到本地。完整代码如下:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> mmengine</span><br><span class="line"><span class="keyword">from</span> tqdm <span class="keyword">import</span> tqdm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mmengine_track_func</span>(<span class="params">func</span>):</span></span><br><span class="line">    <span class="comment"># wraps的作用是将装饰器的信息都传递给被装饰的函数，</span></span><br><span class="line">    <span class="comment"># 参考：https://stackoverflow.com/a/309000</span></span><br><span class="line"><span class="meta">    @wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">wrapped_func</span>(<span class="params">args</span>):</span></span><br><span class="line">        <span class="keyword">return</span> func(*args)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> wrapped_func</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">@mmengine_track_func</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">run</span>(<span class="params">idx, img_path</span>):</span></span><br><span class="line">    img = cv2.imread(img_path)</span><br><span class="line">    img = cv2.resize(img, (<span class="number">200</span>, <span class="number">200</span>))</span><br><span class="line"></span><br><span class="line">    op = <span class="string">f&quot;<span class="subst">&#123;idx&#125;</span>.jpg&quot;</span></span><br><span class="line">    cv2.imwrite(op, img)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    <span class="comment"># 获取所有图片路径</span></span><br><span class="line">    img_paths = glob.glob(<span class="string">&quot;/path/to/folder/*.png&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试开多线程版本，耗时 13ms</span></span><br><span class="line">    params = [(idx, img_path) <span class="keyword">for</span> idx, img_path <span class="keyword">in</span> <span class="built_in">enumerate</span>(img_paths)]</span><br><span class="line">    mmengine.track_parallel_progress(run, params, nproc=<span class="number">8</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 测试不开多线程版本，耗时45ms</span></span><br><span class="line">    t0 = time.time()</span><br><span class="line">    <span class="keyword">for</span> idx, ip <span class="keyword">in</span> tqdm(<span class="built_in">enumerate</span>(img_paths)):</span><br><span class="line">        run.__wrapped__(idx, ip)</span><br><span class="line">    t1 = time.time()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;time:&quot;</span>, t1 - t0)</span><br></pre></td></tr></table></figure><p>这里有一个小的Python知识点：可以通过<code>func.__wrapped__</code> 属性来获取 <em>被装饰的函数</em> 对应的原始函数。</p><p>输出结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;] 4000/4000, 316.3 task/s, elapsed: 13s, ETA:     0s</span><br><span class="line">4000it [00:45, 88.84it/s]</span><br><span class="line">time: 45.0268120765686</span><br></pre></td></tr></table></figure><p>可以看到耗时从45ms下降到13ms，加速比3.4倍。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-背景&quot;&gt;&lt;a href=&quot;#1-背景&quot; class=&quot;headerlink&quot; title=&quot;1. 背景&quot;&gt;&lt;/a&gt;1. 背景&lt;/h3&gt;&lt;p&gt;用Python跑有大量数据的任务的时候，启用多进程加速效果明显。但因为我之前在使用Python的多进程库时总遇到卡住的问题，后来对这块避而远之，总是用别的方法来加速。最近发现OpenMMLab的一些库提供了多进程并行的函数功能，简单好用。比如一个简单的toy例子，OpenCV读图像，resize然后保存，在8个CPU核的 Mac 上，加速比能达到3.4倍(45ms vs 13ms)，也就是以前要跑3个多小时的任务，现在1个小时就能搞定，省了不少时间，更多实际例子也证明了这个函数的加速效果，还是挺实用的。这里写个教程，希望也能方便到别的有同样需要的人，当然同类型的库应该也有很多，这里只是取一瓢饮。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="mmengine" scheme="http://vra.github.io/tags/mmengine/"/>
    
  </entry>
  
  <entry>
    <title>git clean 教程</title>
    <link href="http://vra.github.io/2023/07/30/git-clean-tutorial/"/>
    <id>http://vra.github.io/2023/07/30/git-clean-tutorial/</id>
    <published>2023-07-29T16:21:01.000Z</published>
    <updated>2023-08-09T07:08:19.177Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-引入"><a href="#1-引入" class="headerlink" title="1. 引入"></a>1. 引入</h3><p>git clean 是用来删除 git 仓库中没有被跟踪的文件的命令，在想要快速清理 git 仓库（比如，删除仓库中所有没有跟踪的文件，清除编译生成的临时文件）时很有用。是相比别的git子命令， git clean的配置选项比较少，使用起来简单一些，这里写一个简要教程。<br>友情提示：git clean真的会删除文件，而且没法用git命令来恢复（因为没有被 git 跟踪），所以使用git clean前务必慎重，建议每次删除文件之前先加<code>--dry-run</code> 选项来验证会删除哪些文件，确保没有误删。</p><span id="more"></span><h3 id="2-git-clean-选项的含义"><a href="#2-git-clean-选项的含义" class="headerlink" title="2. git clean 选项的含义"></a>2. git clean 选项的含义</h3><p>先创建一个简单的git 仓库环境来比较清晰地展示各个选项的效果:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">mkdir /tmp/git_clean_demo</span><br><span class="line"><span class="built_in">cd</span> /tmp/git_clean_demo</span><br><span class="line">git init</span><br><span class="line">touch a.py b.py</span><br><span class="line">git add a.py</span><br><span class="line">mkdir -p folder0/folder00</span><br><span class="line">mkdir -p folder0/folder01</span><br><span class="line">touch folder0/folder0.py</span><br><span class="line">touch folder0/folder00/folder00.py</span><br><span class="line">touch folder0/folder01/folder01.py</span><br><span class="line">git add folder0/folder0.py</span><br><span class="line">git add folder0/folder00/</span><br><span class="line">touch folder0/folder00/folder00_v2.py</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;*.pyc&quot;</span> &gt;&gt; .gitignore</span><br><span class="line">touch a.pyc</span><br><span class="line">git add .gitignore</span><br></pre></td></tr></table></figure><p>用<code>git status</code> 查看一下文件跟踪状态：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">On branch main</span><br><span class="line"></span><br><span class="line">No commits yet</span><br><span class="line"></span><br><span class="line">Changes to be committed:</span><br><span class="line">  (use <span class="string">&quot;git rm --cached &lt;file&gt;...&quot;</span> to unstage)</span><br><span class="line">        new file:   a.py</span><br><span class="line">        new file:   folder0/folder0.py</span><br><span class="line">        new file:   folder0/folder00/folder00.py</span><br><span class="line"></span><br><span class="line">Untracked files:</span><br><span class="line">  (use <span class="string">&quot;git add &lt;file&gt;...&quot;</span> to include <span class="keyword">in</span> what will be committed)</span><br><span class="line">        b.py</span><br><span class="line">        folder0/folder00/folder00_v2.py</span><br><span class="line">        folder0/folder01/</span><br></pre></td></tr></table></figure><p>在 Git 的800多个配置选项中，只有一项是关于<code>git clean</code> 命令的：<code>clean.requireForce</code>。这个选项的意思是，使用<code>git clean</code> 时，必须加<code>-f</code>或者<code>--force</code> 参数才能删除文件，否则并不会删除文件，执行时会提示下面信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git clean</span><br><span class="line">fatal: clean.requireForce defaults to <span class="literal">true</span> and neither -i, -n, nor -f given; refusing to clean</span><br></pre></td></tr></table></figure><p>这是一个很好的保护文件不被轻易删除的选项，建议不要修改默认值。</p><p>所以 <code>-f/--force</code>的选项的含义就是强制删除，实际删除文件时必带此选项。</p><p><code>-n/--dry-run</code>表示不实际删除任何东西，只是空跑一下，用来看哪些文件会被删除掉。对于这种破坏性的命令，增加<code>--dry-run</code>选项真的是一个非常好的设定。</p><p>另一个很重要的选项是<code>-d</code>，表示进入<strong>未跟踪</strong>的目录来递归删除文件。注意对已经跟踪的目录，不加<code>-d</code> 命令也会清理其中的未跟踪文件，一定注意！<br>比如刚才创建的git仓库，不加<code>-d</code> 选项删除时结果如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f --dry-run</span><br><span class="line">Would remove b.py</span><br><span class="line">Would remove folder0/folder00/folder00_v2.py</span><br></pre></td></tr></table></figure><p>加了 <code>-d</code>选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d --dry-run </span><br><span class="line">Would remove b.py</span><br><span class="line">Would remove folder0/folder00/folder00_v2.py</span><br><span class="line">Would remove folder0/folder01/</span><br></pre></td></tr></table></figure><p>可以看到不管加不加<code>-d</code>，已经跟踪的目录下的未跟踪文件都会被删除；而只有加了<code>-d</code>，未跟踪的目录和下面的文件才会被删除。</p><p><code>-q/--quiet</code>表示静默操作，除了错误，别的信息不显示，实际效果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d -q --dry-run</span><br></pre></td></tr></table></figure><p>可以看到没有任何输出。</p><p><code>-i/--interactive</code> 表示交互式地删除文件，用于对文件删除进行精细操作。进入交互式界面后，又可以分按模式删除、按数字删除、每次删除前询问几种方式，具体看下面的交互式会话：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d -i --dry-run</span><br><span class="line">Would remove the following items:</span><br><span class="line">  b.py                             folder0/folder00/folder00_v2.py  folder0/folder01/</span><br><span class="line">*** Commands ***</span><br><span class="line">    1: clean                2: filter by pattern    3: select by numbers    4: ask each             5: quit                 6: <span class="built_in">help</span></span><br><span class="line">What now&gt; h</span><br><span class="line">clean               - start cleaning</span><br><span class="line">filter by pattern   - exclude items from deletion</span><br><span class="line">select by numbers   - select items to be deleted by numbers</span><br><span class="line">ask each            - confirm each deletion (like <span class="string">&quot;rm -i&quot;</span>)</span><br><span class="line">quit                - stop cleaning</span><br><span class="line"><span class="built_in">help</span>                - this screen</span><br><span class="line">?                   - <span class="built_in">help</span> <span class="keyword">for</span> prompt selection</span><br><span class="line">Would remove the following items:</span><br><span class="line">  b.py                             folder0/folder00/folder00_v2.py  folder0/folder01/</span><br><span class="line">*** Commands ***</span><br><span class="line">    1: clean                2: filter by pattern    3: select by numbers    4: ask each             5: quit                 6: <span class="built_in">help</span></span><br><span class="line">What now&gt; c</span><br><span class="line">Would remove b.py</span><br><span class="line">Would remove folder0/folder00/folder00_v2.py</span><br><span class="line">Would remove folder0/folder01/</span><br></pre></td></tr></table></figure><p>按规则忽略文件，也就是匹配到规则的图片不进行删除：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d -i --dry-run</span><br><span class="line">Would remove the following items:</span><br><span class="line">  b.py                             folder0/folder00/folder00_v2.py  folder0/folder01/</span><br><span class="line">*** Commands ***</span><br><span class="line">    1: clean                2: filter by pattern    3: select by numbers    4: ask each             5: quit                 6: <span class="built_in">help</span></span><br><span class="line">What now&gt; f</span><br><span class="line">  b.py                             folder0/folder00/folder00_v2.py  folder0/folder01/</span><br><span class="line">Input ignore patterns&gt;&gt; *folder*</span><br><span class="line">  b.py</span><br><span class="line">Input ignore patterns&gt;&gt;</span><br><span class="line">Would remove the following item:</span><br><span class="line">  b.py</span><br><span class="line">*** Commands ***</span><br><span class="line">    1: clean                2: filter by pattern    3: select by numbers    4: ask each             5: quit                 6: <span class="built_in">help</span></span><br><span class="line">What now&gt; c</span><br><span class="line">Would remove b.py</span><br></pre></td></tr></table></figure><p>按数字删除：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d -i --dry-run</span><br><span class="line">Would remove the following items:</span><br><span class="line">  b.py                             folder0/folder00/folder00_v2.py  folder0/folder01/</span><br><span class="line">*** Commands ***</span><br><span class="line">    1: clean                2: filter by pattern    3: select by numbers    4: ask each             5: quit                 6: <span class="built_in">help</span></span><br><span class="line">What now&gt; s</span><br><span class="line">    1: b.py                               2: folder0/folder00/folder00_v2.py    3: folder0/folder01/</span><br><span class="line">Select items to delete&gt;&gt; 1</span><br><span class="line">  * 1: b.py                               2: folder0/folder00/folder00_v2.py    3: folder0/folder01/</span><br><span class="line">Select items to delete&gt;&gt; 2</span><br><span class="line">  * 1: b.py                             * 2: folder0/folder00/folder00_v2.py    3: folder0/folder01/</span><br><span class="line">Select items to delete&gt;&gt; 3</span><br><span class="line">  * 1: b.py                             * 2: folder0/folder00/folder00_v2.py  * 3: folder0/folder01/</span><br><span class="line">Select items to delete&gt;&gt; 4</span><br><span class="line">Huh (4)?</span><br><span class="line">  * 1: b.py                             * 2: folder0/folder00/folder00_v2.py  * 3: folder0/folder01/</span><br><span class="line">Select items to delete&gt;&gt; 5</span><br><span class="line">Huh (5)?</span><br><span class="line">  * 1: b.py                             * 2: folder0/folder00/folder00_v2.py  * 3: folder0/folder01/</span><br><span class="line">Select items to delete&gt;&gt;</span><br><span class="line">Would remove the following items:</span><br><span class="line">  b.py                             folder0/folder00/folder00_v2.py  folder0/folder01/</span><br><span class="line">*** Commands ***</span><br><span class="line">    1: clean                2: filter by pattern    3: select by numbers    4: ask each             5: quit                 6: <span class="built_in">help</span></span><br><span class="line">What now&gt; c</span><br><span class="line">Would remove b.py</span><br><span class="line">Would remove folder0/folder00/folder00_v2.py</span><br><span class="line">Would remove folder0/folder01/</span><br></pre></td></tr></table></figure><p>注意看输入大于未跟踪文件数目的数字时的<code>Huh</code>，有点喜感。</p><p><code>-e/--exclude</code>表示删除时排除满足后面模式的文件，比如<code>-e &quot;*/&quot;</code> 表示排除所有文件夹，<code>-e &quot;*_v2.py&quot;</code>表示排除所有以<code>_v2.py</code>结尾的文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d -e <span class="string">&quot;*/&quot;</span> --dry-run</span><br><span class="line">Would remove b.py</span><br><span class="line"></span><br><span class="line">$ it clean -f -d -e <span class="string">&quot;*_v2.py&quot;</span> --dry-run</span><br><span class="line">Would remove b.py</span><br><span class="line">Would remove folder0/folder01</span><br></pre></td></tr></table></figure><p><code>-x</code>  表示不使用.gitignore中的规则。如果不加这个选项，默认会跳过.gitignore 规则中的文件，启用这个选项后会将<code>.gitignore</code> 中的文件也删除，比如创建示例仓库时我们忽略了<code>*.pyc</code>，前面的结果中都没跳过了这一类文件，加了<code>-x</code>选项后输出如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d -x --dry-run</span><br><span class="line">Would remove a.pyc</span><br><span class="line">Would remove b.py</span><br><span class="line">Would remove folder0/folder00/folder00_v2.py</span><br><span class="line">Would remove folder0/folder01/</span><br></pre></td></tr></table></figure><p><code>a.pyc</code> 也被删除掉了。</p><p><code>-X</code>选项（大写的X）与<code>-x</code> 相反，只删除满足<code>.gitignore</code> 中规则的文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ git clean -f -d -X --dry-run</span><br><span class="line">Would remove a.pyc</span><br></pre></td></tr></table></figure><h3 id="3-总结"><a href="#3-总结" class="headerlink" title="3. 总结"></a>3. 总结</h3><p>实际实践中，我对<code>git clean</code> 用的还不多(严格来说正经使用只用过一次)，本文中如果错误，欢迎批评指正。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-引入&quot;&gt;&lt;a href=&quot;#1-引入&quot; class=&quot;headerlink&quot; title=&quot;1. 引入&quot;&gt;&lt;/a&gt;1. 引入&lt;/h3&gt;&lt;p&gt;git clean 是用来删除 git 仓库中没有被跟踪的文件的命令，在想要快速清理 git 仓库（比如，删除仓库中所有没有跟踪的文件，清除编译生成的临时文件）时很有用。是相比别的git子命令， git clean的配置选项比较少，使用起来简单一些，这里写一个简要教程。&lt;br&gt;友情提示：git clean真的会删除文件，而且没法用git命令来恢复（因为没有被 git 跟踪），所以使用git clean前务必慎重，建议每次删除文件之前先加&lt;code&gt;--dry-run&lt;/code&gt; 选项来验证会删除哪些文件，确保没有误删。&lt;/p&gt;</summary>
    
    
    
    
    <category term="总结" scheme="http://vra.github.io/tags/%E6%80%BB%E7%BB%93/"/>
    
    <category term="Git" scheme="http://vra.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>dinov2_retrieval:一个基于DINOv2 的图片检索应用</title>
    <link href="http://vra.github.io/2023/07/14/dinov2-retrieval/"/>
    <id>http://vra.github.io/2023/07/14/dinov2-retrieval/</id>
    <published>2023-07-13T16:05:35.000Z</published>
    <updated>2023-07-13T16:16:35.363Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p>前些天 Meta 公司发布了 <a href="https://github.com/facebookresearch/dinov2">DINOv2</a> 视觉预训练模型。DINOv2 能够高效地提出图像中的特征，提取的特征可以直接用于像分类等任务，而且只需要一个简单的线性层就能取得比较好的结果。</p><p>为了展示 DINOv2 强大的特征提取能力， Meta 提供了一个在线 <a href="https://dinov2.metademolab.com/">Demo</a>，上传一张图片，就能从一些艺术画作中检索出最相似的作品。</p><p>拿随手拍的照片体验后，DINOv2 特征提取能力确实强大，能够准确地理解图片中的语义信息。</p><p><img data-src="/imgs/dinov2_retrieval/dinov2_demo_result.jpeg"></p><p>由于 DINOv2 预训练模型是开源的，因为基于它来测试实际的效果是可行的。比如，我想找到相册中跟某张照片最相似的图片，就可以用 DINOv2 来测试照片和相册中所有照片的特征，然后计算相册中照片特征与测试照片最相近的那一张，就是我想要的。</p><p>整体思路是很简单直接的，经过一天的开发，终于完成了一个相对完善的Python工具 <a href="https://github.com/vra/dinov2-retrieval">dinov2_retrieval</a>，能够检索若干张图片在测试数据集中最相似的图。</p><p>写完后拿最近拍的一些随机照片跑了一下，检索结果还是挺不错的。最左边是测试图片，右边的5张图是在[Caltech 256](<a href="https://data.caltech.edu/records/nyy15-4j048">Caltech 256</a>)数据集中检索得到的top5相似的图像：<br><img data-src="/imgs/dinov2_retrieval/1688175364717_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688175364731_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688175364741_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688175364753_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688175364766_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688175364775_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688175364786_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688175364801_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688219476149_output.jpg"><br><img data-src="/imgs/dinov2_retrieval/1688219476156_output.jpg"></p><p>通过和ResNet50预训练模型提取的特征做检索对比，发现 DINOv2 提取的特征还是更准确一些，检索结果也更好。</p><p>后面部分详细说说这个工具 dinov2_retrieval 的使用。</p><span id="more"></span><h3 id="2-安装和使用"><a href="#2-安装和使用" class="headerlink" title="2. 安装和使用"></a>2. 安装和使用</h3><p>dinov2_retrieval 已经发布到 PyPI，因此可以使用pip来直接安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install dinov2_retrieval</span><br></pre></td></tr></table></figure><p>安装后在命令行执行<code>dinov2_retrieval -h</code> 来检查安装是否成功：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">dinov2_retrieval -h</span><br><span class="line">usage: dinov2_retrieval [-h] [-s &#123;small,base,large,largest&#125;] [-p MODEL_PATH] [-o OUTPUT_ROOT] -q QUERY -d DATABASE [-n NUM] [--size SIZE]</span><br><span class="line">                        [-m &#123;0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100&#125;] [--disable-cache] [-v]</span><br><span class="line"></span><br><span class="line">optional arguments:</span><br><span class="line">  -h, --<span class="built_in">help</span>            show this <span class="built_in">help</span> message and <span class="built_in">exit</span></span><br><span class="line">  -s &#123;small,base,large,largest&#125;, --model-size &#123;small,base,large,largest&#125;</span><br><span class="line">                        DinoV2 model <span class="built_in">type</span></span><br><span class="line">  -p MODEL_PATH, --model-path MODEL_PATH</span><br><span class="line">                        path to dinov2 model, useful when github is unavailable</span><br><span class="line">  -o OUTPUT_ROOT, --output-root OUTPUT_ROOT</span><br><span class="line">                        root folder to save output results</span><br><span class="line">  -q QUERY, --query QUERY</span><br><span class="line">                        path to a query image file or image folder</span><br><span class="line">  -d DATABASE, --database DATABASE</span><br><span class="line">                        path to the database image file or image folder</span><br><span class="line">  -n NUM, --num NUM     How many images to show <span class="keyword">in</span> retrieval results</span><br><span class="line">  --size SIZE           image output size</span><br><span class="line">  -m &#123;0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100&#125;, --margin &#123;0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100&#125;</span><br><span class="line">                        margin size (<span class="keyword">in</span> pixel) between concatenated images</span><br><span class="line">  --disable-cache       don<span class="string">&#x27;t cache database features, will extract features each time, quite time-consuming for large database</span></span><br><span class="line"><span class="string">  -v, --verbose         show detailed logs</span></span><br></pre></td></tr></table></figure><p>如果有上面的输出说明就安装成功了，否则就有问题，解决不了的情况下可以在<a href="https://github.com/vra/dinov2-retrieval/issues">这里</a>提交issue。</p><p>运行时一般来说只需要设置一下<code>--query</code> 和<code>--database</code> 参数，分别代表测试图像和数据集的地址。两者都可以是单张图片或者目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dinov2_retrieval -q /path/to/query/image -d /path/to/database/images</span><br></pre></td></tr></table></figure><p>检索得到的结果会保存在<code>output</code>目录下。</p><p>另外的选项含义如下：</p><ul><li>-s/–model-size: 模型大小，可以设置small，base，large或者largest</li><li>-p/–model_path: 模型缓存路径，一般是<code>$HOME/.cache/torch/hub/facebookresearch_dinov2_main</code>，对于GitHub连接不太稳定的情况使用此选项可以从本地读取模型</li><li>-o/–output-root: 输出结果的保存目录，默认是<code>output</code></li><li>-n/–num: 显示多少张最相似的图片，默认是1张</li><li>–size: 图像缩放到多大来显示，默认是224</li><li>-m/–margin: 不同图像拼接时的间距，默认10像素</li><li>–disable-cache: 禁用database特征的cache，开启后每次运行都会对database所有图像提取一遍特征，耗时大大增加</li><li>-v/–verbose: 开启debug log，会显示更多有用信息，比如图像的相似度等</li></ul><h2 id="3-思考"><a href="#3-思考" class="headerlink" title="3. 思考"></a>3. 思考</h2><p>写完这个工具后，有一点体会，检索这个任务要做出有意思的东西，还是要有足够丰富有趣的数据库。这也是一个通用的问题，现在的AI有强大的能力，但对于普通开发者来说，AI的能力用到哪里，怎么产生出有意思有意义的实际应用场景，是个值得思考的问题。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h3&gt;&lt;p&gt;前些天 Meta 公司发布了 &lt;a href=&quot;https://github.com/facebookresearch/dinov2&quot;&gt;DINOv2&lt;/a&gt; 视觉预训练模型。DINOv2 能够高效地提出图像中的特征，提取的特征可以直接用于像分类等任务，而且只需要一个简单的线性层就能取得比较好的结果。&lt;/p&gt;
&lt;p&gt;为了展示 DINOv2 强大的特征提取能力， Meta 提供了一个在线 &lt;a href=&quot;https://dinov2.metademolab.com/&quot;&gt;Demo&lt;/a&gt;，上传一张图片，就能从一些艺术画作中检索出最相似的作品。&lt;/p&gt;
&lt;p&gt;拿随手拍的照片体验后，DINOv2 特征提取能力确实强大，能够准确地理解图片中的语义信息。&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/dinov2_demo_result.jpeg&quot;&gt;&lt;/p&gt;
&lt;p&gt;由于 DINOv2 预训练模型是开源的，因为基于它来测试实际的效果是可行的。比如，我想找到相册中跟某张照片最相似的图片，就可以用 DINOv2 来测试照片和相册中所有照片的特征，然后计算相册中照片特征与测试照片最相近的那一张，就是我想要的。&lt;/p&gt;
&lt;p&gt;整体思路是很简单直接的，经过一天的开发，终于完成了一个相对完善的Python工具 &lt;a href=&quot;https://github.com/vra/dinov2-retrieval&quot;&gt;dinov2_retrieval&lt;/a&gt;，能够检索若干张图片在测试数据集中最相似的图。&lt;/p&gt;
&lt;p&gt;写完后拿最近拍的一些随机照片跑了一下，检索结果还是挺不错的。最左边是测试图片，右边的5张图是在[Caltech 256](&lt;a href=&quot;https://data.caltech.edu/records/nyy15-4j048&quot;&gt;Caltech 256&lt;/a&gt;)数据集中检索得到的top5相似的图像：&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364717_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364731_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364741_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364753_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364766_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364775_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364786_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688175364801_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688219476149_output.jpg&quot;&gt;&lt;br&gt;&lt;img data-src=&quot;/imgs/dinov2_retrieval/1688219476156_output.jpg&quot;&gt;&lt;/p&gt;
&lt;p&gt;通过和ResNet50预训练模型提取的特征做检索对比，发现 DINOv2 提取的特征还是更准确一些，检索结果也更好。&lt;/p&gt;
&lt;p&gt;后面部分详细说说这个工具 dinov2_retrieval 的使用。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="Pytorch" scheme="http://vra.github.io/tags/Pytorch/"/>
    
    <category term="pip" scheme="http://vra.github.io/tags/pip/"/>
    
  </entry>
  
  <entry>
    <title>git diff 的一个妙用</title>
    <link href="http://vra.github.io/2023/06/30/git-diff-a-special-use-case/"/>
    <id>http://vra.github.io/2023/06/30/git-diff-a-special-use-case/</id>
    <published>2023-06-30T14:41:49.000Z</published>
    <updated>2023-06-30T14:42:42.742Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-git-diff-常规用法"><a href="#1-git-diff-常规用法" class="headerlink" title="1. git diff 常规用法"></a>1. git diff 常规用法</h3><p>git diff 可以用来比较在git仓库中的两次提交或两个文件的diff，常见用法如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 显示当前代码与最新commit的代码之间的差别</span></span><br><span class="line">git diff</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示暂存（也就是已经git add 但还没有git commit）的代码提交</span></span><br><span class="line">git diff --staged</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示当前代码与&lt;commit-id&gt;时代码的区别</span></span><br><span class="line">git diff &lt;commit-id&gt; </span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示暂存代码与&lt;commit-id&gt;时代码的区别</span></span><br><span class="line">git diff --staged &lt;commit-id&gt; </span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示两次commit-id之间的代码区别</span></span><br><span class="line">git diff &lt;commit-id1&gt; &lt;commit-id2&gt;  </span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示当前分支与 branch1 分支上的代码区别</span></span><br><span class="line">git diff &lt;branch1&gt;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示两个分支上的代码之间的区别</span></span><br><span class="line">git diff &lt;branch1&gt; &lt;branch2&gt;</span><br></pre></td></tr></table></figure><p>所有上述命令后面都可以加一个目录或文件路径来只显示这个目录或文件中的区别：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git diff /path/to/folder</span><br><span class="line"></span><br><span class="line">git diff /path/to/file.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可用git的参数终止符号--，避免文件名和参数重名时将文件名解析为参数</span></span><br><span class="line">git diff --  /path/to/file.py</span><br></pre></td></tr></table></figure><span id="more"></span><h3 id="2-git-diff-妙用"><a href="#2-git-diff-妙用" class="headerlink" title="2. git diff 妙用"></a>2. git diff 妙用</h3><p>git diff 有一个选项<code>--no-index</code> ，可以用来不在git仓库中的两个文件或目录。<br><code>--no-index</code>的git帮助文档中说明如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git diff [&lt;options&gt;] --no-index [--] &lt;path&gt; &lt;path&gt;</span><br><span class="line">This form is to compare the given two paths on the filesystem. You can omit the --no-index option when running the command in a working tree controlled by Git and at least one of the paths points outside the working tree, or when running the command outside a working tree controlled by Git. This form implies --exit-code.</span><br></pre></td></tr></table></figure><p>说明它可以用来比较两个给定的路径。</p><p>那为什么要用<code>git diff</code> 来比较非git仓库里面的两个路径呢，直接用Linux和Mac上自带的<code>diff</code> 命令不好吗？</p><p><code>git diff</code> 相比<code>diff</code> 的优势是它能生成以<code>+</code> 和<code>-</code> 开头的diff结果，红色表示删去，绿色表示添加，因此能很直观地看出增加和删除了哪些地方，而diff给出来的是黑色的代码差别，展示很不直观。</p><p>另外<code>git diff</code>的结果可以写入文件，粘贴到Markdown文件中，大部分 Markdown 渲染器都能够识别diff块，比较好地渲染出diff结果。</p><p>实际操作中，需要在一个git仓库目录中来执行<code>git diff --no-index</code>,例如比较两个文件:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff --no-index ~/a.py ~/b.py</span><br></pre></td></tr></table></figure><p>比较两个目录:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git diff --no-index ~/folder-a ~/folder-b</span><br></pre></td></tr></table></figure><h3 id="One-More-Thing"><a href="#One-More-Thing" class="headerlink" title="One More Thing"></a>One More Thing</h3><p>其实我之前写过一个比较两个目录的Python工具<a href="https://github.com/vra/dompare">dompare</a>(名字含义是directory compare)，通过执行一条命令得到得到两个目录中文件的diff，并且保存到HTML网页中打开浏览器进行展示。感兴趣的小伙伴可以玩一玩。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-git-diff-常规用法&quot;&gt;&lt;a href=&quot;#1-git-diff-常规用法&quot; class=&quot;headerlink&quot; title=&quot;1. git diff 常规用法&quot;&gt;&lt;/a&gt;1. git diff 常规用法&lt;/h3&gt;&lt;p&gt;git diff 可以用来比较在git仓库中的两次提交或两个文件的diff，常见用法如下：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;16&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;17&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;18&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;19&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;20&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 显示当前代码与最新commit的代码之间的差别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 显示暂存（也就是已经git add 但还没有git commit）的代码提交&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff --staged&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 显示当前代码与&amp;lt;commit-id&amp;gt;时代码的区别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff &amp;lt;commit-id&amp;gt; &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 显示暂存代码与&amp;lt;commit-id&amp;gt;时代码的区别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff --staged &amp;lt;commit-id&amp;gt; &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 显示两次commit-id之间的代码区别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff &amp;lt;commit-id1&amp;gt; &amp;lt;commit-id2&amp;gt;  &lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 显示当前分支与 branch1 分支上的代码区别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff &amp;lt;branch1&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 显示两个分支上的代码之间的区别&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff &amp;lt;branch1&amp;gt; &amp;lt;branch2&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;所有上述命令后面都可以加一个目录或文件路径来只显示这个目录或文件中的区别：&lt;/p&gt;
&lt;figure class=&quot;highlight bash&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;git diff /path/to/folder&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff /path/to/file.py&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;# 也可用git的参数终止符号--，避免文件名和参数重名时将文件名解析为参数&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;git diff --  /path/to/file.py&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="Git" scheme="http://vra.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>C++ std::optional 使用教程</title>
    <link href="http://vra.github.io/2023/06/30/cpp-optional-tutorial/"/>
    <id>http://vra.github.io/2023/06/30/cpp-optional-tutorial/</id>
    <published>2023-06-30T14:39:41.000Z</published>
    <updated>2023-06-30T14:40:50.994Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-std-optional-是什么"><a href="#1-std-optional-是什么" class="headerlink" title="1. std::optional 是什么"></a>1. std::optional 是什么</h3><p>C++ 17 引入了std::optional，表示一个可能有值的对象（没有值时就是默认的<code>std::nullopt</code>)，例如这个例子中，std::optional 对象 even_value，如果<code>is_even</code> 为真的话就是128，否则就是默认值<code>std::nullopt</code>: </p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;optiona&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">bool</span> is_even = <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 在 没有值的情况下 std::optional 对象的值为 std::nullopt</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; even_value = is_even ? std::optional&lt;<span class="keyword">int</span>&gt;(<span class="number">128</span>) : std::nullopt;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 可以用 std::optional 对象是否等于 std::nullopt 来判断 std::optional 对象是否有值</span></span><br><span class="line"><span class="keyword">if</span> (even_value != std::nullopt) &#123;</span><br><span class="line">    <span class="comment">// 采用.value 获取 std::optional 对象的值</span></span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;has value, which is &quot;</span> &lt;&lt; even_value.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;no value&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>其实std::optional的作用和Python里面的<code>None</code>比较像，例如上面的例子用Python来写就是这样：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">is_even = <span class="literal">True</span></span><br><span class="line">even_value = <span class="number">128</span> <span class="keyword">if</span> is_even <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"><span class="keyword">if</span> even_value <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;has value, which is&quot;</span>, even_value)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;no value&quot;</span>)</span><br></pre></td></tr></table></figure><span id="more"></span><h3 id="2-为什么要引入-std-optional"><a href="#2-为什么要引入-std-optional" class="headerlink" title="2. 为什么要引入 std::optional"></a>2. 为什么要引入 std::optional</h3><p>我觉得提出std::optional就是因为C++底层缺少<code>None</code> 这个表示，所以将std::nullopt和某种特定类型的变量合并在一起构造成一个<code>std::optional</code>对象，用以解决因为缺少之前<code>None</code>因而存在的一些不怎么直接的用法。</p><p>这里举个例子来说明前面提到的”不直接”的用法。这是一个寻找数组中的第一个非0元素的函数：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">findFirstNonZero</span><span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (arr[i] != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> arr[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">-1</span>; <span class="comment">// 如果数组中没有非0元素，则返回-1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，没找到元素时返回-1，所以当拿到-1时，没法判断是第一个非0元素为-1还是没找到非0元素。<br>改进方案是返回一个pair，第一个位置表示是否包含非0元素，第二个位置表示非0元素的值：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;utility&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">std::pair&lt;<span class="keyword">bool</span>, <span class="keyword">int</span>&gt; <span class="title">findFirstNonZero</span><span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (arr[i] != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> std::<span class="built_in">make_pair</span>(<span class="literal">true</span>, arr[i]);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> std::<span class="built_in">make_pair</span>(<span class="literal">false</span>, <span class="number">-1</span>); <span class="comment">// 如果数组中没有非0元素，则返回false和-1</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>但这样其实比较繁琐且不直观，两个变量的解析和使用成本还是有些高，如果能用一个变量来完成的话就更简洁了。</p><p>采用std::optional可以简化上面的代码：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;optional&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">std::optional&lt;<span class="keyword">int</span>&gt; <span class="title">findFirstNonZero</span><span class="params">(<span class="keyword">int</span> arr[], <span class="keyword">int</span> size)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; size; i++) &#123;</span><br><span class="line">        <span class="keyword">if</span> (arr[i] != <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> arr[i];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> std::nullopt; <span class="comment">// 如果数组中没有非0元素，则返回std::nullopt</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意这里int类型的返回值可以隐式地转换为 std::optional 对象。</p><p>使用这个函数时也只需要判断一下返回值是否为<code>std::nullopt</code> 就可以。</p><p>总之可以将std::optional对象当作支持判断是否为NULL的对象的封装，在不确定对象是否存在的情况下，建议使用。</p><h3 id="3-std-optional-的构造"><a href="#3-std-optional-的构造" class="headerlink" title="3. std::optional 的构造"></a>3. std::optional 的构造</h3><p>空的 std::optional 对象可以用<code>std::nullopt</code> 或者<code>&#123;&#125;</code> 来构造，然后用<code>emplace</code> 函数来插入数值：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.0 采用 std::nullopt 初始化再调用 emplace 插入值</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val0 = std::nullopt;</span><br><span class="line">val0.<span class="built_in">emplace</span>(<span class="number">128</span>);</span><br><span class="line">std::cout &lt;&lt; val0.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.1 采用 &#123;&#125; 初始化再调用 emplace 插入值</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val1 = &#123;&#125;;</span><br><span class="line">val1.<span class="built_in">emplace</span>(<span class="number">128</span>);</span><br><span class="line">std::cout &lt;&lt; val1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure><p>每次调用<code>emplace</code> 时，会清除掉之前的值，因此可以多次调用，且能保证每次都是最新的数值。</p><p>也可以用 <code>std::make_optional</code> 函数来构造：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.7 采用 std::make_optional&lt;T&gt;(val) 初始化</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val7 = std::make_optional&lt;<span class="keyword">int</span>&gt;(<span class="number">128</span>);</span><br><span class="line">std::cout &lt;&lt; val7.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.8 采用 std::make_optional(val) 初始化，自动推导变量类型</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val8 = std::<span class="built_in">make_optional</span>(<span class="number">128</span>);</span><br><span class="line">std::cout &lt;&lt; val8.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>除此之外还有很多种初始化 std::optional 对象的方法，都写在这个示例代码里面了，记得看注释：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 1.2 采用 std::optional&lt;T&gt;(val) 初始化</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val2 = std::optional&lt;<span class="keyword">int</span>&gt;(<span class="number">128</span>);</span><br><span class="line">std::cout &lt;&lt; val2.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.3 采用 std::optional(val) 初始化，自动推导变量类型</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val3 = std::<span class="built_in">optional</span>(<span class="number">128</span>);</span><br><span class="line">std::cout &lt;&lt; val3.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.4 采用 std::optional&lt;T&gt;&#123;val&#125; 初始化</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val4 = std::optional&lt;<span class="keyword">int</span>&gt;&#123;<span class="number">128</span>&#125;;</span><br><span class="line">std::cout &lt;&lt; val4.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.5 采用 std::optional&#123;val&#125; 初始化</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val5 = std::optional&#123;<span class="number">128</span>&#125;;</span><br><span class="line">std::cout &lt;&lt; val5.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 1.6 采用 &#123;val&#125; 初始化</span></span><br><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val6 = &#123;<span class="number">128</span>&#125;;</span><br><span class="line">std::cout &lt;&lt; val6.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="4-std-optional-判断是否有值"><a href="#4-std-optional-判断是否有值" class="headerlink" title="4. std::optional 判断是否有值"></a>4. std::optional 判断是否有值</h3><p>判断 std::optional 对象是否有值可以用 <code>has_value</code>函数，或者判断是否不等于<code>std::nullopt</code>，或者直接用if语句对对象进行判断：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; result1 = <span class="built_in">find_the_first_postive_value</span>(pos_values);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (result1.<span class="built_in">has_value</span>()) &#123;</span><br><span class="line">    std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (result1 != std::nullopt) &#123;</span><br><span class="line">    std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> (result1) &#123;</span><br><span class="line">    std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="5-std-optional-获取值"><a href="#5-std-optional-获取值" class="headerlink" title="5. std::optional 获取值"></a>5. std::optional 获取值</h3><p>获取值的话可以用<code>.value()</code> 函数，或者<code>*</code> 运算符：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> (result1) &#123;</span><br><span class="line">     std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">if</span> (result1) &#123;</span><br><span class="line">     std::cout &lt;&lt; *result1 &lt;&lt; std::endl;</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><p>如果想在std::optional对象为<code>std::nullopt</code>的情况下设置默认值的话，可以用<code>value_or</code> 函数：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val9 = std::nullopt;</span><br><span class="line">std::cout &lt;&lt; val9.<span class="built_in">value_or</span>(<span class="number">-1</span>) &lt;&lt; std::endl; <span class="comment">// 输出 -1</span></span><br><span class="line">val9.<span class="built_in">emplace</span>(<span class="number">128</span>);</span><br><span class="line">std::cout &lt;&lt; val9.<span class="built_in">value_or</span>(<span class="number">-1</span>) &lt;&lt; std::endl; <span class="comment">// 输出 128</span></span><br></pre></td></tr></table></figure><p>很明显，<code>value_or</code>函数中的默认值需要和optional对象的类型一致，否则会编译报错。</p><h3 id="6-没有值时的异常处理"><a href="#6-没有值时的异常处理" class="headerlink" title="6. 没有值时的异常处理"></a>6. 没有值时的异常处理</h3><p>如果在没有值的情况下调用<code>.value</code> 函数，会在运行时报错<code>std::bad_optional_access</code>:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val10 = std::nullopt;</span><br><span class="line">std::cout &lt;&lt; val10.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">libc++abi: terminating due to uncaught exception of type std::bad_optional_access: bad_optional_access</span><br></pre></td></tr></table></figure><p>所以建议使用<code>.value_or</code>来处理，如果要强行使用<code>.value</code>的话，需要使用 try-catch 语句：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">std::optional&lt;<span class="keyword">int</span>&gt; val11 = std::nullopt;</span><br><span class="line"><span class="keyword">try</span> &#123;</span><br><span class="line">    std::cout &lt;&lt; val11.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125; <span class="built_in"><span class="keyword">catch</span></span> (<span class="keyword">const</span> std::bad_optional_access&amp; e) &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;==&gt; error: &quot;</span> &lt;&lt; e.<span class="built_in">what</span>() &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="7-示例代码"><a href="#7-示例代码" class="headerlink" title="7. 示例代码"></a>7. 示例代码</h3><p>上面的所有示例代码汇总：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;optional&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">std::optional&lt;<span class="keyword">int</span>&gt; <span class="title">find_the_first_postive_value</span><span class="params">(<span class="keyword">const</span> std::vector&lt;<span class="keyword">int</span>&gt;&amp; values)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; val : values) &#123;</span><br><span class="line">        <span class="keyword">if</span> (val &gt; <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> std::optional&lt;<span class="keyword">int</span>&gt;(val);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> std::nullopt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">std::optional&lt;<span class="keyword">int</span>&gt; <span class="title">find_the_first_postive_value_v2</span><span class="params">(<span class="keyword">const</span> std::vector&lt;<span class="keyword">int</span>&gt;&amp; values)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">auto</span> it = std::<span class="built_in">find_if</span>(values.<span class="built_in">begin</span>(), values.<span class="built_in">end</span>(), [](<span class="keyword">int</span> val) &#123; <span class="keyword">return</span> val &gt; <span class="number">0</span>; &#125;);</span><br><span class="line">    <span class="keyword">return</span> it != values.<span class="built_in">end</span>() ? std::<span class="built_in">make_optional</span>(*it) : std::nullopt;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">show_backend</span><span class="params">(std::optional&lt;std::string&gt; backend)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (backend) &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;==&gt; use set backend: &quot;</span> &lt;&lt; backend.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;==&gt; use default backend: CPU&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// std::optional 简单例子</span></span><br><span class="line">    <span class="keyword">bool</span> is_even = <span class="literal">true</span>;</span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; even_value = is_even ? std::optional&lt;<span class="keyword">int</span>&gt;(<span class="number">128</span>) : std::nullopt;</span><br><span class="line">    <span class="keyword">if</span> (even_value != std::nullopt) &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;has value, which is &quot;</span> &lt;&lt; even_value.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;no value&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1. std::optional 对象的构造</span></span><br><span class="line">    <span class="comment">// 1.0 采用 std::nullopt 初始化再调用 emplace 插入值</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val0 = std::nullopt;</span><br><span class="line">    val0.<span class="built_in">emplace</span>(<span class="number">128</span>);</span><br><span class="line">    val0.<span class="built_in">emplace</span>(<span class="number">129</span>);</span><br><span class="line">    std::cout &lt;&lt; val0.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.1 采用 &#123;&#125; 初始化再调用 emplace 插入值</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val1 = &#123;&#125;;</span><br><span class="line">    val1.<span class="built_in">emplace</span>(<span class="number">128</span>);</span><br><span class="line">    std::cout &lt;&lt; val1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.2 采用 std::optional&lt;T&gt;(val) 初始化</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val2 = std::optional&lt;<span class="keyword">int</span>&gt;(<span class="number">128</span>);</span><br><span class="line">    std::cout &lt;&lt; val2.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.3 采用 std::optional(val) 初始化，自动推导变量类型</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val3 = std::<span class="built_in">optional</span>(<span class="number">128</span>);</span><br><span class="line">    std::cout &lt;&lt; val3.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.4 采用 std::optional&lt;T&gt;&#123;val&#125; 初始化</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val4 = std::optional&lt;<span class="keyword">int</span>&gt;&#123;<span class="number">128</span>&#125;;</span><br><span class="line">    std::cout &lt;&lt; val4.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.5 采用 std::optional&#123;val&#125; 初始化</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val5 = std::optional&#123;<span class="number">128</span>&#125;;</span><br><span class="line">    std::cout &lt;&lt; val5.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.6 采用 &#123;val&#125; 初始化</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val6 = &#123;<span class="number">128</span>&#125;;</span><br><span class="line">    std::cout &lt;&lt; val6.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.7 采用 std::make_optional&lt;T&gt;(val) 初始化</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val7 = std::make_optional&lt;<span class="keyword">int</span>&gt;(<span class="number">128</span>);</span><br><span class="line">    std::cout &lt;&lt; val7.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 1.8 采用 std::make_optional(val) 初始化，自动推导变量类型</span></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val8 = std::<span class="built_in">make_optional</span>(<span class="number">128</span>);</span><br><span class="line">    std::cout &lt;&lt; val8.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val9 = std::nullopt;</span><br><span class="line">    std::cout &lt;&lt; val9.<span class="built_in">value_or</span>(<span class="number">-1</span>) &lt;&lt; std::endl;</span><br><span class="line">    val9.<span class="built_in">emplace</span>(<span class="number">128</span>);</span><br><span class="line">    std::cout &lt;&lt; val9.<span class="built_in">value_or</span>(<span class="number">-1</span>) &lt;&lt; std::endl;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//    std::optional&lt;int&gt; val10 = std::nullopt;</span></span><br><span class="line">    <span class="comment">//    std::cout &lt;&lt; val10.value() &lt;&lt; std::endl;</span></span><br><span class="line"></span><br><span class="line">    std::optional&lt;<span class="keyword">int</span>&gt; val11 = std::nullopt;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        std::cout &lt;&lt; val11.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125; <span class="built_in"><span class="keyword">catch</span></span> (<span class="keyword">const</span> std::bad_optional_access&amp; e) &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;==&gt; error: &quot;</span> &lt;&lt; e.<span class="built_in">what</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 函数调用例子</span></span><br><span class="line">    std::vector&lt;<span class="keyword">int</span>&gt; neg_values = &#123;<span class="number">-1</span>, <span class="number">-3</span>, <span class="number">-5</span>&#125;;</span><br><span class="line">    std::vector&lt;<span class="keyword">int</span>&gt; pos_values = &#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>&#125;;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> result1 = <span class="built_in">find_the_first_postive_value_v2</span>(pos_values);</span><br><span class="line">    <span class="keyword">if</span> (result1.<span class="built_in">has_value</span>()) &#123;</span><br><span class="line">        std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (result1 != std::nullopt) &#123;</span><br><span class="line">        std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (result1) &#123;</span><br><span class="line">        std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (result1) &#123;</span><br><span class="line">        std::cout &lt;&lt; *result1 &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// try-catch 示例</span></span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">        std::cout &lt;&lt; result1.<span class="built_in">value</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125; <span class="built_in"><span class="keyword">catch</span></span> (<span class="keyword">const</span> std::bad_optional_access&amp; e) &#123;</span><br><span class="line">        std::cout &lt;&lt; <span class="string">&quot;==&gt; error: &quot;</span> &lt;&lt; e.<span class="built_in">what</span>() &lt;&lt; std::endl;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="built_in">show_backend</span>(std::nullopt);</span><br><span class="line">    <span class="built_in">show_backend</span>(std::<span class="built_in">make_optional</span>(<span class="string">&quot;CUDA&quot;</span>));</span><br><span class="line"></span><br><span class="line">    <span class="keyword">auto</span> my_backend = std::optional&lt;std::string&gt;&#123;<span class="string">&quot;MPS&quot;</span>&#125;;</span><br><span class="line">    <span class="built_in">show_backend</span>(my_backend);</span><br><span class="line">    my_backend.<span class="built_in">emplace</span>(<span class="string">&quot;DSP&quot;</span>);</span><br><span class="line">    <span class="built_in">show_backend</span>(my_backend);</span><br><span class="line"></span><br><span class="line">    std::optional&lt;std::vector&lt;<span class="keyword">int</span>&gt;&gt; res = std::optional&lt;std::vector&lt;<span class="keyword">int</span>&gt;&gt;(&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>&#125;);</span><br><span class="line">    std::cout &lt;&lt; res.<span class="built_in">value</span>()[<span class="number">0</span>] &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以通过<code>g++ -std=c++17 main.cpp  &amp;&amp; ./a.out</code> 来编译运行。</p><h3 id="8-参考"><a href="#8-参考" class="headerlink" title="8. 参考"></a>8. 参考</h3><ol><li><a href="https://en.cppreference.com/w/cpp/utility/optional">https://en.cppreference.com/w/cpp/utility/optional</a></li><li><a href="https://devblogs.microsoft.com/cppblog/stdoptional-how-when-and-why">https://devblogs.microsoft.com/cppblog/stdoptional-how-when-and-why</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-std-optional-是什么&quot;&gt;&lt;a href=&quot;#1-std-optional-是什么&quot; class=&quot;headerlink&quot; title=&quot;1. std::optional 是什么&quot;&gt;&lt;/a&gt;1. std::optional 是什么&lt;/h3&gt;&lt;p&gt;C++ 17 引入了std::optional，表示一个可能有值的对象（没有值时就是默认的&lt;code&gt;std::nullopt&lt;/code&gt;)，例如这个例子中，std::optional 对象 even_value，如果&lt;code&gt;is_even&lt;/code&gt; 为真的话就是128，否则就是默认值&lt;code&gt;std::nullopt&lt;/code&gt;: &lt;/p&gt;
&lt;figure class=&quot;highlight cpp&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;11&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;12&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;13&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;14&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;15&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;include&lt;/span&gt; &lt;span class=&quot;meta-string&quot;&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;include&lt;/span&gt; &lt;span class=&quot;meta-string&quot;&gt;&amp;lt;optiona&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;bool&lt;/span&gt; is_even = &lt;span class=&quot;literal&quot;&gt;true&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// 在 没有值的情况下 std::optional 对象的值为 std::nullopt&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;std::optional&amp;lt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;&amp;gt; even_value = is_even ? std::optional&amp;lt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt;&amp;gt;(&lt;span class=&quot;number&quot;&gt;128&lt;/span&gt;) : std::nullopt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// 可以用 std::optional 对象是否等于 std::nullopt 来判断 std::optional 对象是否有值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; (even_value != std::nullopt) &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;comment&quot;&gt;// 采用.value 获取 std::optional 对象的值&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    std::cout &amp;lt;&amp;lt; &lt;span class=&quot;string&quot;&gt;&amp;quot;has value, which is &amp;quot;&lt;/span&gt; &amp;lt;&amp;lt; even_value.&lt;span class=&quot;built_in&quot;&gt;value&lt;/span&gt;() &amp;lt;&amp;lt; std::endl;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125; &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    std::cout &amp;lt;&amp;lt; &lt;span class=&quot;string&quot;&gt;&amp;quot;no value&amp;quot;&lt;/span&gt; &amp;lt;&amp;lt; std::endl;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;

&lt;p&gt;其实std::optional的作用和Python里面的&lt;code&gt;None&lt;/code&gt;比较像，例如上面的例子用Python来写就是这样：&lt;/p&gt;
&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;is_even = &lt;span class=&quot;literal&quot;&gt;True&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;even_value = &lt;span class=&quot;number&quot;&gt;128&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; is_even &lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;if&lt;/span&gt; even_value &lt;span class=&quot;keyword&quot;&gt;is&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;literal&quot;&gt;None&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&amp;quot;has value, which is&amp;quot;&lt;/span&gt;, even_value)&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;else&lt;/span&gt;:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;    &lt;span class=&quot;built_in&quot;&gt;print&lt;/span&gt;(&lt;span class=&quot;string&quot;&gt;&amp;quot;no value&amp;quot;&lt;/span&gt;)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;</summary>
    
    
    
    
    <category term="C++" scheme="http://vra.github.io/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>libtorch系列教程3：优雅地训练MNIST分类模型</title>
    <link href="http://vra.github.io/2023/06/30/libtorch-tutorial3/"/>
    <id>http://vra.github.io/2023/06/30/libtorch-tutorial3/</id>
    <published>2023-06-30T14:38:16.000Z</published>
    <updated>2023-06-30T14:39:28.217Z</updated>
    
    <content type="html"><![CDATA[<p>在这篇文章中，我们对如何使用Libtorch进行MNIST分类模型的训练和测试进行详细描述。首先会浏览官方MNIST示例，然后对其进行模块化重构，为后续别的模型的训练提供 codebase。</p><p>由于Libtorch中包含很多和Pytorch中没有的类型，所以看Libtorch代码的时候时常会遇到不了解的函数或者类，这时候可以在<a href="https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/include/torch">这里</a>查找对应的类的实现，了解其作用。Libtorch C++ 代码中的注释虽然不多但基本够用了。</p><p>这里列举一些常见的类的代码路径，方便查询：</p><ul><li>Datasets: <a href="https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/base.h">https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/base.h</a></li><li>DataLoader:<a href="https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/include/torch/data/dataloader/base.h">https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/include/torch/data/dataloader/base.h</a></li><li>MNIST: <a href="https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/mnist.h">https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/mnist.h</a></li><li>Stack: <a href="https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/transforms/stack.h">https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/transforms/stack.h</a></li><li>RandomSampler:  <a href="https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/random.cpp">https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/random.cpp</a></li><li>SequentialSampler: <a href="https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/sequential.cpp">https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/sequential.cpp</a></li></ul><span id="more"></span><ul><li><h3 id="1-官方MNIST示例"><a href="#1-官方MNIST示例" class="headerlink" title="1. 官方MNIST示例"></a>1. 官方MNIST示例</h3>Libtorch官方的训练代码仓库在<a href="https://github.com/pytorch/examples/tree/main/cpp">这里</a>，拿里面的训练MNIST为例，代码如下：<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstddef&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// Where to find the MNIST dataset.</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">char</span>* kDataRoot = <span class="string">&quot;./data&quot;</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// The batch size for training.</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int64_t</span> kTrainBatchSize = <span class="number">64</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// The batch size for testing.</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int64_t</span> kTestBatchSize = <span class="number">1000</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// The number of epochs to train.</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int64_t</span> kNumberOfEpochs = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// After how many batches to log a new update with the loss value.</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int64_t</span> kLogInterval = <span class="number">10</span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Net</span> :</span> torch::nn::Module &#123;</span><br><span class="line">  <span class="built_in">Net</span>()</span><br><span class="line">      : <span class="built_in">conv1</span>(torch::nn::<span class="built_in">Conv2dOptions</span>(<span class="number">1</span>, <span class="number">10</span>, <span class="comment">/*kernel_size=*/</span><span class="number">5</span>)),</span><br><span class="line">        <span class="built_in">conv2</span>(torch::nn::<span class="built_in">Conv2dOptions</span>(<span class="number">10</span>, <span class="number">20</span>, <span class="comment">/*kernel_size=*/</span><span class="number">5</span>)),</span><br><span class="line">        <span class="built_in">fc1</span>(<span class="number">320</span>, <span class="number">50</span>),</span><br><span class="line">        <span class="built_in">fc2</span>(<span class="number">50</span>, <span class="number">10</span>) &#123;</span><br><span class="line">    <span class="built_in">register_module</span>(<span class="string">&quot;conv1&quot;</span>, conv1);</span><br><span class="line">    <span class="built_in">register_module</span>(<span class="string">&quot;conv2&quot;</span>, conv2);</span><br><span class="line">    <span class="built_in">register_module</span>(<span class="string">&quot;conv2_drop&quot;</span>, conv2_drop);</span><br><span class="line">    <span class="built_in">register_module</span>(<span class="string">&quot;fc1&quot;</span>, fc1);</span><br><span class="line">    <span class="built_in">register_module</span>(<span class="string">&quot;fc2&quot;</span>, fc2);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function">torch::Tensor <span class="title">forward</span><span class="params">(torch::Tensor x)</span> </span>&#123;</span><br><span class="line">    x = torch::<span class="built_in">relu</span>(torch::<span class="built_in">max_pool2d</span>(conv1-&gt;forward(x), <span class="number">2</span>));</span><br><span class="line">    x = torch::<span class="built_in">relu</span>(</span><br><span class="line">        torch::<span class="built_in">max_pool2d</span>(conv2_drop-&gt;forward(conv2-&gt;forward(x)), <span class="number">2</span>));</span><br><span class="line">    x = x.<span class="built_in">view</span>(&#123;<span class="number">-1</span>, <span class="number">320</span>&#125;);</span><br><span class="line">    x = torch::<span class="built_in">relu</span>(fc1-&gt;forward(x));</span><br><span class="line">    x = torch::<span class="built_in">dropout</span>(x, <span class="comment">/*p=*/</span><span class="number">0.5</span>, <span class="comment">/*training=*/</span><span class="built_in">is_training</span>());</span><br><span class="line">    x = fc2-&gt;forward(x);</span><br><span class="line">    <span class="keyword">return</span> torch::<span class="built_in">log_softmax</span>(x, <span class="comment">/*dim=*/</span><span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  torch::nn::Conv2d conv1;</span><br><span class="line">  torch::nn::Conv2d conv2;</span><br><span class="line">  torch::nn::Dropout2d conv2_drop;</span><br><span class="line">  torch::nn::Linear fc1;</span><br><span class="line">  torch::nn::Linear fc2;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> DataLoader&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">train</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">size_t</span> epoch,</span></span></span><br><span class="line"><span class="params"><span class="function">    Net&amp; model,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Device device,</span></span></span><br><span class="line"><span class="params"><span class="function">    DataLoader&amp; data_loader,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::optim::Optimizer&amp; optimizer,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">size_t</span> dataset_size)</span> </span>&#123;</span><br><span class="line">  model.<span class="built_in">train</span>();</span><br><span class="line">  <span class="keyword">size_t</span> batch_idx = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; batch : data_loader) &#123;</span><br><span class="line">    <span class="keyword">auto</span> data = batch.data.<span class="built_in">to</span>(device), targets = batch.target.<span class="built_in">to</span>(device);</span><br><span class="line">    optimizer.<span class="built_in">zero_grad</span>();</span><br><span class="line">    <span class="keyword">auto</span> output = model.forward(data);</span><br><span class="line">    <span class="keyword">auto</span> loss = torch::<span class="built_in">nll_loss</span>(output, targets);</span><br><span class="line">    <span class="built_in">AT_ASSERT</span>(!std::<span class="built_in">isnan</span>(loss.<span class="keyword">template</span> item&lt;<span class="keyword">float</span>&gt;()));</span><br><span class="line">    loss.<span class="built_in">backward</span>();</span><br><span class="line">    optimizer.<span class="built_in">step</span>();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (batch_idx++ % kLogInterval == <span class="number">0</span>) &#123;</span><br><span class="line">      std::<span class="built_in">printf</span>(</span><br><span class="line">          <span class="string">&quot;\rTrain Epoch: %ld [%5ld/%5ld] Loss: %.4f&quot;</span>,</span><br><span class="line">          epoch,</span><br><span class="line">          batch_idx * batch.data.<span class="built_in">size</span>(<span class="number">0</span>),</span><br><span class="line">          dataset_size,</span><br><span class="line">          loss.<span class="keyword">template</span> item&lt;<span class="keyword">float</span>&gt;());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> DataLoader&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">test</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    Net&amp; model,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Device device,</span></span></span><br><span class="line"><span class="params"><span class="function">    DataLoader&amp; data_loader,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">size_t</span> dataset_size)</span> </span>&#123;</span><br><span class="line">  torch::NoGradGuard no_grad;</span><br><span class="line">  model.<span class="built_in">eval</span>();</span><br><span class="line">  <span class="keyword">double</span> test_loss = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int32_t</span> correct = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; batch : data_loader) &#123;</span><br><span class="line">    <span class="keyword">auto</span> data = batch.data.<span class="built_in">to</span>(device), targets = batch.target.<span class="built_in">to</span>(device);</span><br><span class="line">    <span class="keyword">auto</span> output = model.forward(data);</span><br><span class="line">    test_loss += torch::<span class="built_in">nll_loss</span>(</span><br><span class="line">                     output,</span><br><span class="line">                     targets,</span><br><span class="line">                     <span class="comment">/*weight=*/</span>&#123;&#125;,</span><br><span class="line">                     torch::Reduction::Sum)</span><br><span class="line">                     .<span class="keyword">template</span> item&lt;<span class="keyword">float</span>&gt;();</span><br><span class="line">    <span class="keyword">auto</span> pred = output.<span class="built_in">argmax</span>(<span class="number">1</span>);</span><br><span class="line">    correct += pred.<span class="built_in">eq</span>(targets).<span class="built_in">sum</span>().<span class="keyword">template</span> item&lt;<span class="keyword">int64_t</span>&gt;();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  test_loss /= dataset_size;</span><br><span class="line">  std::<span class="built_in">printf</span>(</span><br><span class="line">      <span class="string">&quot;\nTest set: Average loss: %.4f | Accuracy: %.3f\n&quot;</span>,</span><br><span class="line">      test_loss,</span><br><span class="line">      <span class="keyword">static_cast</span>&lt;<span class="keyword">double</span>&gt;(correct) / dataset_size);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">auto</span> <span class="title">main</span><span class="params">()</span> -&gt; <span class="keyword">int</span> </span>&#123;</span><br><span class="line">  torch::<span class="built_in">manual_seed</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  torch::DeviceType device_type;</span><br><span class="line">  <span class="keyword">if</span> (torch::cuda::<span class="built_in">is_available</span>()) &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;CUDA available! Training on GPU.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    device_type = torch::kCUDA;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Training on CPU.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">    device_type = torch::kCPU;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">torch::Device <span class="title">device</span><span class="params">(device_type)</span></span>;</span><br><span class="line"></span><br><span class="line">  Net model;</span><br><span class="line">  model.<span class="built_in">to</span>(device);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> train_dataset = torch::data::datasets::<span class="built_in">MNIST</span>(kDataRoot)</span><br><span class="line">                           .<span class="built_in">map</span>(torch::data::transforms::Normalize&lt;&gt;(<span class="number">0.1307</span>, <span class="number">0.3081</span>))</span><br><span class="line">                           .<span class="built_in">map</span>(torch::data::transforms::Stack&lt;&gt;());</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">size_t</span> train_dataset_size = train_dataset.<span class="built_in">size</span>().<span class="built_in">value</span>();</span><br><span class="line">  <span class="keyword">auto</span> train_loader =</span><br><span class="line">      torch::data::make_data_loader&lt;torch::data::samplers::SequentialSampler&gt;(</span><br><span class="line">          std::<span class="built_in">move</span>(train_dataset), kTrainBatchSize);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">auto</span> test_dataset = torch::data::datasets::<span class="built_in">MNIST</span>(</span><br><span class="line">                          kDataRoot, torch::data::datasets::MNIST::Mode::kTest)</span><br><span class="line">                          .<span class="built_in">map</span>(torch::data::transforms::Normalize&lt;&gt;(<span class="number">0.1307</span>, <span class="number">0.3081</span>))</span><br><span class="line">                          .<span class="built_in">map</span>(torch::data::transforms::Stack&lt;&gt;());</span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">size_t</span> test_dataset_size = test_dataset.<span class="built_in">size</span>().<span class="built_in">value</span>();</span><br><span class="line">  <span class="keyword">auto</span> test_loader =</span><br><span class="line">      torch::data::<span class="built_in">make_data_loader</span>(std::<span class="built_in">move</span>(test_dataset), kTestBatchSize);</span><br><span class="line"></span><br><span class="line">  torch::<span class="function">optim::SGD <span class="title">optimizer</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      model.parameters(), torch::optim::SGDOptions(<span class="number">0.01</span>).momentum(<span class="number">0.5</span>))</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> epoch = <span class="number">1</span>; epoch &lt;= kNumberOfEpochs; ++epoch) &#123;</span><br><span class="line">    <span class="built_in">train</span>(epoch, model, device, *train_loader, optimizer, train_dataset_size);</span><br><span class="line">    <span class="built_in">test</span>(model, device, *test_loader, test_dataset_size);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ul><p>代码具体细节可以先不用理解，后文有一些说明。可以看到所有的模型搭建、数据读取、网络训练和测试代码都混在一个文件里面，别的几个例子里面也是类似的写法。</p><p>这样写当然是可以的，但对于习惯了Pytorch训练的我们来说，这样所有的代码在一个文件中的写法很不易读，<br>修改数据和网络都相互有影响，且不利用真正严肃地模型训练迭代。</p><h3 id="2-重构-MNIST-示例代码"><a href="#2-重构-MNIST-示例代码" class="headerlink" title="2. 重构 MNIST 示例代码"></a>2. 重构 MNIST 示例代码</h3><p>所以一个简单的想法是改进写法，将DataLoader, Model 和训练逻辑拆分出来，分别进行模块化，放到单独的文件中处理。</p><h4 id="2-1-简单拆分的问题"><a href="#2-1-简单拆分的问题" class="headerlink" title="2.1 简单拆分的问题"></a>2.1 简单拆分的问题</h4><p>第一次尝试是将Dataset和DataLoader放到一个模块中，网络定义放到一个模块中，训练和测试代码放到一个模块中。<br>但这样拆分遇到很大问题，核心原因是 Libtorch 的DataLoader类别太复杂了，对于我这种C++了解不深入的人来说改造难度太大。</p><p>举个例子，我们对MNIST Dataset类进行Normalize后Stack，然后构造一个DataLoader对象<code>train_loader</code>，代码如下：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> train_dataset = torch::data::datasets::<span class="built_in">MNIST</span>(data_root)</span><br><span class="line">                             .<span class="built_in">map</span>(torch::data::transforms::Normalize&lt;&gt;(<span class="number">0.1307</span>, <span class="number">0.3081</span>))</span><br><span class="line">                             .<span class="built_in">map</span>(torch::data::transforms::Stack&lt;&gt;());</span><br><span class="line"><span class="keyword">auto</span> train_loader =</span><br><span class="line">        torch::data::make_data_loader&lt;torch::data::samplers::SequentialSampler&gt;(std::<span class="built_in">move</span>(train_dataset), <span class="number">64</span>);</span><br></pre></td></tr></table></figure><p>生成的<code>train_loader</code>对象的类型是：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch::<span class="keyword">disable_if_t</span>&lt;MapDataset&lt;MapDataset&lt;MNIST, Normalize&lt;&gt;&gt;, Stack&lt;&gt;&gt;::is_stateful || !std::is_constructible&lt;SequentialSampler, <span class="keyword">size_t</span>&gt;::value, std::unique_ptr&lt;StatelessDataLoader&lt;MapDataset&lt;MapDataset&lt;MNIST, Normalize&lt;&gt;&gt;, Stack&lt;&gt;&gt;, SequentialSampler&gt;&gt;&gt;</span><br></pre></td></tr></table></figure><p>这个类型太复杂了……</p><p>因为官方示例是所有代码在一个文件，因此可以通过<code>auto</code> 来让编译器自动判定类型，省去了写着一长串类型的问题。</p><p>但如果我们要拆分DataLoader到单独的类里面的话，就没法使用<code>auto</code>，需要显式的指出DataLoader的类型，然而即使是这样一长串的类型写上了，还是会有不知道是哪里的问题，导致编译报错。</p><p>当然也有可能有简单的方法来解决这个问题，欢迎C++高手讨论指导。</p><p>这次体验让我真正体会到了动态类型语言的简洁性，以及Python的所有类型转C++会存在哪些坑。</p><h4 id="2-2-一种比较简单的重构方案"><a href="#2-2-一种比较简单的重构方案" class="headerlink" title="2.2 一种比较简单的重构方案"></a>2.2 一种比较简单的重构方案</h4><p>最后给出了一个妥协的方案：DataSet在单独的类中定义里面，而DataLoader在训练逻辑中构造，避免繁琐的类型问题。</p><p>整体代码结构如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">├── CMakeLists.txt <span class="comment"># CMake配置文件</span></span><br><span class="line">├── main.cpp <span class="comment"># 主入口</span></span><br><span class="line">├── my_dataset.cpp <span class="comment"># 数据集实现</span></span><br><span class="line">├── my_dataset.h </span><br><span class="line">├── my_model.cpp <span class="comment"># 模型定义</span></span><br><span class="line">├── my_model.h</span><br><span class="line">├── my_trainer.cpp <span class="comment"># 训练和测试脚手架代码</span></span><br><span class="line">└── my_trainer.h</span><br></pre></td></tr></table></figure><h5 id="2-2-1-CMake-配置文件"><a href="#2-2-1-CMake-配置文件" class="headerlink" title="2.2.1 CMake 配置文件"></a>2.2.1 CMake 配置文件</h5><p>CMake 配置文件<code>CMakeLists.txt</code>中将几个实现文件加入到编译依赖即可，别的部分与前两篇文章中的类似。</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.0</span> FATAL_ERROR)</span><br><span class="line"><span class="keyword">project</span>(mnist_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要找到Libtorch</span></span><br><span class="line"><span class="keyword">find_package</span>(Torch REQUIRED)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; $&#123;TORCH_CXX_FLAGS&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> main.cpp my_model.cpp my_dataset.cpp my_trainer.cpp)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> <span class="string">&quot;$&#123;TORCH_LIBRARIES&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Libtorch是基于C++14来实现的</span></span><br><span class="line"><span class="keyword">set_property</span>(<span class="keyword">TARGET</span> <span class="variable">$&#123;PROJECT_NAME&#125;</span> PROPERTY CXX_STANDARD <span class="number">14</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure><h5 id="2-2-2-主入口文件定义"><a href="#2-2-2-主入口文件定义" class="headerlink" title="2.2.2 主入口文件定义"></a>2.2.2 主入口文件定义</h5><p>主入口文件实现了超参数设置，网络和数据集初始化，以及调用Trainer进行训练和测试：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_dataset.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_model.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_trainer.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 超参数设置</span></span><br><span class="line">  std::string data_root = <span class="string">&quot;./data&quot;</span>;</span><br><span class="line">  <span class="keyword">int</span> train_batch_size = <span class="number">128</span>;</span><br><span class="line">  <span class="keyword">int</span> test_batch_size = <span class="number">1000</span>;</span><br><span class="line">  <span class="keyword">int</span> total_epoch_num = <span class="number">30</span>;</span><br><span class="line">  <span class="keyword">int</span> log_interval = <span class="number">10</span>;</span><br><span class="line">  <span class="keyword">int</span> num_workers = <span class="number">32</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 设置随机数种子</span></span><br><span class="line">  torch::<span class="built_in">manual_seed</span>(<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获取设备类型</span></span><br><span class="line">  torch::DeviceType device_type = torch::kCPU;</span><br><span class="line">  <span class="keyword">if</span> (torch::cuda::<span class="built_in">is_available</span>()) &#123;</span><br><span class="line">    device_type = torch::kCUDA;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function">torch::Device <span class="title">device</span><span class="params">(device_type)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 构造网络</span></span><br><span class="line">  MyModel model;</span><br><span class="line">  model.<span class="built_in">to</span>(device);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 设置优化器</span></span><br><span class="line">  torch::<span class="function">optim::SGD <span class="title">optimizer</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      model.parameters(), torch::optim::SGDOptions(<span class="number">0.01</span>).momentum(<span class="number">0.5</span>))</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 构造训练和测试dataset</span></span><br><span class="line">  <span class="keyword">auto</span> train_dataset =</span><br><span class="line">      <span class="built_in">MyDataset</span>(data_root, torch::data::datasets::MNIST::Mode::kTrain);</span><br><span class="line">  <span class="keyword">auto</span> test_dataset =</span><br><span class="line">      <span class="built_in">MyDataset</span>(data_root, torch::data::datasets::MNIST::Mode::kTest);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Trainer初始化</span></span><br><span class="line">  <span class="keyword">auto</span> trainer = <span class="built_in">MyTrainer</span>(log_interval);</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">size_t</span> epoch = <span class="number">1</span>; epoch &lt; total_epoch_num; ++epoch) &#123;</span><br><span class="line">   <span class="comment">// 运行训练</span></span><br><span class="line">    trainer.<span class="built_in">train</span>(</span><br><span class="line">        epoch,</span><br><span class="line">        model,</span><br><span class="line">        optimizer,</span><br><span class="line">        device,</span><br><span class="line">        train_dataset,</span><br><span class="line">        train_batch_size,</span><br><span class="line">        num_workers);</span><br><span class="line">        </span><br><span class="line">    <span class="comment">// 运行测试</span></span><br><span class="line">    trainer.<span class="built_in">test</span>(model, device, test_dataset, test_batch_size, num_workers);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="2-2-3-网络定义"><a href="#2-2-3-网络定义" class="headerlink" title="2.2.3 网络定义"></a>2.2.3 网络定义</h5><p>网络结构采用简单的LeNet，两个conv层和2个fc层。<br>头文件 my_model.h 内容:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> once</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyModel</span> :</span> <span class="keyword">public</span> torch::nn::Module &#123;</span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">MyModel</span>();</span><br><span class="line">  <span class="function">torch::Tensor <span class="title">forward</span><span class="params">(torch::Tensor x)</span></span>;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  torch::nn::Conv2d conv1 = <span class="literal">nullptr</span>;</span><br><span class="line">  torch::nn::Conv2d conv2 = <span class="literal">nullptr</span>;</span><br><span class="line">  torch::nn::Dropout2d conv2_drop;</span><br><span class="line">  torch::nn::Linear fc1 = <span class="literal">nullptr</span>;</span><br><span class="line">  torch::nn::Linear fc2 = <span class="literal">nullptr</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>实现文件 my_model.cpp:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_model.h&quot;</span></span></span><br><span class="line"></span><br><span class="line">MyModel::<span class="built_in">MyModel</span>() &#123;</span><br><span class="line">  conv1 = torch::nn::<span class="built_in">Conv2d</span>(torch::nn::<span class="built_in">Conv2dOptions</span>(<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>));</span><br><span class="line">  conv2 = torch::nn::<span class="built_in">Conv2d</span>(torch::nn::<span class="built_in">Conv2dOptions</span>(<span class="number">10</span>, <span class="number">20</span>, <span class="number">5</span>));</span><br><span class="line">  fc1 = torch::nn::<span class="built_in">Linear</span>(<span class="number">320</span>, <span class="number">50</span>);</span><br><span class="line">  fc2 = torch::nn::<span class="built_in">Linear</span>(<span class="number">50</span>, <span class="number">10</span>);</span><br><span class="line"></span><br><span class="line">  <span class="built_in">register_module</span>(<span class="string">&quot;conv1&quot;</span>, conv1);</span><br><span class="line">  <span class="built_in">register_module</span>(<span class="string">&quot;conv2&quot;</span>, conv2);</span><br><span class="line">  <span class="built_in">register_module</span>(<span class="string">&quot;conv2_drop&quot;</span>, conv2_drop);</span><br><span class="line">  <span class="built_in">register_module</span>(<span class="string">&quot;fc1&quot;</span>, fc1);</span><br><span class="line">  <span class="built_in">register_module</span>(<span class="string">&quot;fc2&quot;</span>, fc2);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">torch::Tensor <span class="title">MyModel::forward</span><span class="params">(torch::Tensor x)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// conv1</span></span><br><span class="line">  x = conv1-&gt;forward(x);</span><br><span class="line">  x = torch::<span class="built_in">max_pool2d</span>(x, <span class="number">2</span>);</span><br><span class="line">  x = torch::<span class="built_in">relu</span>(x);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// conv2</span></span><br><span class="line">  x = conv2-&gt;forward(x);</span><br><span class="line">  x = conv2_drop-&gt;forward(x);</span><br><span class="line">  x = torch::<span class="built_in">max_pool2d</span>(x, <span class="number">2</span>);</span><br><span class="line">  x = torch::<span class="built_in">relu</span>(x);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// fc1</span></span><br><span class="line">  x = x.<span class="built_in">view</span>(&#123;<span class="number">-1</span>, <span class="number">320</span>&#125;);</span><br><span class="line">  x = fc1-&gt;forward(x);</span><br><span class="line">  x = torch::<span class="built_in">relu</span>(x);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// dropout</span></span><br><span class="line">  x = torch::<span class="built_in">dropout</span>(x, <span class="number">0.5</span>, <span class="built_in">is_training</span>());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// fc2</span></span><br><span class="line">  x = fc2-&gt;forward(x);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// log softmax</span></span><br><span class="line">  x = torch::<span class="built_in">log_softmax</span>(x, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> x;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到网络的定义还是比较简单直接，可以直接从Python 网络定义迁移过去，几个核心点：</p><ul><li>网络类的定义需要继承<code>torch::nn::Module</code> 类</li><li>实现<code>forward</code> 函数来进行网络前项运算，其中每个层需要显式地调用<code>forward</code> 函数</li></ul><h5 id="2-2-4-数据集定义"><a href="#2-2-4-数据集定义" class="headerlink" title="2.2.4 数据集定义"></a>2.2.4 数据集定义</h5><p>由于 Libtorch 自带 MNIST的实现，我们这里只是做了一个简单的封装，作为模块化的例子。<br>头文件<code>my_dataset.h</code> 内容：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> once</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyDataset</span> &#123;</span></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">MyDataset</span>(</span><br><span class="line">      <span class="keyword">const</span> std::string&amp; data_root,</span><br><span class="line">      torch::data::datasets::MNIST::Mode phase);</span><br><span class="line"></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  torch::data::datasets::MNIST mnist_dataset;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>实现文件<code>my_dataset.cpp</code> 内容：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_dataset.h&quot;</span></span></span><br><span class="line"></span><br><span class="line">MyDataset::<span class="built_in">MyDataset</span>(</span><br><span class="line">    <span class="keyword">const</span> std::string&amp; data_root,</span><br><span class="line">    torch::data::datasets::MNIST::Mode phase)</span><br><span class="line">    : <span class="built_in">mnist_dataset</span>(torch::data::datasets::<span class="built_in">MNIST</span>(data_root, phase)) &#123;&#125;</span><br></pre></td></tr></table></figure><p>这里有一个需要注意的点，由于MNIST类本身没有默认构造函数，所以在<code>MyDataset</code> 类的初始化列表中就必须给成员变量<code>mnist_dataset</code>赋值，否则会报下面的错:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">constructor for &#x27;MyDataset&#x27; must explicitly initialize the member &#x27;mnist_dataset&#x27; which does not have a default constructor</span><br></pre></td></tr></table></figure><h5 id="2-2-5-Trainer定义"><a href="#2-2-5-Trainer定义" class="headerlink" title="2.2.5 Trainer定义"></a>2.2.5 Trainer定义</h5><p>Trainer 包含训练和测试的两个函数，对数据和网络，优化器等输入进行计算，得到输出，计算loss和准确率。<br>头文件<code>my_trainer.h</code>内容：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">pragma</span> once</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_dataset.h&quot;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_model.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyTrainer</span> &#123;</span></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  <span class="built_in">MyTrainer</span>(<span class="keyword">int</span> log_interval) : <span class="built_in">log_interval_</span>(log_interval)&#123;&#125;;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">train</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="keyword">size_t</span> epoch,</span></span></span><br><span class="line"><span class="params"><span class="function">      MyModel&amp; model,</span></span></span><br><span class="line"><span class="params"><span class="function">      torch::optim::Optimizer&amp; optimizer,</span></span></span><br><span class="line"><span class="params"><span class="function">      torch::Device device,</span></span></span><br><span class="line"><span class="params"><span class="function">      MyDataset&amp; train_dataset,</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="keyword">int</span> num_workers)</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">test</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">      MyModel&amp; model,</span></span></span><br><span class="line"><span class="params"><span class="function">      torch::Device device,</span></span></span><br><span class="line"><span class="params"><span class="function">      MyDataset&amp; test_dataset,</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">      <span class="keyword">int</span> num_workers)</span></span>;</span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="keyword">int</span> log_interval_;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>实现文件<code>my_trainer.cpp</code> 内容：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&quot;my_trainer.h&quot;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;cstdio&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MyTrainer::train</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">size_t</span> epoch,</span></span></span><br><span class="line"><span class="params"><span class="function">    MyModel&amp; model,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::optim::Optimizer&amp; optimizer,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Device device,</span></span></span><br><span class="line"><span class="params"><span class="function">    MyDataset&amp; train_dataset,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">int</span> num_workers)</span> </span>&#123;</span><br><span class="line">  model.<span class="built_in">train</span>();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 对MNIST数据进行Normalize和Stack（将多个Tensor stack成一个Tensor)</span></span><br><span class="line">  <span class="keyword">auto</span> dataset = train_dataset.mnist_dataset</span><br><span class="line">                     .<span class="built_in">map</span>(torch::data::transforms::Normalize&lt;&gt;(<span class="number">0.1307</span>, <span class="number">0.3081</span>))</span><br><span class="line">                     .<span class="built_in">map</span>(torch::data::transforms::Stack&lt;&gt;());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 构造 DataLoader, 设置 batch size 和 worker 数目</span></span><br><span class="line">  <span class="keyword">auto</span> data_loader = torch::data::<span class="built_in">make_data_loader</span>(</span><br><span class="line">      dataset,</span><br><span class="line">      torch::data::<span class="built_in">DataLoaderOptions</span>()</span><br><span class="line">          .<span class="built_in">batch_size</span>(batch_size)</span><br><span class="line">          .<span class="built_in">workers</span>(num_workers));</span><br><span class="line">  <span class="keyword">auto</span> dataset_size = dataset.<span class="built_in">size</span>().<span class="built_in">value</span>();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">size_t</span> batch_idx = <span class="number">0</span>;</span><br><span class="line">  <span class="comment">// 网络训练</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">auto</span>&amp; batch : *data_loader) &#123;</span><br><span class="line">    <span class="comment">// 获取数据和label</span></span><br><span class="line">    <span class="keyword">auto</span> data = batch.data.<span class="built_in">to</span>(device);</span><br><span class="line">    <span class="keyword">auto</span> targets = batch.target.<span class="built_in">to</span>(device);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 优化器 梯度清零</span></span><br><span class="line">    optimizer.<span class="built_in">zero_grad</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 模型前向操作，得到预测输出</span></span><br><span class="line">    <span class="keyword">auto</span> output = model.forward(data);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算loss</span></span><br><span class="line">    <span class="keyword">auto</span> loss = torch::<span class="built_in">nll_loss</span>(output, targets);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// loss 反传</span></span><br><span class="line">    loss.<span class="built_in">backward</span>();</span><br><span class="line">    optimizer.<span class="built_in">step</span>();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 打印log信息</span></span><br><span class="line">    <span class="keyword">if</span> (batch_idx++ % log_interval_ == <span class="number">0</span>) &#123;</span><br><span class="line">      std::<span class="built_in">printf</span>(</span><br><span class="line">          <span class="string">&quot;\rTrain Epoch: %ld [%5llu/%5ld] Loss: %.4f&quot;</span>,</span><br><span class="line">          epoch,</span><br><span class="line">          batch_idx * batch.data.<span class="built_in">size</span>(<span class="number">0</span>),</span><br><span class="line">          dataset_size,</span><br><span class="line">          loss.<span class="keyword">template</span> item&lt;<span class="keyword">float</span>&gt;());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">MyTrainer::test</span><span class="params">(</span></span></span><br><span class="line"><span class="params"><span class="function">    MyModel&amp; model,</span></span></span><br><span class="line"><span class="params"><span class="function">    torch::Device device,</span></span></span><br><span class="line"><span class="params"><span class="function">    MyDataset&amp; test_dataset,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">int</span> batch_size,</span></span></span><br><span class="line"><span class="params"><span class="function">    <span class="keyword">int</span> num_workers)</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 测试时要将模型置为eval模式</span></span><br><span class="line">  model.<span class="built_in">eval</span>();</span><br><span class="line">  <span class="keyword">double</span> test_loss = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int32_t</span> correct = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 对MNIST数据进行Normalize和Stack（将多个Tensor stack成一个Tensor)</span></span><br><span class="line">  <span class="keyword">auto</span> dataset = test_dataset.mnist_dataset</span><br><span class="line">                     .<span class="built_in">map</span>(torch::data::transforms::Normalize&lt;&gt;(<span class="number">0.1307</span>, <span class="number">0.3081</span>))</span><br><span class="line">                     .<span class="built_in">map</span>(torch::data::transforms::Stack&lt;&gt;());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 构造 DataLoader, 设置 batch size 和 worker 数目</span></span><br><span class="line">  <span class="keyword">auto</span> data_loader = torch::data::<span class="built_in">make_data_loader</span>(</span><br><span class="line">      dataset,</span><br><span class="line">      torch::data::<span class="built_in">DataLoaderOptions</span>()</span><br><span class="line">          .<span class="built_in">batch_size</span>(batch_size)</span><br><span class="line">          .<span class="built_in">workers</span>(num_workers));</span><br><span class="line">  <span class="keyword">auto</span> dataset_size = dataset.<span class="built_in">size</span>().<span class="built_in">value</span>();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">const</span> <span class="keyword">auto</span>&amp; batch : *data_loader) &#123;</span><br><span class="line">    <span class="comment">// 获取数据和label</span></span><br><span class="line">    <span class="keyword">auto</span> data = batch.data.<span class="built_in">to</span>(device);</span><br><span class="line">    <span class="keyword">auto</span> targets = batch.target.<span class="built_in">to</span>(device);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 模型前向操作，得到预测输出</span></span><br><span class="line">    <span class="keyword">auto</span> output = model.forward(data);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 计算测试时的 loss</span></span><br><span class="line">    test_loss += torch::<span class="built_in">nll_loss</span>(</span><br><span class="line">                     output,</span><br><span class="line">                     targets,</span><br><span class="line">                     <span class="comment">/*weight=*/</span>&#123;&#125;,</span><br><span class="line">                     torch::Reduction::Sum)</span><br><span class="line">                     .item&lt;<span class="keyword">float</span>&gt;();</span><br><span class="line">    <span class="keyword">auto</span> pred = output.<span class="built_in">argmax</span>(<span class="number">1</span>);</span><br><span class="line">    correct += pred.<span class="built_in">eq</span>(targets).<span class="built_in">sum</span>().<span class="keyword">template</span> item&lt;<span class="keyword">int64_t</span>&gt;();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  test_loss /= dataset_size;</span><br><span class="line">  std::<span class="built_in">printf</span>(</span><br><span class="line">      <span class="string">&quot;\nTest set: Average loss: %.4f | Accuracy: %.3f\n&quot;</span>,</span><br><span class="line">      test_loss,</span><br><span class="line">      <span class="keyword">static_cast</span>&lt;<span class="keyword">double</span>&gt;(correct) / dataset_size);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h5 id="2-2-6-编译和运行方式"><a href="#2-2-6-编译和运行方式" class="headerlink" title="2.2.6 编译和运行方式"></a>2.2.6 编译和运行方式</h5><p>我们基于CMake 编译上面的代码，同时下载MNIST数据集，完整的执行命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line">cmake ..  -DCMAKE_PREFIX_PATH=`python -c <span class="string">&#x27;import torch;print(torch.utils.cmake_prefix_path)&#x27;</span>`</span><br><span class="line">make -j8</span><br><span class="line"><span class="comment"># 下载MNIST数据</span></span><br><span class="line">mkdir data &amp;&amp; <span class="built_in">cd</span> data</span><br><span class="line">wget <span class="string">&quot;http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz&quot;</span> &amp;&amp; gunzip train-images-idx3-ubyte.gz</span><br><span class="line">wget <span class="string">&quot;http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz&quot;</span> &amp;&amp; gunzip train-labels-idx1-ubyte.gz</span><br><span class="line">wget <span class="string">&quot;http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz&quot;</span> &amp;&amp; gunzip t10k-images-idx3-ubyte.gz</span><br><span class="line">wget <span class="string">&quot;http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz&quot;</span> &amp;&amp; gunzip t10k-labels-idx1-ubyte.gz</span><br><span class="line"><span class="built_in">cd</span> ../</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行可执行文件</span></span><br><span class="line">./mnist_train</span><br></pre></td></tr></table></figure><p>训练和测试输出如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Train Epoch: 1 [59008/60000] Loss: 0.6824</span><br><span class="line">Test set: Average loss: 0.3265 | Accuracy: 0.910</span><br><span class="line">Train Epoch: 2 [59008/60000] Loss: 0.5521</span><br><span class="line">Test set: Average loss: 0.2018 | Accuracy: 0.941</span><br><span class="line">Train Epoch: 3 [59008/60000] Loss: 0.3403</span><br><span class="line">Test set: Average loss: 0.1523 | Accuracy: 0.954</span><br><span class="line">Train Epoch: 4 [59008/60000] Loss: 0.3885</span><br><span class="line">Test set: Average loss: 0.1236 | Accuracy: 0.965</span><br><span class="line">Train Epoch: 5 [59008/60000] Loss: 0.3502</span><br><span class="line">Test set: Average loss: 0.1083 | Accuracy: 0.967</span><br><span class="line">Train Epoch: 6 [59008/60000] Loss: 0.1389</span><br><span class="line">Test set: Average loss: 0.0961 | Accuracy: 0.970</span><br><span class="line">Train Epoch: 7 [59008/60000] Loss: 0.3550</span><br><span class="line">Test set: Average loss: 0.0899 | Accuracy: 0.972</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>可以看到准确率在逐渐提升。</p><p>这篇文章的内容主要就是这些，后面会根据训练一个实际一些的例子，比如nanoGPT，将在本文的codebase基础上，主要覆盖下面的内容：</p><ul><li>自定义数据集的Dataset类的搭建</li><li>复杂网络的定义(如ResNet, Transformer)</li><li>模型checkpoint的保存和读取</li></ul><p>欢迎点赞和关注！</p>]]></content>
    
    
    <summary type="html">&lt;p&gt;在这篇文章中，我们对如何使用Libtorch进行MNIST分类模型的训练和测试进行详细描述。首先会浏览官方MNIST示例，然后对其进行模块化重构，为后续别的模型的训练提供 codebase。&lt;/p&gt;
&lt;p&gt;由于Libtorch中包含很多和Pytorch中没有的类型，所以看Libtorch代码的时候时常会遇到不了解的函数或者类，这时候可以在&lt;a href=&quot;https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/include/torch&quot;&gt;这里&lt;/a&gt;查找对应的类的实现，了解其作用。Libtorch C++ 代码中的注释虽然不多但基本够用了。&lt;/p&gt;
&lt;p&gt;这里列举一些常见的类的代码路径，方便查询：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Datasets: &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/base.h&quot;&gt;https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/base.h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;DataLoader:&lt;a href=&quot;https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/include/torch/data/dataloader/base.h&quot;&gt;https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/include/torch/data/dataloader/base.h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;MNIST: &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/mnist.h&quot;&gt;https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/datasets/mnist.h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Stack: &lt;a href=&quot;https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/transforms/stack.h&quot;&gt;https://github.com/pytorch/pytorch/blob/main/torch/csrc/api/include/torch/data/transforms/stack.h&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;RandomSampler:  &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/random.cpp&quot;&gt;https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/random.cpp&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;SequentialSampler: &lt;a href=&quot;https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/sequential.cpp&quot;&gt;https://github.com/pytorch/pytorch/tree/main/torch/csrc/api/src/data/samplers/sequential.cpp&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="C++" scheme="http://vra.github.io/tags/C/"/>
    
    <category term="Pytorch" scheme="http://vra.github.io/tags/Pytorch/"/>
    
    <category term="Libtorch" scheme="http://vra.github.io/tags/Libtorch/"/>
    
  </entry>
  
  <entry>
    <title>NeoVim 代码格式化教程</title>
    <link href="http://vra.github.io/2023/06/17/neoformat-python-cpp/"/>
    <id>http://vra.github.io/2023/06/17/neoformat-python-cpp/</id>
    <published>2023-06-17T01:54:45.000Z</published>
    <updated>2023-06-17T03:20:33.226Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p><a href="https://github.com/sbdchd/neoformat">neoformat</a> 是 (Neo)Vim 的代码格式化插件，支持多种语言的格式化。这篇文章覆盖 Neoformat 对 Python 和 C++ 进行格式化的配置，以及如何在保存代码时自动进行格式化，可以直接应用的配置代码段在文章最后。</p><span id="more"></span><h3 id="2-neoformat安装"><a href="#2-neoformat安装" class="headerlink" title="2. neoformat安装"></a>2. neoformat安装</h3><p>采用 <a href="https://github.com/junegunn/vim-plug">Vim-Plug</a> 进行插件管理，在<code>~/.config/nvim/init.vim</code> 中添加下面的插件:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Plug &#x27;sbdchd/neoformat&#x27;</span><br></pre></td></tr></table></figure><p>然后用<code>:PlugInstall</code> 命令来安装插件。由于插件源码在 GitHub 上，国内访问时断时续，一次执行可能安装不成功，可以多执行几次这个命令，直到输出窗口显示安装成功。</p><h3 id="3-neoformat-格式化-Python-代码"><a href="#3-neoformat-格式化-Python-代码" class="headerlink" title="3. neoformat 格式化 Python 代码"></a>3. neoformat 格式化 Python 代码</h3><h4 id="3-1-安装格式化工具"><a href="#3-1-安装格式化工具" class="headerlink" title="3.1 安装格式化工具"></a>3.1 安装格式化工具</h4><p>neoformat本 身不会安装格式化工具，它只会调用系统已经安装好的格式化工具来进行代码格式化，所以你还需要自己手动在系统上安装格式化工具。</p><p>以 Python 格式化为例，我们采用 black 来格式化代码，那么需要先用<code>pip</code> 命令来安装black:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install black</span><br></pre></td></tr></table></figure><p>然后需要确保在命令行执行<code>black --version</code> 命令能正常输出，neoformat 才能找到black:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ black --version</span><br><span class="line">black, 23.3.0 (compiled: yes)</span><br><span class="line">Python (CPython) 3.8.16</span><br></pre></td></tr></table></figure><h4 id="3-2-格式化配置"><a href="#3-2-格式化配置" class="headerlink" title="3.2 格式化配置"></a>3.2 格式化配置</h4><p>安装好以后，我们就可以在<code>~/.config/nvim/init.vim</code> 文件中进行 neoformat 配置:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">let g:neoformat_python_black = &#123;</span><br><span class="line">            \ &#x27;exe&#x27;: &#x27;black&#x27;,</span><br><span class="line">            \ &#x27;args&#x27;: [&#x27;-q&#x27;, &#x27;-&#x27;],</span><br><span class="line">            \ &#x27;stdin&#x27;: 1,</span><br><span class="line">            \ &#125;</span><br><span class="line"></span><br><span class="line">let g:neoformat_enabled_python = [&#x27;black&#x27;]</span><br></pre></td></tr></table></figure><p>这是 VimScript 的语法，<code>let g:neoformat_python_black</code> 是创建一个全局变量<code>neoformat_python_black</code>, 全局变量的特点是所有打开的窗口和缓冲区都可以访问该变量。</p><p>注意这个变量的命名方式，<code>neoformat_&lt;Language&gt;_&lt;formatter&gt;</code>，表示针对某个语言的某一个格式化工具，这个格式化工具的名字会被注册，在下面的enable语句中使用到。</p><p>全局变量的值的含义如下：</p><ul><li><code>exe</code> 表示格式化运行需要执行的程序名，就跟我们在命令行访问某个程序一样的机制，需要知道它叫什么才能来执行。</li><li><code>args</code> 表示程序执行时需要的参数。这里<code>-q</code>是black命令的参数项，表示静默执行，不打印输出；<code>-</code> 表示从标准输入读取内容来格式化</li><li><code>stdin</code>: 这个参数表示是否从标准输入来读取内容来格式化。标准输入对应的是文件的内容，除了标准输入外还有缓存区</li></ul><p>所有的可配置参数参考 <a href="https://github.com/sbdchd/neoformat#config-optional">neoformat 文档</a>。这里我们配置这几个参数项就可以了。</p><p>下面还有一条语句，创建全局变量<code>neoformat_enabled_python</code>，表示针对 Python 启用的格式化工具，这里我们使用上面创建变量后注册的<code>black</code>。</p><h4 id="3-3-执行格式化"><a href="#3-3-执行格式化" class="headerlink" title="3.3 执行格式化"></a>3.3 执行格式化</h4><p>加了上面的 VimScript 配置后，我们在编辑文件时，就可以使用 <code>:Neoformat</code> 命令来格式化代码了。</p><p>如果想要使用特定的格式化工具，可以使用<code>:Neoformat &lt;formater&gt;</code> 来操作。</p><h4 id="3-4-保存文件时自动格式化"><a href="#3-4-保存文件时自动格式化" class="headerlink" title="3.4 保存文件时自动格式化"></a>3.4 保存文件时自动格式化</h4><p>前面的配置我们还需要手动执行<code>:Neoformat</code> 命令来格式化，下面我们添加一些配置到<code>~/.config/nvim/init.vim</code>，在保存文件时自动地进行格式化。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">augroup fmt</span><br><span class="line">  autocmd!</span><br><span class="line">  autocmd BufWritePre * Neoformat</span><br><span class="line">augroup END</span><br></pre></td></tr></table></figure><p>这段代码创建了一个自动化组并命名为<code>fmt</code>，用于将一组命令放在一起，方便管理。</p><p>我们首先使用<code>autocmd!</code>清空这个自动化组中的所有自动化命令，避免影响后面的命令设置。</p><p>然后用<code>autocmd BufWritePre * Neoformat</code>来完成在写buffer之前，对所有类型的文件都执行<code>Neoformat</code>命令。<code>autocmd</code>表示这是一条自动化命令。<code>BufWritePre</code>表示是在Write Buffer之前执行的操作,<code>*</code>表示匹配任意的文件，如果是<code>*.py</code>则只匹配后缀为<code>.py</code>的文件。<code>Neoformat</code> 表示要执行的命令。</p><p>这样，在保存文件时，就可以自动执行代码格式化了。</p><h4 id="3-5-调试命令"><a href="#3-5-调试命令" class="headerlink" title="3.5 调试命令"></a>3.5 调试命令</h4><p>如果出现格式化错误，或者格式化不生效，可以设置 <code>:set verbose=1</code> 来打开 NeoVim 的 log 显示，查看报错信息。实际测试发现这个命令真的很有用，很多信息打印出来后，对于定位问题帮助很大。</p><h3 id="4-neoformat-格式化-C-C-代码"><a href="#4-neoformat-格式化-C-C-代码" class="headerlink" title="4. neoformat 格式化 C/C++ 代码"></a>4. neoformat 格式化 C/C++ 代码</h3><p>对 C/C++代码的格式化与 Python 是类似的，只不过使用的格式化工具不同而已。这里以 <a href="https://clang.llvm.org/docs/ClangFormat.html">clang-format</a> 为例，记录需要执行的步骤。</p><h4 id="4-1-安装格式化工具"><a href="#4-1-安装格式化工具" class="headerlink" title="4.1 安装格式化工具"></a>4.1 安装格式化工具</h4><p>Ubuntu:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt install clang-format</span><br></pre></td></tr></table></figure><p>Mac:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">brew install clang-format</span><br></pre></td></tr></table></figure><h4 id="4-2-格式化配置"><a href="#4-2-格式化配置" class="headerlink" title="4.2 格式化配置"></a>4.2 格式化配置</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">let g:neoformat_c_clangformat = &#123;</span><br><span class="line">            \ &#x27;exe&#x27;: &#x27;clang-format&#x27;,</span><br><span class="line">\ &#x27;args&#x27;: [&#x27;-assume-filename=%:p&#x27;],</span><br><span class="line">            \ &#x27;stdin&#x27;: 1,</span><br><span class="line">            \ &#125;</span><br><span class="line"></span><br><span class="line">let g:neoformat_enabled_c = [&#x27;clangformat&#x27;]</span><br></pre></td></tr></table></figure><p>与 Python black 的配置类似，语言修改为<code>c</code>, formatter 修改为 <code>clangformat</code>，参数有所不同，<code>-assume-filename=%:p</code> 表示将当前编辑的文件名传递给 clang-format，以便它可以正确地处理预编译指令等特殊情况。</p><h4 id="4-3-自定义格式化文件"><a href="#4-3-自定义格式化文件" class="headerlink" title="4.3 自定义格式化文件"></a>4.3 自定义格式化文件</h4><p>如果不想用默认的 clang-format 格式化配置，可以通过下面的方式来生成格式化文件，并通过<code>args</code> 参数传递给Neoformat来使用。</p><p>首先生成一个默认的配置文件，例如选择以google的风格来生成:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clang-format -style=google -dump-config &gt; /Users/name/.clang-format</span><br></pre></td></tr></table></figure><p>然后编辑生成的文件，修改为你想要的格式。例如我想修改默认的2空格缩进为4空格，那么去掉默认文件中的<code># BasedOnStyle:  Google</code>的注释，继承google风格的默认配置，删除后面所有的内容，只修改<code>IndentWidth</code> 项：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">Language:        Cpp</span><br><span class="line">BasedOnStyle:  Google</span><br><span class="line">IndentWidth:     4</span><br></pre></td></tr></table></figure><p>然后用<code>--style=/path/to/.clang-format</code>来代码规范文件：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">let g:neoformat_c_clangformat = &#123;</span><br><span class="line">            \ &#x27;exe&#x27;: &#x27;clang-format&#x27;,</span><br><span class="line">\ &#x27;args&#x27;: [&#x27;-assume-filename=%:p&#x27;, &#x27;--styel=/Users/name/.clang-format&#x27;],</span><br><span class="line">            \ &#x27;stdin&#x27;: 1,</span><br><span class="line">            \ &#125;</span><br><span class="line"></span><br><span class="line">let g:neoformat_enabled_c = [&#x27;clangformat&#x27;]</span><br></pre></td></tr></table></figure><h4 id="4-4-保存文件时自动格式化"><a href="#4-4-保存文件时自动格式化" class="headerlink" title="4.4 保存文件时自动格式化"></a>4.4 保存文件时自动格式化</h4><p>上面 3.4 部分的代码已经开启了保存时自动格式化代码，这里不需要额外增加配置了。</p><h3 id="5-总结"><a href="#5-总结" class="headerlink" title="5. 总结"></a>5. 总结</h3><p>总结下来，涉及到的需要增加在<code>~/.config/nvim/init.vim</code>中的代码如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">call plug#begin(&quot;~/.nvim/bundle&quot;)</span><br><span class="line">...</span><br><span class="line">&quot; 增加neoformat</span><br><span class="line">Plug &#x27;sbdchd/neoformat&#x27;</span><br><span class="line">...</span><br><span class="line">call plug#end()</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">&quot; code format</span><br><span class="line">augroup fmt</span><br><span class="line">  autocmd!</span><br><span class="line">&quot;  autocmd BufWritePre * undojoin | Neoformat</span><br><span class="line">  autocmd BufWritePre * Neoformat</span><br><span class="line">augroup END</span><br><span class="line"></span><br><span class="line">&quot; format python</span><br><span class="line">let g:neoformat_python_black = &#123;</span><br><span class="line">            \ &#x27;exe&#x27;: &#x27;black&#x27;,</span><br><span class="line">            \ &#x27;args&#x27;: [&#x27;-q&#x27;, &#x27;-&#x27;],</span><br><span class="line">            \ &#x27;stdin&#x27;: 1,</span><br><span class="line">            \ &#125;</span><br><span class="line"></span><br><span class="line">let g:neoformat_enabled_python = [&#x27;black&#x27;]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">&quot; format c/c++</span><br><span class="line">let g:neoformat_c_clangformat = &#123;</span><br><span class="line">            \ &#x27;exe&#x27;: &#x27;clang-format&#x27;,</span><br><span class="line">\ &#x27;args&#x27;: [&#x27;-assume-filename=%:p&#x27;],</span><br><span class="line">            \ &#x27;stdin&#x27;: 1,</span><br><span class="line">            \ &#125;</span><br><span class="line"></span><br><span class="line">let g:neoformat_enabled_c = [&#x27;clangformat&#x27;]</span><br></pre></td></tr></table></figure><p>格式化工具需要单独通过命令行来安装:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">python3 -m pip install black</span><br><span class="line">brew install clang-format</span><br></pre></td></tr></table></figure><p>通过 <code>:set verbose=1</code> 来打开 log 信息，对于定位问题很有帮助。</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/sbdchd/neoformat&quot;&gt;neoformat&lt;/a&gt; 是 (Neo)Vim 的代码格式化插件，支持多种语言的格式化。这篇文章覆盖 Neoformat 对 Python 和 C++ 进行格式化的配置，以及如何在保存代码时自动进行格式化，可以直接应用的配置代码段在文章最后。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="C++" scheme="http://vra.github.io/tags/C/"/>
    
    <category term="Vim" scheme="http://vra.github.io/tags/Vim/"/>
    
    <category term="NeoVim" scheme="http://vra.github.io/tags/NeoVim/"/>
    
    <category term="black" scheme="http://vra.github.io/tags/black/"/>
    
    <category term="clang-format" scheme="http://vra.github.io/tags/clang-format/"/>
    
  </entry>
  
  <entry>
    <title>homebrew禁止执行install命令时自动更新</title>
    <link href="http://vra.github.io/2023/06/08/homebrew-disable-auto-update/"/>
    <id>http://vra.github.io/2023/06/08/homebrew-disable-auto-update/</id>
    <published>2023-06-08T14:02:49.000Z</published>
    <updated>2023-06-08T14:13:49.420Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://brew.sh/">Homebrew</a> 是 macOS 下的默认的包管理器，不需要sudo权限就可以安装包，比较好用。</p><p>不过用<code>brew install</code>安装包时有个问题，它默认会先执行<code>brew update</code>来更新brew的版本。但由于brew 的源国内访问比较慢，常常<code>brew update</code>执行耗时比较久，影响每次安装包的体验。</p><p>解决办法是设置<code>HOMEBREW_NO_AUTO_UPDATE</code>环境变量为1，这样每次<code>brew install</code>时跳过更新brew的步骤，实际体验安装包速度提升明显。</p><p>可以添加下面的语句到你的.bashrc或.zshrc中，重启shell即生效:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> HOMEBREW_NO_AUTO_UPDATE=1</span><br></pre></td></tr></table></figure>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://brew.sh/&quot;&gt;Homebrew&lt;/a&gt; 是 macOS 下的默认的包管理器，不需要sudo权限就可以安装包，比较好用。&lt;/p&gt;
&lt;p&gt;不过用&lt;code&gt;brew install&lt;/code&gt;安装包时有个问题，它默认会先执行&lt;code&gt;</summary>
      
    
    
    
    
    <category term="Homebrew" scheme="http://vra.github.io/tags/Homebrew/"/>
    
    <category term="macOS" scheme="http://vra.github.io/tags/macOS/"/>
    
  </entry>
  
  <entry>
    <title>Python 命令补全神器 argcomplete</title>
    <link href="http://vra.github.io/2023/05/28/python-autocomplete-with-argcomplete/"/>
    <id>http://vra.github.io/2023/05/28/python-autocomplete-with-argcomplete/</id>
    <published>2023-05-28T02:33:12.000Z</published>
    <updated>2023-06-09T15:44:59.896Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>在使用Python 命令或者 Python的命令行工具的时候，一个痛点是没有补全。比如<code>python -m</code>后面输入包名字，就没有提示，每次想运行一个http server的时候，都需要搜索一下http服务的包名。另外，像pip，pipx等命令也没有提示，使用不太方便。</p><p>偶然看到<a href="https://github.com/kislyuk/argcomplete">argcomplete</a>这个库，按tab键就可以给Python的命令行添加自动补全，简直是使用Python的一个神器。</p><p>具体来说，argcomplete有下面的特点</p><ul><li>官方支持支持bash和zsh两种shell，对tcsh和fish有第三方贡献者提供的支持（不好意思Windows用户这里又被当做二等公民了😂）</li><li>可以对python命令和pip命令进行补全</li><li>其他任何以argparse解析的第三方包的命令都可以用自动补全，添加argcomplete的几行代码就行</li></ul><p>下面具体展开怎么对已有的工具启用自动补全，以及如何让自己的Python包支持argcomplete。</p><span id="more"></span><h2 id="2-对Python和pip启用自动补全"><a href="#2-对Python和pip启用自动补全" class="headerlink" title="2. 对Python和pip启用自动补全"></a>2. 对Python和pip启用自动补全</h2><p>首先通过<code>pip</code>命令来安装argcomplete:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install argcomplete</span><br></pre></td></tr></table></figure><p>然后执行下面的语句来启用对Python和pip的自动补全:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">activate-global-python-argcomplete</span><br></pre></td></tr></table></figure><p>重启Shell，试试输入<code>pip</code>然后按tab，发现就会列出所有的命令选项。</p><h2 id="3-如何对别的第三方库启用自动补全"><a href="#3-如何对别的第三方库启用自动补全" class="headerlink" title="3. 如何对别的第三方库启用自动补全"></a>3. 如何对别的第三方库启用自动补全</h2><p>有些库的命令行程序是已经支持argcomplete补全，只需要用下面的命令来激活：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">eval</span> <span class="string">&quot;<span class="subst">$(register-python-argcomplete &lt;python-app-name&gt;)</span>&quot;</span></span><br></pre></td></tr></table></figure><p>例如 pipx 包安装后会在系统安装一个命令行程序pipx，且pipx已经支持argcomplete，我们就可以用下面的命令来激活自动补全:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">eval</span> <span class="string">&quot;<span class="subst">$(register-python-argcomplete pipx)</span>&quot;</span></span><br></pre></td></tr></table></figure><p>激活后输入<code>pipx in</code> 按tab键，就可以列出pipx所有以<code>in</code>开头的命令，再按tab键会在各个候选命令之间切换。</p><p>⚠️注意：这个激活命令是只对哪些代码中已经支持了argcomplete语句的程序才生效，如果代码中无这些语句，那是不生效的。</p><h2 id="4-如何让自己的Python库支持自动补全"><a href="#4-如何让自己的Python库支持自动补全" class="headerlink" title="4. 如何让自己的Python库支持自动补全"></a>4. 如何让自己的Python库支持自动补全</h2><p>只需要增加下面几行代码，就能让你的库的命令行支持自动补全:</p><pre><code class="python"># 在ArgumentParser对象初始化前增加这两行# PYTHON_ARGCOMPLETE_OKimport argcomplete, argparse# 原有代码parser = argparse.ArgumentParser()...# 在调用parse_args()函数前增加这一行argcomplete.autocomplete(parser)# 原有代码args = parser.parse_args()...</code></pre><p>然后你的包安装后，对应的命令行程序就可以用<code>eval &quot;$(register-python-argcomplete &lt;app-name&gt;)&quot;</code>来补全了。</p><p>⚠️注意：如果程序执行到<code>argcomplete.autocomplete()</code> 被调用的地方耗时很久的话，用户按tab就会有明显的延迟感。所以尽量将一些比较耗时的操作放在<code>argcomplete.autocomplete()</code> 语句后面，比如一些<code>import</code>语句，常常比较耗时，可以往后放。</p><p>希望这个程序能让你的Python开发变得舒服一些。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h2&gt;&lt;p&gt;在使用Python 命令或者 Python的命令行工具的时候，一个痛点是没有补全。比如&lt;code&gt;python -m&lt;/code&gt;后面输入包名字，就没有提示，每次想运行一个http server的时候，都需要搜索一下http服务的包名。另外，像pip，pipx等命令也没有提示，使用不太方便。&lt;/p&gt;
&lt;p&gt;偶然看到&lt;a href=&quot;https://github.com/kislyuk/argcomplete&quot;&gt;argcomplete&lt;/a&gt;这个库，按tab键就可以给Python的命令行添加自动补全，简直是使用Python的一个神器。&lt;/p&gt;
&lt;p&gt;具体来说，argcomplete有下面的特点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;官方支持支持bash和zsh两种shell，对tcsh和fish有第三方贡献者提供的支持（不好意思Windows用户这里又被当做二等公民了😂）&lt;/li&gt;
&lt;li&gt;可以对python命令和pip命令进行补全&lt;/li&gt;
&lt;li&gt;其他任何以argparse解析的第三方包的命令都可以用自动补全，添加argcomplete的几行代码就行&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;下面具体展开怎么对已有的工具启用自动补全，以及如何让自己的Python包支持argcomplete。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
  </entry>
  
  <entry>
    <title>talkGPT4All 2.0</title>
    <link href="http://vra.github.io/2023/05/27/talkgpt4all-2-0/"/>
    <id>http://vra.github.io/2023/05/27/talkgpt4all-2-0/</id>
    <published>2023-05-27T06:49:45.000Z</published>
    <updated>2023-05-27T06:58:57.916Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h3><p><a href="https://github.com/vra/talkGPT4All">talkGPT4All</a>是基于<a href="https://gpt4all.io/index.html">GPT4All</a>的一个语音聊天程序，运行在本地CPU上，支持Linux，Mac和Windows。它利用OpenAI的Whisper模型将用户输入的语音转换为文本，再调用GPT4All的语言模型得到回答文本，最后利用文本转语音(TTS)的程序将回答文本朗读出来。</p><p>关于 talkGPT4All 1.0的介绍在<a href="https://juejin.cn/post/7217112585802498107">这篇文章</a>。</p><p>talkGPT4All 1.0的<a href="https://www.zhihu.com/zvideo/1625779747656515584">视频效果</a>。</p><p>由于GPT4All一直在迭代，相比上一篇文章发布时(2023-04-10)已经有较大的更新，今天将GPT4All的一些更新同步到talkGPT4All，由于支持的模型和运行模式都有较大的变化，因此发布 talkGPT4All 2.0。</p><p>具体来说，2.0版本相比1.0有下面的更新。</p><p>首先是GPT4All框架支持的语言模型从1个增加到8个，并且可以一键切换模型。具体的模型是</p><ul><li>  Vicuna-7B-1.1-q4_2</li><li>  Vicuna-7B-1.2-q4_2</li><li>  wizardLM-7B.q4_2</li><li>  GPT4All</li><li>  GPT4All-J</li><li>  GPT4All-J-v1.1</li><li>  GPT4All-J-v1.2</li><li>  GPT4All-J-v1.3</li></ul><p>可以看到除了GPT4All系列的模型，这个框架也支持Vicuna和Wizard的模型了。更多模型因为证书和格式的问题，还在集成中。</p><p>根据GPT4All的<a href="https://gpt4all.io/index.html">文档</a>，不同模型在benchmark上的结果：</p><p><img data-src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/988ae9ef513049d68d790e742f9e2139~tplv-k3u1fbpfcp-zoom-1.image"></p><p>可以看到GPT4All系列的模型的指标还是比较高的。</p><p>另一个重要更新是GPT4All发布了更成熟的Python包，可以直接通过pip 来安装，因此1.0中集成的不同平台不同的GPT4All二进制包也不需要了。集成PyPI包的好处多多，既可以查看源码学习内部的实现，又更方便定位问题（之前的二进制包没法调试内部代码），且保证了不同平台安装命令一致（之前不同平台二进制包不同）。</p><p>还有一个变化是GPT4All会自动按需下载模型，因此用户不需要手动下载和维护模型路径。同时将模型统一放置到<a href="https://gpt4all.io/models/">https://gpt4all.io/models/</a> 目录下，测试国内模型下载速度也很快，大家玩起来也会更舒服。</p><p>核心的更新内容就这些，下面对talkGPT4All的安装和使用进行说明，后面有空会添加一些多个语言模型效果的对比视频。</p><span id="more"></span><h3 id="2-安装与使用"><a href="#2-安装与使用" class="headerlink" title="2. 安装与使用"></a>2. 安装与使用</h3><h3 id="2-1-安装"><a href="#2-1-安装" class="headerlink" title="2.1 安装"></a>2.1 安装</h3><p>由于GPT4All, OpenAI Whisper 和TTS工具都是PyPI的包，因此所有的依赖都可以用pip 命令来安装。</p><p>流程大致上就是clone代码，创建Python虚拟环境，安装依赖，开始聊天：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/vra/talkGPT4All.git</span><br><span class="line"><span class="built_in">cd</span> talkGPT4All</span><br><span class="line">python -m venv talkgpt4all-env</span><br><span class="line"><span class="built_in">source</span> talkgpt4all-env/bin/activate</span><br><span class="line">pip install -U pip</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><p>如果在Linux环境下使用，还需要安装 TTS 工具 pyttsx3 的一些前置依赖，例如在Ubuntu下，可以这么安装（别的发行版切换apt 为对应的包管理命令应该就可以）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt update &amp;&amp; sudo apt install -y espeak ffmpeg libespeak1</span><br></pre></td></tr></table></figure><p>依赖安装完后即刻开始聊天：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python chat.py</span><br></pre></td></tr></table></figure><p>语音输入问题，Whisper会识别语音到文字，第一次需要下载模型Whisper的模型，可能耗时会比较久。Whisper 模型默认存储地址是<code>~/.cache/whisper/</code>。</p><p>文字识别后，输入到语言模型部分后会下载语言模型文件，文件默认存储到<code>~/.cache/gpt4all</code> 目录。</p><h3 id="2-2-切换不同的LLM"><a href="#2-2-切换不同的LLM" class="headerlink" title="2.2 切换不同的LLM"></a>2.2 切换不同的LLM</h3><p>默认的语言模型是GPT4All-J-v1.3,，可以通过命令行选项–gpt-model-name来切换模型，所有的选项是：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;ggml-gpt4all-j-v1.3-groovy&quot;</span></span><br><span class="line"><span class="string">&quot;ggml-gpt4all-j-v1.2-jazzy&quot;</span></span><br><span class="line"><span class="string">&quot;ggml-gpt4all-j-v1.1-breezy&quot;</span></span><br><span class="line"><span class="string">&quot;ggml-gpt4all-j&quot;</span></span><br><span class="line"><span class="string">&quot;ggml-gpt4all-l13b-snoozy&quot;</span></span><br><span class="line"><span class="string">&quot;ggml-vicuna-7b-1.1-q4_2&quot;</span></span><br><span class="line"><span class="string">&quot;ggml-vicuna-13b-1.1-q4_2&quot;</span></span><br><span class="line"><span class="string">&quot;ggml-wizardLM-7B.q4_2&quot;</span></span><br></pre></td></tr></table></figure><p>例如可以这样使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python chat.py --gpt-model-name ggml-wizardLM-7B.q4_2</span><br></pre></td></tr></table></figure><p>如果模型未下载过，会进行下载。</p><p>这里有个小问题，GPT4All工具貌似没有对模型的完整性进行校验，所以如果之前模型下载没完成就退出，再次进入后会加载不完整的文件，造成报错。所以需要手动删除不完整的文件再次完整下载后使用。</p><h3 id="2-3-切换不同大小的Whisper模型"><a href="#2-3-切换不同大小的Whisper模型" class="headerlink" title="2.3 切换不同大小的Whisper模型"></a>2.3 切换不同大小的Whisper模型</h3><p>OpenAI Whisper 也有一系列的模型，模型越大识别结果应该是越准。talkGPT4All默认使用的是base模型，也提供了命令行参数–whisper-model-type 来修改，所有的可选项是:</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot;tiny.en&quot;</span></span><br><span class="line"><span class="string">&quot;tiny&quot;</span></span><br><span class="line"><span class="string">&quot;base.en&quot;</span></span><br><span class="line"><span class="string">&quot;base&quot;</span></span><br><span class="line"><span class="string">&quot;small.en&quot;</span></span><br><span class="line"><span class="string">&quot;small&quot;</span></span><br><span class="line"><span class="string">&quot;medium.en&quot;</span></span><br><span class="line"><span class="string">&quot;medium&quot;</span></span><br><span class="line"><span class="string">&quot;large-v1&quot;</span></span><br><span class="line"><span class="string">&quot;large-v2&quot;</span></span><br><span class="line"><span class="string">&quot;large&quot;</span></span><br></pre></td></tr></table></figure><p>例如可以这样使用：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python chat.py --whisper-model-type large</span><br></pre></td></tr></table></figure><h3 id="2-4-调整声音语速"><a href="#2-4-调整声音语速" class="headerlink" title="2.4 调整声音语速"></a>2.4 调整声音语速</h3><p>talkGPT4All也提供了一个参数–voice rate 来调整 TTS发音的速度，默认是165，设置越大速度越快:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python chat.py --voice-rate 200</span><br></pre></td></tr></table></figure><h3 id="3-缺陷和改进思考"><a href="#3-缺陷和改进思考" class="headerlink" title="3. 缺陷和改进思考"></a>3. 缺陷和改进思考</h3><p>其实talkGPT4All一直以来的缺陷是比较明显的：</p><ol><li> 大模型在CPU上出词太慢</li><li> 离线的文本转语音的程序太生硬</li></ol><p>针对第一个问题，我的思考是这样，要在非Nvidia GPU设备上流畅运行基于Transformer结构的大语言模型，除了4比特量化、fp16这种 low-hang fruit外，必须要做很多底层的AI工程的优化，这个我觉得我自己是没有能力来完成的，甚至我猜测，可能GPT4All背后的Nomic AI团队也没有这方面的积累来解决这个问题。</p><p>可喜的是最近看到了<a href="https://github.com/mlc-ai/mlc-llm">MLC LLM</a>这个工作，是TVM 团队利用TVM Unity来优化语言模型，成功地将Vicuna-7B运行到了<a href="https://github.com/mlc-ai/mlc-llm/blob/main/android/README.md">Android</a>和<a href="https://github.com/mlc-ai/mlc-llm/blob/main/ios/README.md">iOS</a>手机上，我自己用小米12 Pro测试每秒能输出3～4个token，体验算是比较好的。这也是我第一次在自己手机上运行大语言模型，也意识到真正要提高大语言模型的覆盖设备，一个极致优化的底层AI工具是必不可少的。</p><p>所以对talkGPT4All这个项目感兴趣的朋友也可以了解一下MLC LLM这个工作，我认为在未来这个项目会促进LLM的真正落地。</p><p>针对第二个问题，说实话还没有找到比较自然的离线 TTS Python工具，如果看到这篇文章的你有这方面的经验，欢迎评论交流～</p>]]></content>
    
    
    <summary type="html">&lt;h3 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h3&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/vra/talkGPT4All&quot;&gt;talkGPT4All&lt;/a&gt;是基于&lt;a href=&quot;https://gpt4all.io/index.html&quot;&gt;GPT4All&lt;/a&gt;的一个语音聊天程序，运行在本地CPU上，支持Linux，Mac和Windows。它利用OpenAI的Whisper模型将用户输入的语音转换为文本，再调用GPT4All的语言模型得到回答文本，最后利用文本转语音(TTS)的程序将回答文本朗读出来。&lt;/p&gt;
&lt;p&gt;关于 talkGPT4All 1.0的介绍在&lt;a href=&quot;https://juejin.cn/post/7217112585802498107&quot;&gt;这篇文章&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;talkGPT4All 1.0的&lt;a href=&quot;https://www.zhihu.com/zvideo/1625779747656515584&quot;&gt;视频效果&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;由于GPT4All一直在迭代，相比上一篇文章发布时(2023-04-10)已经有较大的更新，今天将GPT4All的一些更新同步到talkGPT4All，由于支持的模型和运行模式都有较大的变化，因此发布 talkGPT4All 2.0。&lt;/p&gt;
&lt;p&gt;具体来说，2.0版本相比1.0有下面的更新。&lt;/p&gt;
&lt;p&gt;首先是GPT4All框架支持的语言模型从1个增加到8个，并且可以一键切换模型。具体的模型是&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;  Vicuna-7B-1.1-q4_2&lt;/li&gt;
&lt;li&gt;  Vicuna-7B-1.2-q4_2&lt;/li&gt;
&lt;li&gt;  wizardLM-7B.q4_2&lt;/li&gt;
&lt;li&gt;  GPT4All&lt;/li&gt;
&lt;li&gt;  GPT4All-J&lt;/li&gt;
&lt;li&gt;  GPT4All-J-v1.1&lt;/li&gt;
&lt;li&gt;  GPT4All-J-v1.2&lt;/li&gt;
&lt;li&gt;  GPT4All-J-v1.3&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;可以看到除了GPT4All系列的模型，这个框架也支持Vicuna和Wizard的模型了。更多模型因为证书和格式的问题，还在集成中。&lt;/p&gt;
&lt;p&gt;根据GPT4All的&lt;a href=&quot;https://gpt4all.io/index.html&quot;&gt;文档&lt;/a&gt;，不同模型在benchmark上的结果：&lt;/p&gt;
&lt;p&gt;&lt;img data-src=&quot;https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/988ae9ef513049d68d790e742f9e2139~tplv-k3u1fbpfcp-zoom-1.image&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以看到GPT4All系列的模型的指标还是比较高的。&lt;/p&gt;
&lt;p&gt;另一个重要更新是GPT4All发布了更成熟的Python包，可以直接通过pip 来安装，因此1.0中集成的不同平台不同的GPT4All二进制包也不需要了。集成PyPI包的好处多多，既可以查看源码学习内部的实现，又更方便定位问题（之前的二进制包没法调试内部代码），且保证了不同平台安装命令一致（之前不同平台二进制包不同）。&lt;/p&gt;
&lt;p&gt;还有一个变化是GPT4All会自动按需下载模型，因此用户不需要手动下载和维护模型路径。同时将模型统一放置到&lt;a href=&quot;https://gpt4all.io/models/&quot;&gt;https://gpt4all.io/models/&lt;/a&gt; 目录下，测试国内模型下载速度也很快，大家玩起来也会更舒服。&lt;/p&gt;
&lt;p&gt;核心的更新内容就这些，下面对talkGPT4All的安装和使用进行说明，后面有空会添加一些多个语言模型效果的对比视频。&lt;/p&gt;</summary>
    
    
    
    
    <category term="GPT" scheme="http://vra.github.io/tags/GPT/"/>
    
    <category term="LLM" scheme="http://vra.github.io/tags/LLM/"/>
    
    <category term="GPT4All" scheme="http://vra.github.io/tags/GPT4All/"/>
    
  </entry>
  
  <entry>
    <title>mac 编译问题解决——building for macOS-x86_64 but attempting to link with file built for xxx</title>
    <link href="http://vra.github.io/2023/05/27/mac-ranlib-issue/"/>
    <id>http://vra.github.io/2023/05/27/mac-ranlib-issue/</id>
    <published>2023-05-26T17:17:15.000Z</published>
    <updated>2023-05-26T17:19:20.646Z</updated>
    
    <content type="html"><![CDATA[<p>在编译TVM的一个<a href="https://github.com/mlc-ai/relax">fork版本</a>时，遇到下面的报错：</p><blockquote><p>ld: warning: ignoring file libbacktrace/lib/libbacktrace.a, building for macOS-x86_64 but attempting to link with file built for unknown-unsupported file format ( 0x21 0x3C 0x61 0x72 0x63 0x68 0x3E 0x0A<br> 0x2F 0x20 0x20 0x20 0x20 0x20 0x20 0x20 )<br>Undefined symbols for architecture x86_64:<br>  “_backtrace_create_state”, referenced from:<br>      __GLOBAL__sub_I_logging.cc in logging.cc.o<br>  “_backtrace_full”, referenced from:<br>      tvm::runtime::Backtrace() in logging.cc.o<br>  “_backtrace_syminfo”, referenced from:<br>      tvm::runtime::(anonymous namespace)::BacktraceFullCallback(void*, unsigned long, char const*, int, char const*) in logging.cc.o<br>ld: symbol(s) not found for architecture x86_64<br>clang: error: linker command failed with exit code 1 (use -v to see invocation)<br>make[3]: *** [libtvm_runtime.dylib] Error 1<br>make[2]: *** [CMakeFiles/tvm_runtime.dir/all] Error 2<br>make[2]: *** Waiting for unfinished jobs….</p></blockquote><p>搜索了一下，发现核心原因是Mac下ranlib命令采用了GNU版本，而非Apple版本导致的，下面详细展开报错原因和解决办法。</p><span id="more"></span><p>在Mac下，有两套编译工具链，GNU的和Apple（通过Xcode安装）的，GNU的以<code>gcc</code>为代表，而Apple的则以<code>clang</code>为代表，在这两个核心编译工具周围，又有很多别的小的编译工具。</p><p>通过log输出发现，编译工具用的是<code>/usr/bin/cc</code>, 执行<code>/usr/bin/cc --version</code> 命令，输出如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ /usr/bin/cc --version</span><br><span class="line">Apple clang version 14.0.0 (clang-1400.0.29.202)</span><br><span class="line">Target: x86_64-apple-darwin22.2.0</span><br><span class="line">Thread model: posix</span><br><span class="line">InstalledDir: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin</span><br></pre></td></tr></table></figure><p>可以看到是Apple的编译工具链Apple clang。</p><p>在编译过程中，发现log中有下面的输出：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ibtool: install: ranlib /path/to/relax/build/libbacktrace/lib/libbacktrace.a</span><br></pre></td></tr></table></figure><p>可以看到调用了<code>ranlib</code>命令来生成<code>libbacktrace.a</code>。</p><p>通过<code>which ranlib</code> 验证ranlib的路径：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">which</span> ranlib</span><br><span class="line">/usr/<span class="built_in">local</span>/opt/binutils/bin/ranlib</span><br><span class="line"></span><br><span class="line">$ ranlib --version</span><br><span class="line">GNU ranlib (GNU Binutils) 2.40</span><br><span class="line">Copyright (C) 2023 Free Software Foundation, Inc.</span><br><span class="line">This program is free software; you may redistribute it under the terms of</span><br><span class="line">the GNU General Public License version 3 or (at your option) any later version.</span><br><span class="line">This program has absolutely no warranty.</span><br></pre></td></tr></table></figure><p>可以看到，找到的是GPN版本的ranlib，而不是跟编译工具匹配的Apple的ranlib（路径是<code>/usr/bin/ranlib</code>)。</p><p>如果是Apple的ranlib工具的话，<code>ranlib --version</code>输出应该是下面这样：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ranlib</span> --version</span><br><span class="line">error: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib: unknown option character `-<span class="string">&#x27; in: --version</span></span><br><span class="line"><span class="string">Usage: /Applications/Xcode.app/Contents/Developer/Toolchains/XcodeDefault.xctoolchain/usr/bin/ranlib [-sactfqLT] [-] archive [...]</span></span><br></pre></td></tr></table></figure><p>那为什么会有两套工具链混合使用导致出错的问题？这是因为路径设置优先级的原因，在PATH中，<code>/usr/local/opt/binutils/bin</code>在<code>/usr/bin</code>的前面：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="variable">$PATH</span></span><br><span class="line">...:/usr/<span class="built_in">local</span>/opt/binutils/bin:/usr/bin:...</span><br></pre></td></tr></table></figure><p>所以在搜索可执行文件时，先找到了GNU的ranlib，而这个又与Apple的编译工具链不兼容。导致编译出错。</p><p>那<code>ranlib</code>是干什么用的呢？根据ChatGPT， ranlib功能如下：</p><blockquote><p>ranlib是一个命令行工具，用于在静态库中创建索引（也称为符号表）。索引提供静态库中所有符号（函数、变量等）的列表。它帮助编译器和链接器在链接时更快地查找和解析符号。当一个程序需要链接静态库时，链接器会使用ranlib创建的索引来确定静态库中包含的符号，以便正确地链接程序。</p></blockquote><p>可以看到，ranlib对于编译静态库来说，是必不可少的（与<code>ar -s</code>完全等效）。</p><p>其实我不记得在PATH中添加过<code>/usr/local/opt/binutils/bin</code>这个目录，应该是安装某些包后自动更新的。</p><p>那这个问题该怎么解决呢？通过上面的分析，我们也能发现其实解决办法也比较直观，总体来说有两种，一种是修改PATH中两个目录的寻找优先级，保证先找到的是Apple的工具，也就是<code>/usr/bin</code>目录在<code>/usr/local/opt</code> 前面；另一种是直接卸载GNU的工具<code>binutils</code>，这样就不会有冲突。</p><p>在这里我选择执行第二种，具体命令为：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ brew uninstall binutils</span><br></pre></td></tr></table></figure><p>然后再检查<code>ranlib --version</code> 命令的输出，确认是Apple的工具链后再<code>make clean</code>，重新编译即可。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ol><li><a href="https://stackoverflow.com/a/72904009">https://stackoverflow.com/a/72904009</a></li><li><a href="https://github.com/bitcoin/bitcoin/issues/20825#issuecomment-753444519">https://github.com/bitcoin/bitcoin/issues/20825#issuecomment-753444519</a></li></ol>]]></content>
    
    
    <summary type="html">&lt;p&gt;在编译TVM的一个&lt;a href=&quot;https://github.com/mlc-ai/relax&quot;&gt;fork版本&lt;/a&gt;时，遇到下面的报错：&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ld: warning: ignoring file libbacktrace/lib/libbacktrace.a, building for macOS-x86_64 but attempting to link with file built for unknown-unsupported file format ( 0x21 0x3C 0x61 0x72 0x63 0x68 0x3E 0x0A&lt;br&gt; 0x2F 0x20 0x20 0x20 0x20 0x20 0x20 0x20 )&lt;br&gt;Undefined symbols for architecture x86_64:&lt;br&gt;  “_backtrace_create_state”, referenced from:&lt;br&gt;      __GLOBAL__sub_I_logging.cc in logging.cc.o&lt;br&gt;  “_backtrace_full”, referenced from:&lt;br&gt;      tvm::runtime::Backtrace() in logging.cc.o&lt;br&gt;  “_backtrace_syminfo”, referenced from:&lt;br&gt;      tvm::runtime::(anonymous namespace)::BacktraceFullCallback(void*, unsigned long, char const*, int, char const*) in logging.cc.o&lt;br&gt;ld: symbol(s) not found for architecture x86_64&lt;br&gt;clang: error: linker command failed with exit code 1 (use -v to see invocation)&lt;br&gt;make[3]: *** [libtvm_runtime.dylib] Error 1&lt;br&gt;make[2]: *** [CMakeFiles/tvm_runtime.dir/all] Error 2&lt;br&gt;make[2]: *** Waiting for unfinished jobs….&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;搜索了一下，发现核心原因是Mac下ranlib命令采用了GNU版本，而非Apple版本导致的，下面详细展开报错原因和解决办法。&lt;/p&gt;</summary>
    
    
    
    
    <category term="总结" scheme="http://vra.github.io/tags/%E6%80%BB%E7%BB%93/"/>
    
    <category term="macOS" scheme="http://vra.github.io/tags/macOS/"/>
    
    <category term="GNU" scheme="http://vra.github.io/tags/GNU/"/>
    
    <category term="Apple" scheme="http://vra.github.io/tags/Apple/"/>
    
    <category term="XCode" scheme="http://vra.github.io/tags/XCode/"/>
    
    <category term="ranlib" scheme="http://vra.github.io/tags/ranlib/"/>
    
    <category term="binutils" scheme="http://vra.github.io/tags/binutils/"/>
    
    <category term="TVM" scheme="http://vra.github.io/tags/TVM/"/>
    
  </entry>
  
  <entry>
    <title>用 Material for MkDocs 来生成专业的技术文档</title>
    <link href="http://vra.github.io/2023/05/17/mkdocs-material-tutorial/"/>
    <id>http://vra.github.io/2023/05/17/mkdocs-material-tutorial/</id>
    <published>2023-05-17T11:31:37.000Z</published>
    <updated>2023-05-17T11:37:34.893Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>对于程序员来说，写技术文档是一项必备的技能。由于GitHub和Markdown格式的普及，很多时候我们可以用markdown来简便地写出技术文档，并且 通过GitHub Pages等工具快速地进行技术文档的部署。</p><p>虽然GItHub Pages默认支持静态文档框架<a href="https://jekyllrb.com/">Jekyll</a>，也包含一些简单的<a href="https://pages.github.com/themes/">主题</a>，但对于文档和教程比较多的项目来说，使用GitHub Pages的默认部署工具还不够用，主要体现在下面几个方面：</p><ul><li>Markdown本身支持的语法比较简单，一些复杂的像Warning等提示没法直接用Pages的默认主题来实现</li><li>Pages 默认显示的是单页文档，没有侧边栏、导航栏等工具</li><li>Pages 默认主题无法搜索文档内容</li><li>Pages 不支持选择<code>Linux</code>或<code>Windows</code> 后显示不同执行语句的功能</li><li>…</li></ul><p><a href="https://squidfunk.github.io/mkdocs-material/">Material for MkDocs</a> 是 <a href="https://www.mkdocs.org/">MkDocs</a>的一个主题配置，同时也是一个功能齐全的静态网站生成工具，能够解决上面提到的GitHub Pages的问题。</p><p>Material for MkDocs 使用广泛，下面是一些大公司和知名开源项目的使用例子：</p><ul><li><a href="https://aws.github.io/copilot-cli/">AWS Copilot CLI </a></li><li><a href="https://google.github.io/accompanist/">Google Accompanist</a></li><li><a href="https://microsoft.github.io/code-with-engineering-playbook/">MicroSoft Code With Engineering Playbook</a></li><li><a href="https://mozillafoundation.github.io/engineering-handbook/">Mozilla Foundation Engineering Handbook</a></li><li><a href="https://netflix.github.io/titus/">Netflix Titus</a></li><li><a href="https://docs.infra.centos.org/">CentOS Infra docs</a></li><li><a href="https://www.electron.build/">electron-builder</a></li><li><a href="https://kops.sigs.k8s.io/">Kubernetes</a></li></ul><p>虽然我还没有比较复杂的开源项目需要用mkdocs-material来管理文档，但看到GitHub Pages的一些限制，最近有空还是学了一下这个工具，以备后续项目中使用。这里做一些简单记录，方便以后查找。</p><p>需要说明的是，Material for MkDocs 是一个比较复杂的工具，很多配置项这里没有提到，根据需要在官方<a href="https://squidfunk.github.io/mkdocs-material/setup/">Setup</a>文档中查看使用说明。</p><p>另外一种学习配置的方式是直接查看上面提到的开源项目源码根目录下的<code>mkdocs.yml</code>文件，复制这个文件过去，就能得到类似的布局效果。</p><p>这个教程里面的示例页面：<a href="https://vra.github.io/mkdocs-material-example/">https://vra.github.io/mkdocs-material-example/</a><br>示例页面的配置文件：<a href="https://github.com/vra/mkdocs-material-example/blob/main/mkdocs.yml">https://github.com/vra/mkdocs-material-example/blob/main/mkdocs.yml</a></p><span id="more"></span><h2 id="2-1-安装"><a href="#2-1-安装" class="headerlink" title="2.1 安装"></a>2.1 安装</h2><p>可以直接使用 <code>pip</code> 来安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install mkdocs-material</span><br></pre></td></tr></table></figure><p>使用下面的命令测试是否安装成功：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mkdocs -h</span><br></pre></td></tr></table></figure><p>其他从docker安装、从GitHub安装的方式参考<a href="https://squidfunk.github.io/mkdocs-material/getting-started/#with-docker">官方文档</a>。</p><h3 id="2-2-使用"><a href="#2-2-使用" class="headerlink" title="2.2 使用"></a>2.2 使用</h3><p>mkdocs-material 的使用命令比较简单，概括来说就是三板斧：</p><ol><li><code>mkdocs new .</code>： 在当前目录生成<code>docs</code>目录和<code>mkdocs.yml</code> 配置文件</li><li><code>mkdocs serve</code>： 在本地运行文档生成服务，可在浏览器中访问<code>localhost:8000</code>查看文档的效果</li><li><code>mkdocs build</code>： 非必需，在<code>sites</code> 目录中生成最终的HTML文件</li></ol><p>由于命令比较简单，没有什么太多东西，因而核心要做的事情其实是：</p><ul><li>写markdown 格式的文档文件</li><li>修改配置文件<code>mkdocs.yml</code></li></ul><p>在<code>mkdocs serve</code> 运行的过程中，更新完 <code>mkdocs.yml</code>配置文件后，文档生成效果实时更新。</p><h3 id="2-3-上传文档到-GitHub-Pages"><a href="#2-3-上传文档到-GitHub-Pages" class="headerlink" title="2.3 上传文档到 GitHub Pages"></a>2.3 上传文档到 GitHub Pages</h3><p>mkdocs-material 一个很棒的特性是可以一键将代码部署到GIthub Pages上，并且通过GitHub Actions配置，Push 代码时自动更新文档。<br>假如你的GitHub 仓库地址是<code>https://github.com/user/repo</code>，那完成配置后你就可以在<code>https://user.github.io/repo</code> 网址查看你的mkdocs-material 文档了。</p><p>具体来说，假设你已经创建了一个Git 仓库，需要做下面的事情：</p><ol><li>将<code>mkdocs.yml</code> 和<code>docs</code> 目录提交到Git仓库</li><li>增加GitHub Action 配置文件<code>.github/workflows/ci.yml</code>，写入下面的内容并提交到GitHub:<figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">name:</span> <span class="string">ci</span> </span><br><span class="line"><span class="attr">on:</span></span><br><span class="line">  <span class="attr">push:</span></span><br><span class="line">    <span class="attr">branches:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">master</span> </span><br><span class="line">      <span class="bullet">-</span> <span class="string">main</span></span><br><span class="line"><span class="attr">permissions:</span></span><br><span class="line">  <span class="attr">contents:</span> <span class="string">write</span></span><br><span class="line"><span class="attr">jobs:</span></span><br><span class="line">  <span class="attr">deploy:</span></span><br><span class="line">    <span class="attr">runs-on:</span> <span class="string">ubuntu-latest</span></span><br><span class="line">    <span class="attr">steps:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">uses:</span> <span class="string">actions/checkout@v3</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">uses:</span> <span class="string">actions/setup-python@v4</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">python-version:</span> <span class="number">3.</span><span class="string">x</span></span><br><span class="line">      <span class="bullet">-</span> <span class="attr">run:</span> <span class="string">echo</span> <span class="string">&quot;cache_id=$(date --utc &#x27;+%V&#x27;)&quot;</span> <span class="string">&gt;&gt;</span> <span class="string">$GITHUB_ENV</span> </span><br><span class="line">      <span class="bullet">-</span> <span class="attr">uses:</span> <span class="string">actions/cache@v3</span></span><br><span class="line">        <span class="attr">with:</span></span><br><span class="line">          <span class="attr">key:</span> <span class="string">mkdocs-material-$&#123;&#123;</span> <span class="string">env.cache_id</span> <span class="string">&#125;&#125;</span></span><br><span class="line">          <span class="attr">path:</span> <span class="string">.cache</span></span><br><span class="line">          <span class="attr">restore-keys:</span> <span class="string">|</span></span><br><span class="line"><span class="string">            mkdocs-material-</span></span><br><span class="line"><span class="string"></span>      <span class="bullet">-</span> <span class="attr">run:</span> <span class="string">pip</span> <span class="string">install</span> <span class="string">mkdocs-material</span> </span><br><span class="line">      <span class="bullet">-</span> <span class="attr">run:</span> <span class="string">mkdocs</span> <span class="string">gh-deploy</span> <span class="string">--force</span></span><br><span class="line"></span><br></pre></td></tr></table></figure></li><li>在GitHub仓库的<code>Settings</code> -&gt; <code>Pages</code> -&gt; <code>Build and deployment</code> 部分，Source 选项选择”Deploy from a branch”, Branch 选择<code>gh-pages</code>, folder选择<code>/(root)</code><br>经过这个配置后，每次向<code>master</code> 或<code>main</code> 分支push代码，会自动更新<code>user.github.io/repo</code>下的文档。</li></ol>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h2&gt;&lt;p&gt;对于程序员来说，写技术文档是一项必备的技能。由于GitHub和Markdown格式的普及，很多时候我们可以用markdown来简便地写出技术文档，并且 通过GitHub Pages等工具快速地进行技术文档的部署。&lt;/p&gt;
&lt;p&gt;虽然GItHub Pages默认支持静态文档框架&lt;a href=&quot;https://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt;，也包含一些简单的&lt;a href=&quot;https://pages.github.com/themes/&quot;&gt;主题&lt;/a&gt;，但对于文档和教程比较多的项目来说，使用GitHub Pages的默认部署工具还不够用，主要体现在下面几个方面：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Markdown本身支持的语法比较简单，一些复杂的像Warning等提示没法直接用Pages的默认主题来实现&lt;/li&gt;
&lt;li&gt;Pages 默认显示的是单页文档，没有侧边栏、导航栏等工具&lt;/li&gt;
&lt;li&gt;Pages 默认主题无法搜索文档内容&lt;/li&gt;
&lt;li&gt;Pages 不支持选择&lt;code&gt;Linux&lt;/code&gt;或&lt;code&gt;Windows&lt;/code&gt; 后显示不同执行语句的功能&lt;/li&gt;
&lt;li&gt;…&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;a href=&quot;https://squidfunk.github.io/mkdocs-material/&quot;&gt;Material for MkDocs&lt;/a&gt; 是 &lt;a href=&quot;https://www.mkdocs.org/&quot;&gt;MkDocs&lt;/a&gt;的一个主题配置，同时也是一个功能齐全的静态网站生成工具，能够解决上面提到的GitHub Pages的问题。&lt;/p&gt;
&lt;p&gt;Material for MkDocs 使用广泛，下面是一些大公司和知名开源项目的使用例子：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://aws.github.io/copilot-cli/&quot;&gt;AWS Copilot CLI &lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://google.github.io/accompanist/&quot;&gt;Google Accompanist&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://microsoft.github.io/code-with-engineering-playbook/&quot;&gt;MicroSoft Code With Engineering Playbook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://mozillafoundation.github.io/engineering-handbook/&quot;&gt;Mozilla Foundation Engineering Handbook&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://netflix.github.io/titus/&quot;&gt;Netflix Titus&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://docs.infra.centos.org/&quot;&gt;CentOS Infra docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.electron.build/&quot;&gt;electron-builder&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://kops.sigs.k8s.io/&quot;&gt;Kubernetes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;虽然我还没有比较复杂的开源项目需要用mkdocs-material来管理文档，但看到GitHub Pages的一些限制，最近有空还是学了一下这个工具，以备后续项目中使用。这里做一些简单记录，方便以后查找。&lt;/p&gt;
&lt;p&gt;需要说明的是，Material for MkDocs 是一个比较复杂的工具，很多配置项这里没有提到，根据需要在官方&lt;a href=&quot;https://squidfunk.github.io/mkdocs-material/setup/&quot;&gt;Setup&lt;/a&gt;文档中查看使用说明。&lt;/p&gt;
&lt;p&gt;另外一种学习配置的方式是直接查看上面提到的开源项目源码根目录下的&lt;code&gt;mkdocs.yml&lt;/code&gt;文件，复制这个文件过去，就能得到类似的布局效果。&lt;/p&gt;
&lt;p&gt;这个教程里面的示例页面：&lt;a href=&quot;https://vra.github.io/mkdocs-material-example/&quot;&gt;https://vra.github.io/mkdocs-material-example/&lt;/a&gt;&lt;br&gt;示例页面的配置文件：&lt;a href=&quot;https://github.com/vra/mkdocs-material-example/blob/main/mkdocs.yml&quot;&gt;https://github.com/vra/mkdocs-material-example/blob/main/mkdocs.yml&lt;/a&gt;&lt;/p&gt;</summary>
    
    
    
    
    <category term="Docs" scheme="http://vra.github.io/tags/Docs/"/>
    
    <category term="Markdown" scheme="http://vra.github.io/tags/Markdown/"/>
    
  </entry>
  
  <entry>
    <title>Rye:一个实验性质的Python包管理系统</title>
    <link href="http://vra.github.io/2023/05/17/rye-intro/"/>
    <id>http://vra.github.io/2023/05/17/rye-intro/</id>
    <published>2023-05-17T02:08:32.000Z</published>
    <updated>2023-05-17T02:17:20.438Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://mitsuhiko.github.io/rye/">Rye</a> 是<a href="https://flask.palletsprojects.com/en/2.3.x/">Flask</a>的作者<a href="https://github.com/mitsuhiko">Armin Ronacher</a>最近推出的一个实验性质的Python包管理系统，目的是解决Python包管理目前面临的工具链碎片化的问题。</p><p>大家知道，Python目前的包管理系统很多，包括 poetry, pip, pipenv, pyenv, venv, virtualenv, pdm, hatch 等等，它们都是优秀的工具，提出时都是解决了一定的问题，但没有哪个工具能够做到主流，因此也增加了系统的碎片化程度。</p><p>另一方面，conda等工具能提供不同版本的 Python，管理不同的环境，但每个环境的 Python 不是共享的，环境创建一多，环境目录就变得很大，且内部机制很不透明，有时也会遇到冲突没法解决的问题。</p><p>另一方面，Python 在Linux/macOS上的安装也面临一些问题，例如用包管理器安装的  Python和用户手动安装的 Python 有的时候会混淆，导致一些混乱，例如在 Fedora 上，用<code>pip install</code> 安装包可能会导致系统的包管理命令<code>dnf</code> 出错。<a href="https://peps.python.org/pep-0668">PEP 668</a>尝试对这些问题给出一个解决方案，但也需要不同的系统来支持，目前看还任重道远。</p><p>由于Armin也是一个Rust 开发者，而Rust基于标准化的<code>rustup</code>和<code>cargo</code>两个工具，配合配置文件来进行包管理，目前做的比较好，没有Python面临的碎片化问题。受Rust的启发，作者提出了Rye，并且期望能够启发Python社区提出类似Rust的标准包管理工具。</p><p>具体来说，Rye 提出了一些解决这些问题的思路：</p><ul><li>提出一个workspace的概念，workspace类似一个项目目录，或者一个git仓库，一个workspace下只有一个Python版本，不同workspace Python版本相互隔离，每个项目采用<code>pyproject.toml</code>来进行配置</li><li>不使用系统自带的Python，相反地，在每个项目目录的中下载一个standalone的python，解决不同版本的冲突问题</li><li>不暴露pip命令，通过<code>rye add</code> + <code>rye sync</code> 来管理包的依赖，避免包A和包B依赖不同版本的包C而导致的不兼容问题</li><li>区分开发环境和正式环境，因为一些包在开发时会用到一些调试工具，但作为第三方库被引入的时候并不需要</li><li>支持import本地workspace作为第三方库包</li></ul><p>但同时也有一个问题：rye会不会是另一个做不到主流的Python包管理系统，从而进一步增加Python包管理的碎片化呢？作者也有这个考虑，因此写了一个讨论帖 <a href="https://github.com/mitsuhiko/rye/discussions/6">Should Rye Exist?</a>来讨论这个问题，同时关于Rye的设计初衷，可以参考<a href="https://mitsuhiko.github.io/rye/philosophy/">这里</a>作者的思考。</p><p>个人观点：Rye的出现给Python社区引入了一些新鲜的解决现有问题的思路。使用Rye一段时间后，发现至少使用standalone 的Python版本是一个解决冲突的好的方式。通过几个简单的命令来解决版本管理的问题是比较直观的，提出Rye应该是利大于弊的，也就是有益程度大于碎片化增加的程度。</p><p>总之不管是<a href="https://peps.python.org/pep-0668">PEP 668</a>中标记版本管理是系统的还是Python的，还是<a href="https://peps.python.org/pep-0711/">PEP 711</a>来单独下发Python解释器二进制文件，还是Rye的出现，都是Python社区意识到Python包管理问题的严重性，进而做出的一些有益尝试。期待在未来，有更标准化的工具，Python的开发也更容易。</p><p>下面将对Rye的安装和使用进行简单介绍。</p><span id="more"></span><h3 id="2-1-安装rustup"><a href="#2-1-安装rustup" class="headerlink" title="2.1 安装rustup"></a>2.1 安装rustup</h3><p>Rye是基于Rust 开发的，而Rust 有标准的包安装工具<code>cargo</code>，Rust编译器和<code>cargo</code>都需要用<code>rustup</code>来安装，因此安装预编译的Rye包需要先安装<code>rustup</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl --proto <span class="string">&#x27;=https&#x27;</span> --tlsv1.2 -sSf https://sh.rustup.rs | sh</span><br></pre></td></tr></table></figure><p>执行完后，重启Shell，输入<code>cargo -V</code>，如果不报错，说明安装成功。</p><h3 id="2-2-安装Rye"><a href="#2-2-安装Rye" class="headerlink" title="2.2 安装Rye"></a>2.2 安装Rye</h3><p>有了cargo后，使用下面的命令安装Rye:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cargo install --git https://github.com/mitsuhiko/rye rye</span><br></pre></td></tr></table></figure><p>通过命令行执行<code>rye -h</code> 来判断 Rye是否安装成功。</p><p>同时可以将<code>$HOME/.rye/shims</code> 添加到环境变量<code>PATH</code> 中，这样打开Shell后运行<code>python</code> 就用的是Rye安装到standalone Python，否则你需要用<code>rye run python</code> 来启用Rye的Python解释器。</p><p>更新Rye到最新版：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rye self update</span><br></pre></td></tr></table></figure><p>删除Rye:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cargo uninstall rye</span><br></pre></td></tr></table></figure><h3 id="2-3-初始化一个Rye项目"><a href="#2-3-初始化一个Rye项目" class="headerlink" title="2.3 初始化一个Rye项目"></a>2.3 初始化一个Rye项目</h3><p>使用<code>rye init project-name</code> 来创建一个Rye项目目录</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rye init test_rye</span><br><span class="line"><span class="built_in">cd</span> test_rye</span><br><span class="line">tree</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">├── .git</span><br><span class="line">├── .gitignore</span><br><span class="line">├── .python-version</span><br><span class="line">├── README.md</span><br><span class="line">├── pyproject.toml</span><br><span class="line">└── src</span><br><span class="line">    └── test_rye</span><br><span class="line">        └── __init__.py</span><br></pre></td></tr></table></figure><p>可以看到创建了.git 目录， .gitignore 文件，README.md，配置文件<code>pyproject.toml</code> 和一个示例的源码文件<code>src/test_rye/__init__.py</code>。</p><h3 id="2-4-Python-版本管理"><a href="#2-4-Python-版本管理" class="headerlink" title="2.4 Python 版本管理"></a>2.4 Python 版本管理</h3><p>为了固定开发环境，我们可以利用<code>rye pin python-version</code> 来固定Python的版本，例如<code>rye pin cpython@3.10.11</code> 会将Python版本固定为3.10.11。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cpython@可以省略</span></span><br><span class="line">rye pin cpython@3.10.11</span><br><span class="line">rye pin 3.10.11</span><br></pre></td></tr></table></figure><p>由于默认使用的Python版本是Cpython的，因此在执行rye命令时可以将<code>Cpython@</code> 前缀省去。</p><p>注意 <code>rye pin</code>命令并不立即改变Python的版本，只是修改配置文件<code>.python-version</code>，在<code>rye sync</code> 执行时才进行实际的修改。</p><p>可以多次执行<code>rye pin</code> 来调整Python的版本。</p><p>然后执行<code>rye sync</code> 来同步配置，具体来说，第一次执行这个命令的时候，Rye会下载一个单独的Python解释器，放置到<code>$HOME/.rye/py</code>目录下，链接到项目的<code>.venv</code> 目录下，因此同一个Python版本在磁盘上只有一份，这与conda是不同的。</p><p>更一般地，可以使用<code>rye toolchain</code> 来查看、拉取和删除Python版本。</p><p><code>rye toolchain list</code> 用来显示所有已经安装的Python版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rye toolcahin list</span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cpython@3.11.3 (/Users/yunfeng/.rye/py/cpython@3.11.3/install/bin/python3)</span><br><span class="line">cpython@3.11.1 (/Users/yunfeng/.rye/py/cpython@3.11.1/install/bin/python3)</span><br><span class="line">cpython@3.10.11 (/Users/yunfeng/.rye/py/cpython@3.10.11/install/bin/python3)</span><br><span class="line">cpython@3.10.9 (/Users/yunfeng/.rye/py/cpython@3.10.9/install/bin/python3)</span><br></pre></td></tr></table></figure><p><code>rye toolchain list --include-downloadable</code> 会列出所有可以下载的Python版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">`rye toolchain list --include-downloadable` </span><br></pre></td></tr></table></figure><p>输出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cpython@3.10.8 (downloadable)</span><br><span class="line">cpython@3.10.7 (downloadable)</span><br><span class="line">cpython@3.10.6 (downloadable)</span><br><span class="line">cpython@3.10.5 (downloadable)</span><br><span class="line">cpython@3.10.4 (downloadable)</span><br><span class="line">cpython@3.10.3 (downloadable)</span><br><span class="line">cpython@3.10.2 (downloadable)</span><br><span class="line">cpython@3.10.0 (downloadable)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>注意已经下载的Python版本不在这个输出中。</p><p><code>rye toolchain fetch</code>（简写为<code>rye fetch</code>) 可以直接拉取某个Python版本:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rye toolchain fetch 3.8.16</span><br></pre></td></tr></table></figure><p><code>rye toolchain remove</code> 可以删除某个Python版本：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rye toolchain remove 3.8.16</span><br></pre></td></tr></table></figure><h3 id="2-5-添加依赖包"><a href="#2-5-添加依赖包" class="headerlink" title="2.5 添加依赖包"></a>2.5 添加依赖包</h3><p>可以通过<code>rye add package-name</code> 来安装像numpy等第三方，这个命令支持安装GitHub和本地的包，一些示例的用法如下:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">rye add numpy</span><br><span class="line"><span class="comment"># 同时安装几个包</span></span><br><span class="line">rye add six easydict</span><br><span class="line"><span class="comment"># 设置安装包的版本</span></span><br><span class="line">rye add <span class="string">&quot;Flask&gt;=2.0&quot;</span></span><br><span class="line"><span class="comment"># 只在development环境添加包</span></span><br><span class="line">rye add --dev black</span><br><span class="line"><span class="comment"># 添加github上的包</span></span><br><span class="line">rye add Flask --git=https://github.com/pallets/flask</span><br><span class="line"><span class="comment"># 添加本地目录的包</span></span><br><span class="line">rye add My-Utility --path ./my-utility</span><br></pre></td></tr></table></figure><p>同样的，<code>rye add</code>并不会实际安装包，只会修改配置文件<code>pyproject.toml</code> 中的<code>dependencies</code> 项，等执行<code>rye sync</code>的时候才真正安装。</p><h3 id="2-6-Rye工作流"><a href="#2-6-Rye工作流" class="headerlink" title="2.6 Rye工作流"></a>2.6 Rye工作流</h3><p>我自己探索的Rye工作流大概是这样：</p><ol><li><code>rye init project-name</code> 来初始化项目目录</li><li><code>git add</code> 和<code>git commit</code> 来提交初始状态的代码，方便定位后续代码和配置文件的更新</li><li><code>rye pin</code> 指定Python版本</li><li>修改代码，<code>rye add package-name</code> 来增加代码依赖的包</li><li><code>rye sync</code>来安装Python，安装依赖包，更新配置文件</li><li><code>rye run python</code> 执行代码测试</li><li>可选：<code>rye build</code> 来生成可发布的wheel文件</li><li>可选：<code>rye publish</code> 上传包到pypi</li></ol><p>需要注意的是，Rye只负责依赖管理，具体的调试代码工作，还需要自己来进行，使用你熟悉的代码测试方式就可以了。</p><p>额外补充一下，可以使用<code>rye shell</code> 来打开一个新的启用了Rye Python的Shell来进行代码调试。</p><h3 id="2-7-安装可执行的-global-Python工具"><a href="#2-7-安装可执行的-global-Python工具" class="headerlink" title="2.7 安装可执行的 global Python工具"></a>2.7 安装可执行的 global Python工具</h3><p>某些python包除了包含Python源码外，还包含一些命令行工具，Rye称这些工具为<code>global tool</code> ，因为它们不是在某个环境中才能使用，而是全局可使用的。这些工具可以用<code>rye install package-name</code>来安装，例如:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rye install black</span><br></pre></td></tr></table></figure><p>使用方式为<code>rye run tool-name</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rye run black -h</span><br></pre></td></tr></table></figure><p>这些包都存放在<code>$HOME/.rye/shims</code> 目录下。<br>如果要删除 global tool，可以使用<code>rye uninstall</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rye uninstall black</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;&lt;a href=&quot;https://mitsuhiko.github.io/rye/&quot;&gt;Rye&lt;/a&gt; 是&lt;a href=&quot;https://flask.palletsprojects.com/en/2.3.x/&quot;&gt;Flask&lt;/a&gt;的作者&lt;a href=&quot;https://github.com/mitsuhiko&quot;&gt;Armin Ronacher&lt;/a&gt;最近推出的一个实验性质的Python包管理系统，目的是解决Python包管理目前面临的工具链碎片化的问题。&lt;/p&gt;
&lt;p&gt;大家知道，Python目前的包管理系统很多，包括 poetry, pip, pipenv, pyenv, venv, virtualenv, pdm, hatch 等等，它们都是优秀的工具，提出时都是解决了一定的问题，但没有哪个工具能够做到主流，因此也增加了系统的碎片化程度。&lt;/p&gt;
&lt;p&gt;另一方面，conda等工具能提供不同版本的 Python，管理不同的环境，但每个环境的 Python 不是共享的，环境创建一多，环境目录就变得很大，且内部机制很不透明，有时也会遇到冲突没法解决的问题。&lt;/p&gt;
&lt;p&gt;另一方面，Python 在Linux/macOS上的安装也面临一些问题，例如用包管理器安装的  Python和用户手动安装的 Python 有的时候会混淆，导致一些混乱，例如在 Fedora 上，用&lt;code&gt;pip install&lt;/code&gt; 安装包可能会导致系统的包管理命令&lt;code&gt;dnf&lt;/code&gt; 出错。&lt;a href=&quot;https://peps.python.org/pep-0668&quot;&gt;PEP 668&lt;/a&gt;尝试对这些问题给出一个解决方案，但也需要不同的系统来支持，目前看还任重道远。&lt;/p&gt;
&lt;p&gt;由于Armin也是一个Rust 开发者，而Rust基于标准化的&lt;code&gt;rustup&lt;/code&gt;和&lt;code&gt;cargo&lt;/code&gt;两个工具，配合配置文件来进行包管理，目前做的比较好，没有Python面临的碎片化问题。受Rust的启发，作者提出了Rye，并且期望能够启发Python社区提出类似Rust的标准包管理工具。&lt;/p&gt;
&lt;p&gt;具体来说，Rye 提出了一些解决这些问题的思路：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;提出一个workspace的概念，workspace类似一个项目目录，或者一个git仓库，一个workspace下只有一个Python版本，不同workspace Python版本相互隔离，每个项目采用&lt;code&gt;pyproject.toml&lt;/code&gt;来进行配置&lt;/li&gt;
&lt;li&gt;不使用系统自带的Python，相反地，在每个项目目录的中下载一个standalone的python，解决不同版本的冲突问题&lt;/li&gt;
&lt;li&gt;不暴露pip命令，通过&lt;code&gt;rye add&lt;/code&gt; + &lt;code&gt;rye sync&lt;/code&gt; 来管理包的依赖，避免包A和包B依赖不同版本的包C而导致的不兼容问题&lt;/li&gt;
&lt;li&gt;区分开发环境和正式环境，因为一些包在开发时会用到一些调试工具，但作为第三方库被引入的时候并不需要&lt;/li&gt;
&lt;li&gt;支持import本地workspace作为第三方库包&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;但同时也有一个问题：rye会不会是另一个做不到主流的Python包管理系统，从而进一步增加Python包管理的碎片化呢？作者也有这个考虑，因此写了一个讨论帖 &lt;a href=&quot;https://github.com/mitsuhiko/rye/discussions/6&quot;&gt;Should Rye Exist?&lt;/a&gt;来讨论这个问题，同时关于Rye的设计初衷，可以参考&lt;a href=&quot;https://mitsuhiko.github.io/rye/philosophy/&quot;&gt;这里&lt;/a&gt;作者的思考。&lt;/p&gt;
&lt;p&gt;个人观点：Rye的出现给Python社区引入了一些新鲜的解决现有问题的思路。使用Rye一段时间后，发现至少使用standalone 的Python版本是一个解决冲突的好的方式。通过几个简单的命令来解决版本管理的问题是比较直观的，提出Rye应该是利大于弊的，也就是有益程度大于碎片化增加的程度。&lt;/p&gt;
&lt;p&gt;总之不管是&lt;a href=&quot;https://peps.python.org/pep-0668&quot;&gt;PEP 668&lt;/a&gt;中标记版本管理是系统的还是Python的，还是&lt;a href=&quot;https://peps.python.org/pep-0711/&quot;&gt;PEP 711&lt;/a&gt;来单独下发Python解释器二进制文件，还是Rye的出现，都是Python社区意识到Python包管理问题的严重性，进而做出的一些有益尝试。期待在未来，有更标准化的工具，Python的开发也更容易。&lt;/p&gt;
&lt;p&gt;下面将对Rye的安装和使用进行简单介绍。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="Rust" scheme="http://vra.github.io/tags/Rust/"/>
    
  </entry>
  
  <entry>
    <title>git 回滚代码并保留提交历史</title>
    <link href="http://vra.github.io/2023/05/16/git-roll-back-code-and-save-commit-history/"/>
    <id>http://vra.github.io/2023/05/16/git-roll-back-code-and-save-commit-history/</id>
    <published>2023-05-16T07:55:28.000Z</published>
    <updated>2023-05-16T07:56:23.905Z</updated>
    
    <content type="html"><![CDATA[<p>在使用git时，有时候需要回退最新代码到之前的某次提交或某个tag，将中间的所有代码提交去掉。同时保持中间的提交记录。实际应用时发现这个动作没有比较好的实现方式。</p><p>例如，如果使用<code>git revert commit-id</code>, 那么只会会退<code>commit-id</code> 对应的那次提交，之后的提交不受影响，仍然存在，不是我们想要的效果。</p><p>如果使用<code>git reset</code>, 那操作就比较麻烦，需要使用<code>--hard</code> 和<code>--force</code> 等比较危险的命令，具体如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git reset --hard commit-id</span><br><span class="line">git push --force</span><br></pre></td></tr></table></figure><p>这样做除了使用比较危险的命令选项外，还有个问题是没法保留中间的提交历史，这不是我们想要的。</p><p>搜索发现，利用git diff和git apply可以来比较清晰的完成这个需求，整体的思路是：</p><ol><li>得到当前最新提交到回退提交之间的代码diff，将diff保存为文件</li><li>利用<code>git apply</code> 将diff作用到代码上，回到之前的代码状态</li><li>提交代码</li></ol><p>具体来说，假设当前最新提交就在分支<code>current-branch</code>上，回退提交为<code>prev-commit</code>,这个回退提交可以是一次commit id，也可以是一个tag，也可以是一个分支名。执行命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">git checkout prev-commit</span><br><span class="line">git diff current-branch &gt; ~/diff.patch</span><br><span class="line">git checkout current-branch</span><br><span class="line">cat ~/diff.patch | git apply</span><br><span class="line">git commit -am <span class="string">&quot;roll back to prev-commit&quot;</span></span><br><span class="line">git push</span><br></pre></td></tr></table></figure><p>这样就能既回退代码，又保留提交历史。</p><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><ul><li><a href="https://stackoverflow.com/a/33890073">https://stackoverflow.com/a/33890073</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;在使用git时，有时候需要回退最新代码到之前的某次提交或某个tag，将中间的所有代码提交去掉。同时保持中间的提交记录。实际应用时发现这个动作没有比较好的实现方式。&lt;/p&gt;
&lt;p&gt;例如，如果使用&lt;code&gt;git revert commit-id&lt;/code&gt;, 那么只会会退</summary>
      
    
    
    
    
    <category term="Git" scheme="http://vra.github.io/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>talkGPT4All</title>
    <link href="http://vra.github.io/2023/04/02/chatbot4all/"/>
    <id>http://vra.github.io/2023/04/02/chatbot4all/</id>
    <published>2023-04-01T22:11:54.000Z</published>
    <updated>2023-04-02T00:45:18.594Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p>TL;DR: talkGPT4All 是一个在PC本地运行的基于talkGPT和GPT4All的语音聊天程序，通过OpenAI Whisper将输入语音转文本，再将输入文本传给GPT4All获取回答文本，最后利用发音程序将文本读出来，构建了完整的语音交互聊天过程。</p><p>实际使用效果<a href="https://www.zhihu.com/zvideo/1625779747656515584">视频</a>。</p><p>实际上，它只是几个工具的简易组合，没有什么创新的地方（甚至不支持多轮聊天，只支持英文），但 talkGPT4All 有下面几个比较好的特点</p><ul><li>所有算法本地运行，不涉及API的调用，避免了国内无法访问OpenAI API的问题</li><li>CPU 运行，无须 GPU 显卡</li><li>占内存小，实测8G内存就可以跑起来</li><li>速度还可以，测试8G Windows 一轮聊天小于1分钟， 16G Mac 一轮聊天小于30秒</li><li>集成的AI还算智能，至少答能对题，回答看起来是符合英语语法的</li></ul><p>目前支持平台和验证的情况如下:</p><ul><li>Mac M1，已经验证可用</li><li>Windows，已经验证可用</li><li>Mac intel，未验证</li><li>Linux，未验证<br>如果有对应机器的朋友感兴趣的话，可以帮忙验证一下，有问题可以提PR和issue。</li></ul><p>想体验的朋友可以参考 GitHub README进行快速安装，也可以在这篇文章中跟着我一步步来进行。</p><span id="more"></span><h2 id="2-为什么造这个轮子"><a href="#2-为什么造这个轮子" class="headerlink" title="2. 为什么造这个轮子"></a>2. 为什么造这个轮子</h2><p>聊天机器人是我比较喜欢的一个应用，机器+人类的思维是一个很有意思的场景。另一方面，通过一个智能机器人来练习英语口语，也是一个很实际的应用。</p><p>一直以来，想要做一个含有智能的聊天机器人应用都是难度很大的，尤其是智能化的程度，受学术研究进展的制约，没法做到很高。然而近期的AI LLM大爆发，让开发一个真正智能的AI聊天机器人越来越容易。</p><p>最早看到的是基于<a href="https://github.com/ggerganov/whisper.cpp">whisper.cpp</a>的<a href="https://github.com/ggerganov/whisper.cpp/tree/master/examples/talk.wasm">talk.wasm</a>, 一个基于Whisper+GPT-2的浏览器对话机器人，实际测试后发现GPT-2还不够智能，回答很多时候都答非所问。</p><p>然后是在ChatGPT出来后，我在想能不能做一个Whisper + ChatGPT的智能聊天机器人呢，搜索后发现whisper.cpp的讨论区已经有人在<a href="https://github.com/ggerganov/whisper.cpp/discussions/167#discussioncomment-4334628">讨论</a>这个事情，不过没看到成品。</p><p>在ChatGPT 开放API后，有人做了一个MacOS上的基于OpenAI API的语音聊天机器人<a href="https://github.com/chenyukang/talkGPT">talkGPT</a>，简单好用，唯一的问题是需要借助OpenAI API，目前国内是不太好访问的。</p><p>再然后是<a href="https://github.com/ggerganov/llama.cpp">llama.cpp</a>，通过量化和大量工程优化，让原本参数量很大的<a href="https://github.com/facebookresearch/llama">LLaMA</a>模型可以跑在普通的笔记本上（现在甚至支持在Android上运行！)，但实际测试经过量化后 LLaMA 7B 模型智能程度不太高，有时候会出错，而 更大的LLaMA 15B 和 30B 在8GB内存的Windows 机器上跑起来就比较难了（最新进展：大小20GB的30B模型可以在8G的系统上运行了，参见这个<a href="https://github.com/ggerganov/llama.cpp/pull/613">优化</a>和这里的<a href="https://github.com/ggerganov/llama.cpp/discussions/638">讨论</a>)。</p><p>这周又出现了<a href="https://github.com/nomic-ai/gpt4all">gpt4all</a>，基于 LLaMA 7B模型，采集近80万的GPT-3.5-Turbo对话数据进行finetune，效果看起来比 LLaMA 7B 要好。作者发布了他们训练好的经过量化的模型，大小3.9G，以及可以直接在PC上运行的二进制聊天程序，可以直接在各个平台运行。</p><p>然后长久以来的TODO 可以实现了，在缝合了talkGPT和GPT4All后，就有了talkGPT4All。简单来说，是把talkGPT的OpenAI API 换成了 GPT4All提供的本地可以运行的量化模型，也可以说是在GPT4All的基础上添加了语音转文本和文本转语音的功能。</p><p>那下面我们来看看怎么安装和运行这个缝合怪吧。</p><h2 id="3-构建环境"><a href="#3-构建环境" class="headerlink" title="3. 构建环境"></a>3. 构建环境</h2><p>由于整个程序设计到 Python 代码环境的搭建、Whisper 语音转文本模型的下载、GPT4All 语言模型的下载、GPT4All 聊天程序的下载、文本转语音程序的下载，整体链路略长，下面分步骤分平台分别进行详细说明。</p><h3 id="3-1-Python环境的搭建"><a href="#3-1-Python环境的搭建" class="headerlink" title="3.1 Python环境的搭建"></a>3.1 Python环境的搭建</h3><p>在不同平台 Python 代码环境的搭建是一致的。</p><p>推荐使用&gt;= 3.8的Python版本，因为新版本的Python有一定的速度提升。低版本可能一些功能不支持。<br>首先clone代码:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/vra/talkGPT4All.git</span><br></pre></td></tr></table></figure><p>后面假设代码仓库的根目录为<code>&lt;ROOT&gt;</code>来进行命令说明。</p><p>基于 Python自带的 venv 来搭建隔离的环境，并进行依赖安装:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> &lt;ROOT&gt;</span><br><span class="line">python -m venv talkgpt4all</span><br><span class="line"><span class="built_in">source</span> talkgpt4all/bin/activate</span><br><span class="line">pip install -U pip</span><br><span class="line">pip install -r requirements.txt</span><br></pre></td></tr></table></figure><h3 id="3-2-Whisper-语音转文本模型下载"><a href="#3-2-Whisper-语音转文本模型下载" class="headerlink" title="3.2 Whisper 语音转文本模型下载"></a>3.2 Whisper 语音转文本模型下载</h3><p>Whisper 模型在调用时会自动下载，但有时候在命令行下载速度比较慢，我们可以在浏览器中提前下载后放置到对应目录，解决这个问题。<br>Whisper 的所有模型地址参见<a href="https://github.com/openai/whisper/blob/b80bcf610d89960bc658b61af9c333fc6d978d78/whisper/__init__.py#L18-L29">这里</a>，我们用的是<code>base.pt</code>，地址是<a href="https://openaipublic.azureedge.net/main/whisper/models/ed3a0b6b1c0edf879ad9b11b1af5a0e6ab5db9205f891f668f8b0e6c6326e34e/base.pt">https://openaipublic.azureedge.net/main/whisper/models/ed3a0b6b1c0edf879ad9b11b1af5a0e6ab5db9205f891f668f8b0e6c6326e34e/base.pt</a>，放置的目录是<code>$HOME/.cache/whisper</code>（Windows上是<code>C:\Users\username\.cache\whisper</code>),通过浏览器或 wget 下载<code>base.pt</code>到这个目录就行。</p><h3 id="3-3-GPT4All-语言模型的下载"><a href="#3-3-GPT4All-语言模型的下载" class="headerlink" title="3.3 GPT4All 语言模型的下载"></a>3.3 GPT4All 语言模型的下载</h3><p>语言模型放置目录是<code>&lt;ROOT&gt;/models</code>，根据 GPT4All <a href="https://github.com/nomic-ai/gpt4all#try-it-yourself">文档</a>，下载方式包括</p><ul><li><a href="https://the-eye.eu/public/AI/models/nomic-ai/gpt4all/gpt4all-lora-quantized.bin">链接</a>直接下载</li><li><a href="https://tinyurl.com/gpt4all-lora-quantized">torrent</a>下载</li></ul><p>选择其中一种方式，将下载后的模型放置到<code>&lt;ROOT&gt;/models</code>目录下。</p><h3 id="3-3-GPT4All-聊天程序下载"><a href="#3-3-GPT4All-聊天程序下载" class="headerlink" title="3.3 GPT4All 聊天程序下载"></a>3.3 GPT4All 聊天程序下载</h3><p>GPT4All 的作者打包了多平台的二进制聊天程序，可以下载后直接使用，不用从源码编译 C++ 文件。</p><p>聊天程序的放置目录是<code>&lt;ROOT&gt;/bin</code>，不同平台的下载地址如下：</p><ul><li>Mac M1: <a href="https://raw.githubusercontent.com/nomic-ai/gpt4all/main/chat/gpt4all-lora-quantized-OSX-m1">https://raw.githubusercontent.com/nomic-ai/gpt4all/main/chat/gpt4all-lora-quantized-OSX-m1</a></li><li>Mac Intel : <a href="https://raw.githubusercontent.com/nomic-ai/gpt4all/main/chat/gpt4all-lora-quantized-OSX-Intel">https://raw.githubusercontent.com/nomic-ai/gpt4all/main/chat/gpt4all-lora-quantized-OSX-Intel</a></li><li>Linux : <a href="https://raw.githubusercontent.com/nomic-ai/gpt4all/main/chat/gpt4all-lora-quantized-linux-x86">https://raw.githubusercontent.com/nomic-ai/gpt4all/main/chat/gpt4all-lora-quantized-linux-x86</a></li><li>Windows : <a href="https://raw.githubusercontent.com/nomic-ai/gpt4all/main/chat/gpt4all-lora-quantized-win64.exe">https://raw.githubusercontent.com/nomic-ai/gpt4all/main/chat/gpt4all-lora-quantized-win64.exe</a></li></ul><p>下载你的平台的文件，放置到<code>&lt;ROOT&gt;/bin</code>。</p><h3 id="3-4-文本转语音程序下载"><a href="#3-4-文本转语音程序下载" class="headerlink" title="3.4 文本转语音程序下载"></a>3.4 文本转语音程序下载</h3><p>在 Mac 下，自带<a href="https://ss64.com/osx/say.html">say命令</a>，可以将文本转语音，因此不需要额外安装工具。</p><p>在 Linux 下，有<a href="https://espeak.sourceforge.net/">espeak</a>命令可以来完成文本转语音，但需要额外安装，Ubuntu下的安装命令为<code>sudo apt install espeak</code>，别的发行版也可以用包管理安装。如果不行的话，尝试<a href="https://espeak.sourceforge.net/download.html">下载源码</a>自行编译安装。</p><p>Windows 下有一个 say 命令的替代 <a href="https://github.com/p-groarke/wsay">wsay</a>, 可以在<a href="https://github.com/p-groarke/wsay/releases/tag/v1.5.0">这里</a>下载 wsay.exe，放置到<code>&lt;ROOT&gt;/bin</code>目录下。</p><h3 id="4-使用"><a href="#4-使用" class="headerlink" title="4. 使用"></a>4. 使用</h3><p>安装完成后，进入<code>&lt;ROOT&gt;</code>目录，启用虚拟环境，使用<code>python chat.py --platform &lt;platform&gt;</code>运行程序，<code>&lt;platform&gt;</code>分别是<code>mac-m1</code>, <code>mac-intel</code>, <code>linux</code>, <code>windows</code>。</p><p>Mac M1:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python chat.py --platform mac-m1</span><br></pre></td></tr></table></figure><p>Mac Intel:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python chat.py --platform mac-intel</span><br></pre></td></tr></table></figure><p>Linux:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python chat.py --platform linux</span><br></pre></td></tr></table></figure><p>Windows:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python chat.py --platform windows</span><br></pre></td></tr></table></figure><p>⚠️注意：目前只测试过 Mac M1 和 Windows，别的平台未测试，如有问题，欢迎提 <a href="https://github.com/vra/talkGPT4All/issues">issue</a> 和 <a href="https://github.com/vra/talkGPT4All/pulls">PR</a> 。</p><p>在 Mac 上使用效果如下:<br><img data-src="/imgs/talkgpt4all-mac-m1.jpg"></p><p>也可以参见本文开头的视频或<a href="https://www.zhihu.com/zvideo/1625779747656515584">这里</a>。</p><h3 id="5-后续改进思路"><a href="#5-后续改进思路" class="headerlink" title="5. 后续改进思路"></a>5. 后续改进思路</h3><p>目前实现还是比较粗糙，计划后续会增加下面的功能（按实现难度从低到高排列）：</p><ul><li>验证 Linux，Mac Intel 和 WSL2 下能否正常运行</li><li>增加多轮对话支持</li><li>增加中文支持</li><li>去掉编译好的二进制程序，包含 llama.cpp 源码，自行编译，支持更灵活的使用</li><li>更多效果更好模型的添加</li></ul><p>欢迎基于这个仓库进行修改和代码分发，期待创造出更有新意、更有应用价值的东西～</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h2&gt;&lt;p&gt;TL;DR: talkGPT4All 是一个在PC本地运行的基于talkGPT和GPT4All的语音聊天程序，通过OpenAI Whisper将输入语音转文本，再将输入文本传给GPT4All获取回答文本，最后利用发音程序将文本读出来，构建了完整的语音交互聊天过程。&lt;/p&gt;
&lt;p&gt;实际使用效果&lt;a href=&quot;https://www.zhihu.com/zvideo/1625779747656515584&quot;&gt;视频&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;实际上，它只是几个工具的简易组合，没有什么创新的地方（甚至不支持多轮聊天，只支持英文），但 talkGPT4All 有下面几个比较好的特点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;所有算法本地运行，不涉及API的调用，避免了国内无法访问OpenAI API的问题&lt;/li&gt;
&lt;li&gt;CPU 运行，无须 GPU 显卡&lt;/li&gt;
&lt;li&gt;占内存小，实测8G内存就可以跑起来&lt;/li&gt;
&lt;li&gt;速度还可以，测试8G Windows 一轮聊天小于1分钟， 16G Mac 一轮聊天小于30秒&lt;/li&gt;
&lt;li&gt;集成的AI还算智能，至少答能对题，回答看起来是符合英语语法的&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;目前支持平台和验证的情况如下:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Mac M1，已经验证可用&lt;/li&gt;
&lt;li&gt;Windows，已经验证可用&lt;/li&gt;
&lt;li&gt;Mac intel，未验证&lt;/li&gt;
&lt;li&gt;Linux，未验证&lt;br&gt;如果有对应机器的朋友感兴趣的话，可以帮忙验证一下，有问题可以提PR和issue。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;想体验的朋友可以参考 GitHub README进行快速安装，也可以在这篇文章中跟着我一步步来进行。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Linux" scheme="http://vra.github.io/tags/Linux/"/>
    
    <category term="AI" scheme="http://vra.github.io/tags/AI/"/>
    
    <category term="ChatBot" scheme="http://vra.github.io/tags/ChatBot/"/>
    
    <category term="GPT" scheme="http://vra.github.io/tags/GPT/"/>
    
    <category term="Whisper" scheme="http://vra.github.io/tags/Whisper/"/>
    
    <category term="Mac" scheme="http://vra.github.io/tags/Mac/"/>
    
    <category term="Windows" scheme="http://vra.github.io/tags/Windows/"/>
    
  </entry>
  
  <entry>
    <title>neovim telescope 插件简要教程</title>
    <link href="http://vra.github.io/2023/03/28/neovim-telescope/"/>
    <id>http://vra.github.io/2023/03/28/neovim-telescope/</id>
    <published>2023-03-28T15:44:37.000Z</published>
    <updated>2023-03-28T16:23:19.254Z</updated>
    
    <content type="html"><![CDATA[<h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p><a href="https://github.com/nvim-telescope/telescope.nvim/">telescope</a> 是一款强大的 neovim 插件，可以在 neovim 中提供文件名搜索和文本内容搜索的功能，以及更多复杂的功能，具体的show case可以看<a href="https://github.com/nvim-telescope/telescope.nvim/wiki/Showcase">这里</a>。我安装 telescope 主要是想利用它在大型项目中的文件名搜索和文本内容搜索能力，这里记录一下安装流程和使用概要。</p><span id="more"></span><h2 id="2-安装"><a href="#2-安装" class="headerlink" title="2. 安装"></a>2. 安装</h2><p>首先需要安装 neovim。具体步骤可以看<a href="https://github.com/neovim/neovim/wiki/Installing-Neovim">这里</a>。</p><p>注意 telescope 需要nvim 0.7.0及以后的版本，因此如果你neovim 版本本身比较低的话，需要升级。</p><p>安装 neovim 后还需要进行配置。我的 neovim 配置是复制的这个<a href="https://github.com/bigeagle/neovim-config">仓库</a>，按照README来进行操作，可以快速地安装好，这里不赘述。</p><p>telescope 支持多种插件系统，我使用的 vim-plug，在<code>~/.config/nvim/init.vim</code> 添加下面两行：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Plug <span class="string">&#x27;nvim-lua/plenary.nvim&#x27;</span></span><br><span class="line">Plug <span class="string">&#x27;nvim-telescope/telescope.nvim&#x27;</span>, &#123; <span class="string">&#x27;tag&#x27;</span>: <span class="string">&#x27;0.1.1&#x27;</span> &#125;</span><br></pre></td></tr></table></figure><p>然后在nvim中输入<code>:PlugInstall</code> 来安装插件。</p><p>由于插件是在GitHub上下载的，有时候可能安装会卡住，需要多尝试几次，即多次执行<code>:PlugInstall</code>命令。</p><p>安装完成后，执行<code>:Telescope find_files</code>来验证安装是否正确。如果能弹出输入框，说明安装成功了。</p><p>这个命令用来模糊匹配当前目录下的所有文件名，对于快速切换编辑文件非常方便。</p><h2 id="3-live-grep-功能"><a href="#3-live-grep-功能" class="headerlink" title="3. live_grep 功能"></a>3. <code>live_grep</code> 功能</h2><p>除了<code>find_files</code>命令，<code>live_grep</code>也是一个很有用的命令，可以快速搜索某些代码，把含搜索代码的文件打开。</p><p>这个功能需要依赖<a href="https://github.com/BurntSushi/ripgrep">ripgrep</a>，因此要先安装它，具体安装命令如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># mac</span></span><br><span class="line">brew install ripgrep</span><br><span class="line"></span><br><span class="line"><span class="comment"># debian/ubuntu</span></span><br><span class="line">sudo apt-get install ripgrep</span><br><span class="line"></span><br><span class="line"><span class="comment"># arch</span></span><br><span class="line">pacman -S ripgrep</span><br><span class="line"></span><br><span class="line"><span class="comment"># centos</span></span><br><span class="line">sudo yum-config-manager --add-repo=https://copr.fedorainfracloud.org/coprs/carlwgeorge/ripgrep/repo/epel-7/carlwgeorge-ripgrep-epel-7.repo</span><br><span class="line">sudo yum install ripgrep</span><br><span class="line"></span><br><span class="line"><span class="comment"># windows </span></span><br><span class="line">scoop install ripgrep</span><br></pre></td></tr></table></figure><p>安装完后在命令行输入<code>ag -h</code> 验证安装是否成功。</p><p>ag 安装完成后，在nvim输入<code>:Telescope live_grep</code> 就可以搜索你想要的代码了。</p><h2 id="4-快捷键"><a href="#4-快捷键" class="headerlink" title="4. 快捷键"></a>4. 快捷键</h2><p>上面的两个常用功能输入都比较繁琐，有没有什么快捷键可以快速打开呢？是有的，官方GitHub给出了几行代码，加入到<code>~/.config/nvim/init.vim</code>的最后：</p><figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&quot; Find files using Telescope command-line sugar.</span></span><br><span class="line"><span class="keyword">nnoremap</span> <span class="symbol">&lt;leader&gt;</span>ff <span class="symbol">&lt;cmd&gt;</span>Telescope find_files<span class="symbol">&lt;cr&gt;</span></span><br><span class="line"><span class="keyword">nnoremap</span> <span class="symbol">&lt;leader&gt;</span>fg <span class="symbol">&lt;cmd&gt;</span>Telescope live_grep<span class="symbol">&lt;cr&gt;</span></span><br><span class="line"><span class="keyword">nnoremap</span> <span class="symbol">&lt;leader&gt;</span>fb <span class="symbol">&lt;cmd&gt;</span>Telescope <span class="keyword">buffers</span><span class="symbol">&lt;cr&gt;</span></span><br><span class="line"><span class="keyword">nnoremap</span> <span class="symbol">&lt;leader&gt;</span>fh <span class="symbol">&lt;cmd&gt;</span>Telescope help_tags<span class="symbol">&lt;cr&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&quot; Using Lua functions</span></span><br><span class="line"><span class="keyword">nnoremap</span> <span class="symbol">&lt;leader&gt;</span>ff <span class="symbol">&lt;cmd&gt;</span><span class="keyword">lua</span> require(<span class="string">&#x27;telescope.builtin&#x27;</span>).find_files()<span class="symbol">&lt;cr&gt;</span></span><br><span class="line"><span class="keyword">nnoremap</span> <span class="symbol">&lt;leader&gt;</span>fg <span class="symbol">&lt;cmd&gt;</span><span class="keyword">lua</span> require(<span class="string">&#x27;telescope.builtin&#x27;</span>).live_grep()<span class="symbol">&lt;cr&gt;</span></span><br><span class="line"><span class="keyword">nnoremap</span> <span class="symbol">&lt;leader&gt;</span>fb <span class="symbol">&lt;cmd&gt;</span><span class="keyword">lua</span> require(<span class="string">&#x27;telescope.builtin&#x27;</span>).<span class="keyword">buffers</span>()<span class="symbol">&lt;cr&gt;</span></span><br><span class="line"><span class="keyword">nnoremap</span> <span class="symbol">&lt;leader&gt;</span>fh <span class="symbol">&lt;cmd&gt;</span><span class="keyword">lua</span> require(<span class="string">&#x27;telescope.builtin&#x27;</span>).help_tags()<span class="symbol">&lt;cr&gt;</span></span><br></pre></td></tr></table></figure><p>然后在Normal模式输入<code>\ff</code>就可以打开<code>find_files</code>命令窗口，输入<code>\fg</code>就可以打开<code>live_grep</code>窗口了。</p><p>更多详细命令和功能参见GitHub 页面。</p>]]></content>
    
    
    <summary type="html">&lt;h2 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://github.com/nvim-telescope/telescope.nvim/&quot;&gt;telescope&lt;/a&gt; 是一款强大的 neovim 插件，可以在 neovim 中提供文件名搜索和文本内容搜索的功能，以及更多复杂的功能，具体的show case可以看&lt;a href=&quot;https://github.com/nvim-telescope/telescope.nvim/wiki/Showcase&quot;&gt;这里&lt;/a&gt;。我安装 telescope 主要是想利用它在大型项目中的文件名搜索和文本内容搜索能力，这里记录一下安装流程和使用概要。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Linux" scheme="http://vra.github.io/tags/Linux/"/>
    
    <category term="Vim" scheme="http://vra.github.io/tags/Vim/"/>
    
    <category term="NeoVim" scheme="http://vra.github.io/tags/NeoVim/"/>
    
  </entry>
  
  <entry>
    <title>libtorch系列教程2：torch::Tensor的使用</title>
    <link href="http://vra.github.io/2023/02/25/libtorch-tutorial2/"/>
    <id>http://vra.github.io/2023/02/25/libtorch-tutorial2/</id>
    <published>2023-02-25T09:15:46.000Z</published>
    <updated>2023-02-27T04:48:49.191Z</updated>
    
    <content type="html"><![CDATA[<p>系列教程列表：</p><ul><li><a href="https://vra.github.io/2023/02/25/libtorch-tutorial1/">Libtorch系列教程1：一个丝滑的C++ Tensor库</a> </li><li><a href="https://vra.github.io/2023/02/25/libtorch-tutorial2/">Libtorch系列教程2：torch::Tensor的使用</a> </li></ul><p>这篇文章中，我们暂时忽略网络训练和推理，详细展开Libtorch中Tensor对象的使用，看看将Libtorch当作一个纯粹的Tensor库来使用时，有哪些注意事项。如有未涉及的内容，请访问Libtorch<a href="https://pytorch.org/cppdocs/">官方文档</a>，通过搜索框获取更多的信息。Libtorch的环境搭建参考<a href="https://vra.github.io/2023/02/25/libtorch-tutorial1/">上一篇文章</a>。</p><span id="more"></span><h2 id="1-torch-Tensor基本操作"><a href="#1-torch-Tensor基本操作" class="headerlink" title="1. torch::Tensor基本操作"></a>1. torch::Tensor基本操作</h2><p>Libtorch中的Tensor是与Pytorch中的Tensor对应的，使用方式上很类似，只在一些Python语法C++不支持的时候有些不同，例如slice操作。<br>使用Libtorch前需要包含 Libtorch 的头文件<code>torch/torch.h</code>:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br></pre></td></tr></table></figure><p>这篇文章用到的所有函数都在此头文件中声明，而且所有的函数namespace都是<code>torch</code>，因此都可以以<code>torch::xxx</code>的形式来调用。</p><h3 id="1-1-Tensor创建"><a href="#1-1-Tensor创建" class="headerlink" title="1.1 Tensor创建"></a>1.1 Tensor创建</h3><p>Tensor 创建的方式比较多，包括从字面量创建，从C++ 原生的数组创建，从vector创建，从Libtorch自带的函数创建等。</p><p>从字面量创建:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch::Tensor foo = torch::<span class="built_in">tensor</span>(&#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>&#125;);</span><br></pre></td></tr></table></figure><p>从C++ 原生的float数组创建，使用<code>from_blob</code>函数:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">float</span> arr[] = &#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>&#125;;</span><br><span class="line"><span class="comment">// 第二个参数表示创建的Tensor shape，会自动对原生数组进行reshape</span></span><br><span class="line">torch::Tensor bar = torch::<span class="built_in">from_blob</span>(arr, &#123;<span class="number">1</span>, <span class="number">4</span>&#125;); <span class="comment">// shape是[1, 4]</span></span><br><span class="line">bar = torch::<span class="built_in">from_blob</span>(arr, &#123;<span class="number">2</span>, <span class="number">2</span>&#125;); <span class="comment">// shape是[2, 2]</span></span><br></pre></td></tr></table></figure><p>其中第二个参数表示创建的Tensor shape，会自动对原生数组进行reshape。</p><p>从vector 创建，使用<code>from_blob</code>函数:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">std::vector&lt;<span class="keyword">float</span>&gt; v = &#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>&#125;;</span><br><span class="line">bar = torch::<span class="built_in">from_blob</span>(v.<span class="built_in">data</span>(), &#123;<span class="number">2</span>, <span class="number">2</span>&#125;);</span><br></pre></td></tr></table></figure><p>还可以用Libtorch的函数创建，跟Numpy和Pytorch类似:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">foo = torch::<span class="built_in">arange</span>(<span class="number">4</span>);</span><br><span class="line">foo = torch::<span class="built_in">eye</span>(<span class="number">2</span>);</span><br><span class="line">foo = torch::<span class="built_in">ones</span>(<span class="number">2</span>);</span><br><span class="line">bar = torch::<span class="built_in">ones_like</span>(foo);</span><br><span class="line">foo = torch::<span class="built_in">rand</span>(<span class="number">4</span>);</span><br><span class="line">foo = torch::<span class="built_in">randn</span>(<span class="number">4</span>);</span><br><span class="line">foo = torch::<span class="built_in">zeros</span>(<span class="number">2</span>);</span><br><span class="line">bar = torch::<span class="built_in">zeros_like</span>(foo);</span><br></pre></td></tr></table></figure><p>创建好以后，Tensor对应可以直接用<code>std::cout</code>来输出:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch::Tensor foo = torch::<span class="built_in">tensor</span>(&#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>&#125;);</span><br><span class="line">std::cout &lt;&lt;<span class="string">&quot;==&gt; foo is:\n&quot;</span> &lt;&lt; foo &lt;&lt; std::endl;</span><br></pre></td></tr></table></figure><p>输出如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">==&gt; foo is:</span><br><span class="line"> 1</span><br><span class="line"> 2</span><br><span class="line"> 3</span><br><span class="line"> 4</span><br><span class="line">[ CPUFloatType&#123;4&#125; ]</span><br></pre></td></tr></table></figure><p>可以看到最后打印了Tensor的类型。</p><h3 id="1-2-Tensor对象的属性函数"><a href="#1-2-Tensor对象的属性函数" class="headerlink" title="1.2 Tensor对象的属性函数"></a>1.2 Tensor对象的属性函数</h3><p>创建Tensor后，我们还需要看到它的一些属性，判断是否跟预期相符。注意Libtorch的Tensor是没有公开可访问的属性attribute的，Tensor信息需要属性函数来获取。常见的属性函数包括:</p><ul><li>dim(): Tensor的维度</li><li>sizes(): 跟Pytorch中的shape属性一样</li><li>size(n): 第N个维度的shape</li><li>numel(): 总的元素数目，sizes中的每个元素相乘</li><li>dtype(): 数据类型</li><li>device(): Tensor所在的设备类型，CPU, CUDA, MPS等。</li></ul><p>使用方式如下:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Tensor 属性函数</span></span><br><span class="line">torch::Tensor foo = torch::<span class="built_in">randn</span>(&#123;<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>&#125;);</span><br><span class="line"><span class="keyword">auto</span> dim = foo.<span class="built_in">dim</span>(); <span class="comment">// 4</span></span><br><span class="line"><span class="keyword">auto</span> sizes = foo.<span class="built_in">sizes</span>(); <span class="comment">// [1, 3, 224, 224]</span></span><br><span class="line"><span class="keyword">auto</span> size_0 = foo.<span class="built_in">size</span>(<span class="number">0</span>); <span class="comment">// 1</span></span><br><span class="line"><span class="keyword">auto</span> numel = foo.<span class="built_in">numel</span>(); <span class="comment">// 150528</span></span><br><span class="line"><span class="keyword">auto</span> dtype = foo.<span class="built_in">dtype</span>(); <span class="comment">// float</span></span><br><span class="line"><span class="keyword">auto</span> scalar_type = foo.<span class="built_in">scalar_type</span>(); <span class="comment">// Float</span></span><br><span class="line"><span class="keyword">auto</span> device = foo.<span class="built_in">device</span>(); <span class="comment">// cpu</span></span><br></pre></td></tr></table></figure><h3 id="1-3-Tensor对象的索引"><a href="#1-3-Tensor对象的索引" class="headerlink" title="1.3 Tensor对象的索引"></a>1.3 Tensor对象的索引</h3><p>Tensor 默认是支持<code>[]</code>操作符的，因此可以使用这样的方式来获取元素：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> foo = torch::<span class="built_in">randn</span>(&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;);</span><br><span class="line"><span class="keyword">float</span> value = foo[<span class="number">0</span>][<span class="number">1</span>][<span class="number">2</span>][<span class="number">2</span>];</span><br></pre></td></tr></table></figure><p>另一种方式是用Tensor对象的<code>index</code>函数，它的优势是支持slice。<br>对于单个元素，可以类似Pytorch中，直接用<code>index(&#123;i, j, k&#125;)</code>的方式来索引：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> foo = torch::<span class="built_in">randn</span>(&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;);</span><br><span class="line"><span class="keyword">float</span> value = foo.<span class="built_in">index</span>(&#123;<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>&#125;);</span><br></pre></td></tr></table></figure><p>那么python中很常用的slice呢？例如<code>foo[..., :2, 1:, :-1]</code>，该怎么在Libtorch中表示？<br>这里需要用到<code>torch::indexing::Slice</code> 对象，来实现Python中的Slice，看看下面的例子你就明白了：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> torch::indexing;</span><br><span class="line"></span><br><span class="line"><span class="keyword">auto</span> foo = torch::<span class="built_in">randn</span>(&#123;<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>&#125;);</span><br><span class="line"><span class="comment">// 等效于Python中的foo[:, 0:1, 2:, :-1]</span></span><br><span class="line"><span class="keyword">auto</span> bar = foo.<span class="built_in">index</span>(&#123;<span class="built_in">Slice</span>(), <span class="built_in">Slice</span>(<span class="number">0</span>, <span class="number">1</span>), <span class="built_in">Slice</span>(<span class="number">2</span>, None), <span class="built_in">Slice</span>(None, <span class="number">-1</span>)&#125;);</span><br></pre></td></tr></table></figure><p>应该是能满足Python中slice同样的使用场景。</p><h3 id="1-4-更新Tensor中元素的值"><a href="#1-4-更新Tensor中元素的值" class="headerlink" title="1.4 更新Tensor中元素的值"></a>1.4 更新Tensor中元素的值</h3><p>有了索引之后，我们就可以更新Tensor的值了：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch::Tensor foo = torch::<span class="built_in">tensor</span>(&#123;<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>&#125;);</span><br><span class="line">foo[<span class="number">0</span>] = <span class="number">10.0</span>;</span><br><span class="line">foo.<span class="built_in">index</span>(&#123;<span class="number">0</span>&#125;) = <span class="number">2.0</span>;</span><br></pre></td></tr></table></figure><p>但还没找到用给部分Tensor元素赋值的方法，类似Python中的<code>foo[:2] = bar</code>，欢迎补充。</p><h3 id="1-5-获取Tensor中的数据"><a href="#1-5-获取Tensor中的数据" class="headerlink" title="1.5 获取Tensor中的数据"></a>1.5 获取Tensor中的数据</h3><p>Tensor是一个Libtorch的对象，那怎么把它中的数据拿出来保存到文件中或传给别的函数呢？<br>使用<code>data_ptr</code>函数就可以:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch::Tensor foo = torch::<span class="built_in">randn</span>(&#123;<span class="number">3</span>, <span class="number">3</span>&#125;);</span><br><span class="line"><span class="keyword">float</span>* data = foo.data_ptr&lt;<span class="keyword">float</span>&gt;();</span><br></pre></td></tr></table></figure><p>对于单个元素的Tensor，还可以用<code>item</code>函数得到具体的数值:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch::Tensor one_element_tensor = foo.<span class="built_in">index</span>(&#123;<span class="built_in">Slice</span>(), <span class="built_in">Slice</span>(<span class="number">0</span>, <span class="number">1</span>), <span class="built_in">Slice</span>(<span class="number">0</span>, <span class="number">1</span>), <span class="built_in">Slice</span>(<span class="number">0</span>, <span class="number">1</span>)&#125;);</span><br><span class="line"><span class="keyword">float</span> value = one_element_tensor.item&lt;<span class="keyword">float</span>&gt;();</span><br></pre></td></tr></table></figure><h3 id="1-6-数据类型"><a href="#1-6-数据类型" class="headerlink" title="1.6 数据类型"></a>1.6 数据类型</h3><p>Libtorch中支持float16, float32, float64, int8, int16, int32, uint8这几类的Tensor数据类型，可以用<code>to</code>函数来进行类型转换：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 数据类型, 参见 https://pytorch.org/cppdocs/api/file_torch_csrc_api_include_torch_types.h.html#variables</span></span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kF16);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kF32);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kF64);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kFloat16);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kFloat32);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kFloat64);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kI8);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kI16);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kI32);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kI64);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kInt8);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kInt16);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kInt32);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kInt64);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kU8);</span><br><span class="line">bar = foo.<span class="built_in">to</span>(torch::kUInt8);</span><br></pre></td></tr></table></figure><p>全部数据类型，参见官方文档的<a href="https://pytorch.org/cppdocs/api/file_torch_csrc_api_include_torch_types.h.html#variables">数据类型页面</a>。</p><h3 id="1-7-设备类型"><a href="#1-7-设备类型" class="headerlink" title="1.7 设备类型"></a>1.7 设备类型</h3><p>设备类型是Tensor保存的设备的种类。由于Libtorch不仅仅支持CPU，还支持各种类型的GPU，因此有很多设备类型。</p><p>所有的设备类型参见<a href="https://pytorch.org/cppdocs/api/file_c10_core_DeviceType.h.html#variables">这里</a>。<br>需要注意的是，设备是跟编译时的配置，机器是否支持强相关的，而且某些设备支持并不好，例如我想用下面的代码将CPU上的Tensor转移到MPS上：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">auto</span> foo = torch::<span class="built_in">randn</span>(&#123;<span class="number">3</span>, <span class="number">3</span>&#125;);</span><br><span class="line"><span class="keyword">auto</span> bar = foo.<span class="built_in">to</span>(torch::kMPS);</span><br></pre></td></tr></table></figure><p>编译是没有问题的，但运行时会报下面的错:</p><blockquote><p>libc++abi: terminating with uncaught exception of type c10::TypeError: Cannot convert a MPS Tensor to float64 dtype as the MPS framework doesn’t support float64. Please use float32 instead.</p></blockquote><p>提示说MPS不支持float64，但我打印<code>foo</code>的类型，它其实是float32，本身报错比较奇怪，搜了一圈也没找到怎么解决。</p><h3 id="1-8-Tensor-变形函数"><a href="#1-8-Tensor-变形函数" class="headerlink" title="1.8 Tensor 变形函数"></a>1.8 Tensor 变形函数</h3><p>很多时候我们需要将Tensor进行形状的修改，这方面Libtorch支持的比较好，这些操作都支持:</p><ul><li>reshape</li><li>flatten</li><li>squeeze</li><li>unsqueeze</li><li>transpose</li><li>cat/concat/concatenate</li></ul><p>而且支持<code>torch::reshape</code>这种静态函数和<code>tensor.reshape</code>这种对象函数。下面是一些例子:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 变形操作</span></span><br><span class="line">bar = foo.<span class="built_in">reshape</span>(&#123;<span class="number">2</span>, <span class="number">-1</span>&#125;);</span><br><span class="line">bar = foo.<span class="built_in">flatten</span>();</span><br><span class="line">bar = foo.<span class="built_in">squeeze</span>();</span><br><span class="line">bar = foo.<span class="built_in">unsqueeze</span>(<span class="number">0</span>);</span><br><span class="line">bar = torch::<span class="built_in">unsqueeze</span>(foo, <span class="number">-1</span>);</span><br><span class="line">bar = foo.<span class="built_in">transpose</span>(<span class="number">0</span>, <span class="number">1</span>).<span class="built_in">transpose</span>(<span class="number">2</span>, <span class="number">3</span>).<span class="built_in">transpose</span>(<span class="number">3</span>, <span class="number">1</span>);</span><br><span class="line">bar = torch::<span class="built_in">transpose</span>(foo, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">bar = torch::<span class="built_in">cat</span>(&#123;foo, foo&#125;, <span class="number">2</span>);</span><br></pre></td></tr></table></figure><p>一个比较特殊的地方是transpose只支持两个轴的交换，多个轴的交换需要调用多次来实现。</p><h3 id="1-9-Tensor之间的操作函数"><a href="#1-9-Tensor之间的操作函数" class="headerlink" title="1.9 Tensor之间的操作函数"></a>1.9 Tensor之间的操作函数</h3><p>Tensor库中，Tensor和Tensor之间的操作是很常见的，比如求矩阵相乘，内积外积等，有内置的函数支持能避免很多额外的开发工作。这里是一些例子:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">foo = torch::<span class="built_in">randn</span>(&#123;<span class="number">3</span>, <span class="number">3</span>&#125;);</span><br><span class="line">bar = torch::<span class="built_in">matmul</span>(foo, foo);</span><br><span class="line">bar = foo.<span class="built_in">matmul</span>(foo);</span><br><span class="line">bar = torch::<span class="built_in">cross</span>(foo, foo);</span><br><span class="line">bar = torch::<span class="built_in">mul</span>(foo, foo);</span><br></pre></td></tr></table></figure><h3 id="1-10-线性代数相关函数"><a href="#1-10-线性代数相关函数" class="headerlink" title="1.10 线性代数相关函数"></a>1.10 线性代数相关函数</h3><p><code>torch::linalg</code> namespace中包含常见的线性代数操作，几个简单的使用例子:</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bar = torch::linalg::<span class="built_in">inv</span>(foo);</span><br><span class="line">bar = torch::linalg::<span class="built_in">norm</span>(foo, <span class="number">2</span>, &#123;<span class="number">0</span>, <span class="number">1</span>&#125;, <span class="literal">false</span>, torch::nullopt);</span><br></pre></td></tr></table></figure><p>所有支持的函数详见<a href="https://pytorch.org/cppdocs/api/file_torch_csrc_api_include_torch_linalg.h.html#file-torch-csrc-api-include-torch-linalg-h">官方文档</a></p><h3 id="1-11-神经网络相关函数"><a href="#1-11-神经网络相关函数" class="headerlink" title="1.11 神经网络相关函数"></a>1.11 神经网络相关函数</h3><p>神经网络是torch的核心模块，常见的一些激活函数，卷积层都可以以函数的形式作用在Tensor上，这里写几个简单的例子：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">bar = torch::<span class="built_in">softmax</span>(foo, <span class="number">-1</span>);</span><br><span class="line">bar = torch::<span class="built_in">sigmoid</span>(foo);</span><br><span class="line">bar = torch::<span class="built_in">relu</span>(foo);</span><br><span class="line">bar = torch::<span class="built_in">gelu</span>(foo);</span><br></pre></td></tr></table></figure>]]></content>
    
    
    <summary type="html">&lt;p&gt;系列教程列表：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://vra.github.io/2023/02/25/libtorch-tutorial1/&quot;&gt;Libtorch系列教程1：一个丝滑的C++ Tensor库&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://vra.github.io/2023/02/25/libtorch-tutorial2/&quot;&gt;Libtorch系列教程2：torch::Tensor的使用&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;这篇文章中，我们暂时忽略网络训练和推理，详细展开Libtorch中Tensor对象的使用，看看将Libtorch当作一个纯粹的Tensor库来使用时，有哪些注意事项。如有未涉及的内容，请访问Libtorch&lt;a href=&quot;https://pytorch.org/cppdocs/&quot;&gt;官方文档&lt;/a&gt;，通过搜索框获取更多的信息。Libtorch的环境搭建参考&lt;a href=&quot;https://vra.github.io/2023/02/25/libtorch-tutorial1/&quot;&gt;上一篇文章&lt;/a&gt;。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="C++" scheme="http://vra.github.io/tags/C/"/>
    
    <category term="Pytorch" scheme="http://vra.github.io/tags/Pytorch/"/>
    
    <category term="Libtorch" scheme="http://vra.github.io/tags/Libtorch/"/>
    
  </entry>
  
  <entry>
    <title>Libtorch系列教程1：一个丝滑的C++ Tensor库</title>
    <link href="http://vra.github.io/2023/02/25/libtorch-tutorial1/"/>
    <id>http://vra.github.io/2023/02/25/libtorch-tutorial1/</id>
    <published>2023-02-24T19:03:51.000Z</published>
    <updated>2023-02-27T00:32:59.543Z</updated>
    
    <content type="html"><![CDATA[<p>系列教程列表：</p><ul><li><a href="https://vra.github.io/2023/02/25/libtorch-tutorial1/">Libtorch系列教程1：一个丝滑的C++ Tensor库</a> </li><li><a href="https://vra.github.io/2023/02/25/libtorch-tutorial2/">Libtorch系列教程2：torch::Tensor的使用</a> </li></ul><h2 id="1-概述"><a href="#1-概述" class="headerlink" title="1. 概述"></a>1. 概述</h2><p><a href="https://pytorch.org/cppdocs/installing.html">Libtorch</a>是Pytorch的C++接口，实现了在C++中进行网络训练、网络推理的功能。</p><p>除此之外，由于Libtorch中的大部份接口都是与Pytorch一致的，所以Libtorch还是一个很强大的张量库，有着类似Pytorch的清晰接口，这在C++中很难得的。如果你用过C++ Tensor库，就会发现写法比较复杂，学习成本。因为强类型的限制和通用容器类型的缺失，C++相比Python天然更复杂，库设计者因为语言使用习惯，以及为了性能等因素，设计的接口一般都是高效但难用的。而Libtorch采用了与Pytorch类似的函数接口，如果你使用过Pytorch的话，使用Libtorch学习成本很低，后面会看到具体的例子。</p><p>另一个问题是，很多Python库中基础的操作，例如<code>numpy.einsum</code>函数，在C++中没有合适的替代，看看<a href="https://stackoverflow.com/questions/65347170/numpy-einsum-equivalent-for-xtensor-c">这些</a>搜索你就知道了。Libtorch解决了这个问题，Pytorch中有的它都有，所以在C++中可以简单地用<code>torch::einsum</code>来使用einsum函数，简直是C++开发者的福音。</p><p>此外Libtorch 是支持GPU的，主要用于模型的推理过程，但我猜测使用GPU的话，Libtorch的Tensor操作在速度上相比别的C++ Tensor 库可能有优势，具体速度需要测试对比。当然使用C++代码的话速度不是瓶颈，本身CPU代码就够快了。</p><p>Libtorch另一个优势是编译简单，只要你安装了Pytorch，Libtorch就可以直接使用，省去了复杂的安装和配置，一分钟内就能跑起来一个简单的的示例程序。</p><p>总结来说，Libtorch有以下很吸引人的特性：</p><ul><li>强大如Numpy和Pytorch的C++ Tensor库，写法优雅丝滑，并且是支持GPU的。</li><li>可以训练神经网络</li><li>可以推理神经网络模型，用在C++环境的模型部署场景</li><li>编译简单</li></ul><p>由于Pytorch开发团队是以Python优先的思路来进行Pytorch的开发的，因此我感觉Libtorch的重视程度不是很高，文档和教程也比较少，官网的示例也几乎没有，因此写一个比较完善的教程是比较有意义的。</p><p>这个系列文章中，我会对Libtorch 的Tensor库和推理神经网络过程进行介绍，因为这些内容在实际对于用Libtorch来进行网络训练的部分进行跳过，因为这部分使用的场景不是很多（用Python训练网络比C++香多了)。</p><p>本篇以Mac下的操作为例，对Libtorch的安装和简单使用进行介绍，后续内容近期会更新，敬请关注。</p><span id="more"></span><h2 id="2-Libtorch-安装"><a href="#2-Libtorch-安装" class="headerlink" title="2. Libtorch 安装"></a>2. Libtorch 安装</h2><p>如果你已经安装过Pytorch，那么就不用额外安装Libtorch了，因为Pytorch自带了Libtorch的CMake config 文件，使用<code>torch.utils.cmake_prefix_path</code>语句就能打印出来，可以直接被CMake使用，编译时添加如下的选项：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-DCMAKE_PREFIX_PATH=`python -c <span class="string">&#x27;import torch;print(torch.utils.cmake_prefix_path)&#x27;</span></span><br></pre></td></tr></table></figure><p>如果没有安装过Pytorch，那直接去<a href="https://pytorch.org/">Pytorch官网</a>下载Libtorch 压缩包，解压到本地目录即可，后面使用CMake来指向这里的路径就行。假如解压到<code>LIBTORCH_ROOT</code>目录，编译时添加下面的选项:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">-DCMAKE_PREFIX_PATH=&lt;LIBTORCH_ROOT&gt;</span><br></pre></td></tr></table></figure><h2 id="3-使用CMake-编译一个简单例子"><a href="#3-使用CMake-编译一个简单例子" class="headerlink" title="3. 使用CMake 编译一个简单例子"></a>3. 使用CMake 编译一个简单例子</h2><p>这里写一个简单的Libtorch例子，创建一个5x5的矩阵，然后调用<code>einsum</code>函数来计算矩阵的迹（对角线元素的和）：</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 引入Torch头文件，Tensor类在此头文件中，别的类会在另外的头文件中</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;torch/torch.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 使用arange构造一个一维向量，再用reshape变换到5x5的矩阵</span></span><br><span class="line">  torch::Tensor foo = torch::<span class="built_in">arange</span>(<span class="number">25</span>).<span class="built_in">reshape</span>(&#123;<span class="number">5</span>, <span class="number">5</span>&#125;);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算矩阵的迹</span></span><br><span class="line">  torch::Tensor bar  = torch::<span class="built_in">einsum</span>(<span class="string">&quot;ii&quot;</span>, foo);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 输出矩阵和对应的迹</span></span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;==&gt; matrix is:\n &quot;</span> &lt;&lt; foo &lt;&lt; std::endl;</span><br><span class="line">  std::cout &lt;&lt; <span class="string">&quot;==&gt; trace of it is:\n &quot;</span> &lt;&lt; bar &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意reshape中需要用花括号，因为C++没有tuple类型，Python中的<code>(5,5)</code>需要在C++中改写为<code>&#123;5, 5&#125;</code>。除此之外，是不是跟Python代码很相似？</p><p>记得保存上面的代码为<code>libtorch_trace.cpp</code>，因为CMake配置中需要写文件名。</p><p>然后在同级目录编写<code>CMakeLists.txt</code>文件:</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.0</span> FATAL_ERROR)</span><br><span class="line"><span class="keyword">project</span>(libtorch_trace)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 需要找到Libtorch</span></span><br><span class="line"><span class="keyword">find_package</span>(Torch REQUIRED)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(CMAKE_CXX_FLAGS <span class="string">&quot;$&#123;CMAKE_CXX_FLAGS&#125; $&#123;TORCH_CXX_FLAGS&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">add_executable</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> libtorch_trace.cpp)</span><br><span class="line"><span class="keyword">target_link_libraries</span>(<span class="variable">$&#123;PROJECT_NAME&#125;</span> <span class="string">&quot;$&#123;TORCH_LIBRARIES&#125;&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Libtorch是基于C++14来实现的</span></span><br><span class="line"><span class="keyword">set_property</span>(<span class="keyword">TARGET</span> <span class="variable">$&#123;PROJECT_NAME&#125;</span> PROPERTY CXX_STANDARD <span class="number">14</span>)</span><br></pre></td></tr></table></figure><p>然后执行下面的命令来编译:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir build</span><br><span class="line"><span class="built_in">cd</span> build</span><br><span class="line"><span class="comment"># 如果是通过Pytorch</span></span><br><span class="line">cmake -DCMAKE_PREFIX_PATH=`python -c <span class="string">&#x27;import torch;print(torch.utils.cmake_prefix_path)&#x27;</span>` ..</span><br><span class="line"><span class="comment">#下载的单独Libtorch</span></span><br><span class="line"><span class="comment"># cmake -DCMAKE_PREFIX_PATH=&lt;LIBTORCH_ROOT&gt; ..</span></span><br><span class="line">make -j8</span><br></pre></td></tr></table></figure><p>编译完成后使用下面的命令来执行可执行文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./libtorch_trace</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">==&gt; matrix is:</span><br><span class="line">   0   1   2   3   4</span><br><span class="line">  5   6   7   8   9</span><br><span class="line"> 10  11  12  13  14</span><br><span class="line"> 15  16  17  18  19</span><br><span class="line"> 20  21  22  23  24</span><br><span class="line">[ CPULongType&#123;5,5&#125; ]</span><br><span class="line">==&gt; trace of it is:</span><br><span class="line"> 60</span><br><span class="line">[ CPULongType&#123;&#125; ]</span><br></pre></td></tr></table></figure><p>那么我们的第一个例子就完成了。</p><h2 id="4-参考"><a href="#4-参考" class="headerlink" title="4. 参考"></a>4. 参考</h2><ul><li><a href="https://pytorch.org/cppdocs/installing.html">https://pytorch.org/cppdocs/installing.html</a></li></ul>]]></content>
    
    
    <summary type="html">&lt;p&gt;系列教程列表：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://vra.github.io/2023/02/25/libtorch-tutorial1/&quot;&gt;Libtorch系列教程1：一个丝滑的C++ Tensor库&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://vra.github.io/2023/02/25/libtorch-tutorial2/&quot;&gt;Libtorch系列教程2：torch::Tensor的使用&lt;/a&gt; &lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&quot;1-概述&quot;&gt;&lt;a href=&quot;#1-概述&quot; class=&quot;headerlink&quot; title=&quot;1. 概述&quot;&gt;&lt;/a&gt;1. 概述&lt;/h2&gt;&lt;p&gt;&lt;a href=&quot;https://pytorch.org/cppdocs/installing.html&quot;&gt;Libtorch&lt;/a&gt;是Pytorch的C++接口，实现了在C++中进行网络训练、网络推理的功能。&lt;/p&gt;
&lt;p&gt;除此之外，由于Libtorch中的大部份接口都是与Pytorch一致的，所以Libtorch还是一个很强大的张量库，有着类似Pytorch的清晰接口，这在C++中很难得的。如果你用过C++ Tensor库，就会发现写法比较复杂，学习成本。因为强类型的限制和通用容器类型的缺失，C++相比Python天然更复杂，库设计者因为语言使用习惯，以及为了性能等因素，设计的接口一般都是高效但难用的。而Libtorch采用了与Pytorch类似的函数接口，如果你使用过Pytorch的话，使用Libtorch学习成本很低，后面会看到具体的例子。&lt;/p&gt;
&lt;p&gt;另一个问题是，很多Python库中基础的操作，例如&lt;code&gt;numpy.einsum&lt;/code&gt;函数，在C++中没有合适的替代，看看&lt;a href=&quot;https://stackoverflow.com/questions/65347170/numpy-einsum-equivalent-for-xtensor-c&quot;&gt;这些&lt;/a&gt;搜索你就知道了。Libtorch解决了这个问题，Pytorch中有的它都有，所以在C++中可以简单地用&lt;code&gt;torch::einsum&lt;/code&gt;来使用einsum函数，简直是C++开发者的福音。&lt;/p&gt;
&lt;p&gt;此外Libtorch 是支持GPU的，主要用于模型的推理过程，但我猜测使用GPU的话，Libtorch的Tensor操作在速度上相比别的C++ Tensor 库可能有优势，具体速度需要测试对比。当然使用C++代码的话速度不是瓶颈，本身CPU代码就够快了。&lt;/p&gt;
&lt;p&gt;Libtorch另一个优势是编译简单，只要你安装了Pytorch，Libtorch就可以直接使用，省去了复杂的安装和配置，一分钟内就能跑起来一个简单的的示例程序。&lt;/p&gt;
&lt;p&gt;总结来说，Libtorch有以下很吸引人的特性：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;强大如Numpy和Pytorch的C++ Tensor库，写法优雅丝滑，并且是支持GPU的。&lt;/li&gt;
&lt;li&gt;可以训练神经网络&lt;/li&gt;
&lt;li&gt;可以推理神经网络模型，用在C++环境的模型部署场景&lt;/li&gt;
&lt;li&gt;编译简单&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;由于Pytorch开发团队是以Python优先的思路来进行Pytorch的开发的，因此我感觉Libtorch的重视程度不是很高，文档和教程也比较少，官网的示例也几乎没有，因此写一个比较完善的教程是比较有意义的。&lt;/p&gt;
&lt;p&gt;这个系列文章中，我会对Libtorch 的Tensor库和推理神经网络过程进行介绍，因为这些内容在实际对于用Libtorch来进行网络训练的部分进行跳过，因为这部分使用的场景不是很多（用Python训练网络比C++香多了)。&lt;/p&gt;
&lt;p&gt;本篇以Mac下的操作为例，对Libtorch的安装和简单使用进行介绍，后续内容近期会更新，敬请关注。&lt;/p&gt;</summary>
    
    
    
    
    <category term="Python" scheme="http://vra.github.io/tags/Python/"/>
    
    <category term="C++" scheme="http://vra.github.io/tags/C/"/>
    
    <category term="Pytorch" scheme="http://vra.github.io/tags/Pytorch/"/>
    
    <category term="Libtorch" scheme="http://vra.github.io/tags/Libtorch/"/>
    
  </entry>
  
</feed>
